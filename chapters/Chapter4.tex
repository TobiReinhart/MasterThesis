\dictum{
After providing the reader with a concise introduction to functional programming in Haskell, focusing on the main differences with more traditional imperative programming languages, we proceed by illustrating the developed Haskell tensor algebra library sparse-tensor in detail. Not only will we explain the basic functionality contained therein, but the following chapters are also intended as a short introduction manual for potential future users. Finally, we also clarify how the construction of Lorentz invariant basis tensors is implemented in sparse-tensor.
The sparse-tensor library is available via \cite{sparse-tensor}.}   

\section{A Concise Introduction to Functional Programming in Haskell}
For the following chapter to constitute a self-contained description of the developed computer algebra also for readers that are unfamiliar with the programming language Haskell \cite{Marlow_haskell2010} we now furnish a quick introduction to it. We mainly treat its particularities compared with more traditional imperative languages.
A more in-depth description of the Haskell programming language, including 
further details can be found in any good textbook as for instance in \cite{Thompson99thecraft}, \cite{bird_2014}, \cite{hutton_2007}. Introductions to Haskell that rather resemble user guides aiming to allow the reader to write his own programs rapidly can be found in  \cite{OSullivan2008} and  \cite{Lipovaca:2011:LYH:2018642}. Much information is also collected in the HaskellWiki \cite{wiki:xxx}.

Compared with traditional imperative programming languages such as $C$, the most striking difference of Haskell lies in the fact that it employs a \textbf{\textit{purely functional}} programming paradigm. In imperative languages, a computer program consists of a sequence of commands for the computer to execute step by step. Thus not only the individual steps but also the specific order of their execution, the control flow, is explicitly provided by the programmer. Typically this is achieved by using control structures such as loops or conditional statements. At each time during its execution, the computer program can be said to occupy a known state that can be described by the explicit information that is stored in the entirety of its variables. A single computation step then precisely tells the program how to transfer from a given state to the subsequent one. This corresponds to a change in the values of some of the variables that describe the program's state. Apart from invoking the desired modifications of its variables that are necessary for the specific computation additionally such a computer program might undesirably change values of further variables. This is referred to as \textit{\textbf{side effects}}. As a consequence executing such a computer program that contains side effects multiple times might not return the same result each time as side effects that possibly occur during the execution can alter the outcome. Needless to say that unexpected side effects compromise a broad source of potential errors and are hence best avoided.

One way of avoiding side effects is by employing a functional programming paradigm. In contrast to an imperative programming style, a computer program no longer consists of a step by step instruction of state changes but is given by a mathematical function that maps some input data to the desired output. The execution of the program then simply corresponds to the evaluation of this function. As such a function obviously only depends on the data it is given as input, it is completely free of side effects.

In practice, one can build a computer program using such a functional programming style by composing it of several individual functions that describe sub-parts of the program via usual function composition. Besides of composition in most functional languages functions of multiple variables can also be  \textit{\textbf{curried}} to obtain an equivalent higher-order function that only takes a single argument but now yields a function as a result:
\begin{center}
    \mintinline{haskell}{f :: (A,B) -> C} $\ \xrightarrow{ \ currying \ } \ $
    \mintinline{haskell}{f' :: A -> (B -> C)}
\end{center}
Evaluating the curried function \mintinline{haskell}{f'} on some value \mintinline{haskell}{a :: A} simply corresponds to the partial evaluation of the original function \mintinline{haskell}{f}, i.e., leaving free the second slot of \mintinline{haskell}{f} and thus returning a function that maps from \mintinline{haskell}{B}  to \mintinline{haskell}{C}. Note that currying really establishes an equivalence between functions that take multiple arguments and higher-order functions of a single argument.
In fact in Haskell all functions take precisely one argument, i.e., functions that take several arguments are always represented in curried form. The general form of a function definition in Haskell reads:
\begin{center}
\begin{cminted}{haskell}
f :: A -> (B -> C)
f a b = ...  
\end{cminted}
\end{center}
Here
\mintinline{haskell}{f a b = (f a) b} denotes the evaluation of the function \mintinline{haskell}{f} first on \mintinline{haskell}{a} and then applying the resulting function to \mintinline{haskell}{b}. Note that the function evaluation is left associative.
In the above, we already used the second remarkable feature of the  Haskell programming language to some extent, the Haskell \textbf{\textit{type system}}. Every Haskell expression has a type that is denoted by \mintinline{haskell}{expr :: type}. Hence the first line in the above function definition simply declares   the type of the function \mintinline{haskell}{f :: A -> (B -> C)}.
Function evaluation behaves type sensitive, i.e., we might only evaluate a given function on a specific value if the type of the value matches the type of the function.
Note that whereas the evaluation of functions behaves left-associative, the corresponding types are right-associative. The type of the above function can thus also be written as \mintinline{haskell}{f :: A -> B -> C}.

New data types can be defined by specifying \textbf{\textbf{data constructors}}, functions that describe how the new data type is constructed in terms of known data types. We can illustrate this by constructing a data type that represents a pair of integers:
\begin{center}
\begin{cminted}{haskell}
data IntPair = IntPair Int Int 
\end{cminted}
\end{center}
The left-hand side of this definition simply contains the name of the new data type. The right-hand side declares the type of the constructor function, which is usually given the same name as the data type. 
Note that the constructor \mintinline{haskell}{IntPair} by definition yields a function that when applied to two integers returns an expression with type IntPair. 

When defining functions in Haskell one can \textbf{\textit{pattern match}} against data constructors. In other words one may define the function value in terms of values that are given to the constructor of the input argument. To provide an example we can define a function that computes the sum of the two integers contained in an \mintinline{haskell}{IntPair}:
\begin{center}
\begin{cminted}{haskell}
sumIntPair :: IntPair -> Int 
sumIntPair (IntPair x y) = x + y
\end{cminted}
\end{center}
where we used the infix notation for the addition of integers. 

The unique property of the Haskell type system, however, is that already when compiling Haskell code the compiler checks whether the types of the various function applications contained therein match. If not, the program is immediately rejected by the compiler. Thus potential type errors are already avoided at compile time and therefore cannot lead to runtime errors. Languages that employ this form of type checking are also called \textit{\textbf{statically typed}}. Also, our developed tensor algebra package intensively relies on the Haskell type system. It is built such that typos that might occur when using it to compute tensorial expressions almost always yield a type error. Especially when dealing with longer computations, this is a massive advantage as such typos or similar mistakes are then already detected when compiling the code and not after possibly several hours of runtime.  

Finally, the last special feature that Haskell admits is its \textit{\textbf{lazy evaluation}} strategy. Functions are only explicitly evaluated on their arguments once the result is needed. This does not only allow for performance improvements as by employing lazy evaluation, unnecessary function calls can be avoided; it also provides further advantages. For instance, lazy evaluation enables the use of conditional control structures in a purely functional style. The usual conditional if statement, for instance, can be given by a function:
\begin{center}
\begin{cminted}{haskell}
if' :: Bool -> a -> a -> a
if' False _ y = y 
if' True x _ = x 
\end{cminted}
\end{center}
Note that when the boolean evaluates to \mintinline{haskell}{True} this function simply returns the expression \mintinline{haskell}{x}. Thus when using lazy evaluation in this case, \mintinline{haskell}{y} is never evaluated. Similarly \mintinline{haskell}{x} is not evaluated when the boolean is \mintinline{haskell}{False}. 

Besides the above particularities of Haskell as a programming language in order to fully grasp the essence of the algorithms that will be outlined in the following section, it is crucial to accustom oneself with some essential functions that are heavily used in functional programming languages to iteration over data structures. These techniques can be understood best by considering a particular example. Amongst the simplest data structures that can be used for the iteration are lists. In Haskell, the list data structure is defined as follows: 
\begin{center}
\begin{cminted}{haskell}
data [a] = [ ] | a : [a] 
\end{cminted}
\end{center}
Thus it has two constructors \mintinline{haskell}{[ ]} constructing the empty list and \mintinline{haskell}{(:)} appending an element to the front of a given list. Note that \mintinline{haskell}{(:)} is written as an infix operator.
The first heavily used function is the \mintinline{haskell}{map}function:
\begin{center}
\begin{cminted}{haskell}
map :: (a -> b) -> [a] -> [b]
map f [] = [] 
map f (x:xs) =  f x : map f xs 
\end{cminted}
\end{center}
Hence the map function takes a function and a list and applies he function to each element of the list returning the resulting list.
The concept of mapping a function over a data structure is not restricted to lists but can also be employed for more general structures. Data types that can be mapped over must be instances of the \mintinline{haskell}{Functor} type class.

The second important concept are the fold operators \mintinline{haskell}{foldl} and \mintinline{haskell}{foldr}. They exist in several versions. The main idea consists of taking a binary function, a start argument and a list, or any other instance of the \mintinline{haskell}{Foldable} type class, containing additional arguments and then successively reducing the list by first applying the function to the start argument and the first argument of the list and then proceeding by evaluating the function on the thus obtained result and the next list element. One can distinguish folds by whether they reduce the list starting from its first (left fold, \mintinline{haskell}{foldl}) or its last (right fold, \mintinline{haskell}{foldr}) value. A left fold operator can be implemented in Haskell according to:
\begin{center}
\begin{cminted}{haskell}
foldl :: (a -> b -> a) -> a -> [b] -> a 
foldl f x [] = x 
foldl f x (y:ys) = foldl f (f x y) ys
\end{cminted}
\end{center}
Obviously, there exist many more such functions that allow each particular problem at hand to be treated in a suitable way using a purely functional programming paradigm. Details can be found in the provided literature.

\section{sparse-tensor: A Typesafe Tensor Algebra Library }
With the short introduction to Haskell at hand, we now explain the main ideas that underlie the developed Haskell library sparse-tensor. Ultimately the aim consists of solving the perturbative equivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}). Recall that these three systems are merely compromised of linear equations. In principle, it should thus be straight forward to re-express these in suitable matrix from and use standard matrix computer algebra to solve them. Practically however extracting the appropriate matrices from the systems is a different question as for doing so one needs to evaluate the tensorial expressions in all free indices and to that end also
explicitly express all occurring contractions and symmetrizations.
Thus using solely standard matrix computer algebra to solve the perturbative equivariance equations is hardly possible.

On the other hand existing tensor algebra systems (such as Cadabra (see \cite{cadabra1} and \cite{cadabra2}) and the Physics package \cite{MaplePhysics} currently provided by the computer algebra system Maple to name a few) are mostly tailored towards a flexible, symbolic treatment of tensors, not an efficient retrieval of their particular components and thus are not entirely suited for the specific purpose of solving the perturbative equivariance equations. 

These observations led us to develop our own computer algebra framework. We quickly summarize the main ideas that underlie its construction.
As a first observation note that except for parameters that label the independent Lorentz invariant expressions in the expansion coefficients and thus occur linearly in the perturbative equivariance equations all remaining expressions only contain rational numbers. In particular, the components of the tensors involved in these equations are explicitly known. Besides, when solving the perturbative equivariance equations, we need to evaluate the tensorial equation for all its free indices, and thus, the explicit components are also all needed. Hence, in fact, we never have to treat the occurring tensors symbolically as abstract objects but can think of them as containers that are simply used to store values in a particular way. 

Furthermore, it is crucial to grasp that the use of abstract indices $I, A, ...$ leads to a severe reduction in computation costs and is thus best included in the computer program. For instance, contracting two blocks with $4$ indices each that both admit the symmetry of an area metric tensor against each other requires the summation of $4^4 = 256$ expressions. In contrast to that introducing an abstract index $A = 0,...,20$ that runs over the independent components such an area metric block contains, the contraction only requires the summation of $21$ expressions.

Finally, we can additionally derive a benefit from the fact that the tensors that occur in the treatment of the perturbative equivariance equations are to a large extent only sparsely occupied. Hence it is best to also only work with the non zero components in the computer program. Providing an example the area metric intertwiner with components displayed in (\ref{AreaI}) only contains $144$ non-vanishing components out of a total of $21 \cdot 4^4 = 5376$ total components which admits to an occupation level of around $2.68 \%$.

Summing up, we want to incorporate the following functionality in our computer program:
\begin{itemize}
    \item treatment of tensors with multiple abstract indices each running over an individual index range. 
    \item Sparse storage of tensor components.
    \item Optimization towards computing explicit component operations over abstract algebraic manipulation of tensors.
\end{itemize}

Additionally to the three requirements displayed above we are going to encode the generalized rank of a given tensor --- for standard tensors described by contravariant and covariant indices the rank is expressed as pair ($\#$ of contravariant indices, $\#$ of covariant indices), consequently we express the rank of a tensor that takes $n$ contravariant and $n$ covariant index types as the corresponding $2n$-tuple of numbers of indices --- directly in its type. Doing so Haskell's type system guarantees the prevention of possible errors that might, for instance, occur when by mistake adding tensors of different ranks. The detection of such errors then already happens during compile time. During our usage of the developed computer algebra framework, this feature really turned out to be vital as entering the complicated equations involving multiple indices that one wishes to solve in the perturbative framework of Constructive Gravity often led to minor mistakes that where then immediately detected when compiling the program and not after possibly several hours of runtime.

We start with the underlying data type that we defined to represent a tensor of arbitrary rank that only takes a single index type, i.e., We can think of such a type as representing tensors with only contravariant indices of the given type. Using Haskell's generalized algebraic data types, in short \textit{\textbf{GADT}}s the data type definition of such a single index tensor is displayed in listing \ref{TensorDat}. 

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40
]{haskell}
-- | Basic tensor data types.
data Tensor n k v where
    -- | Constructor of leaf values.
    Scalar :: v -> Tensor 0 k v
    -- | Constructs a @'Tensor'@ from a @'TMap'@ of index sub
    --   tensor pairs.
    Tensor :: TMap k (Tensor n k v) -> Tensor (n+1) k v
    -- | Represents a @'Tensor'@ that is identical zero.
    ZeroTensor :: Tensor n k v
\end{minted} 
\caption{Tensor Data Type.}\label{TensorDat}
\end{listing}

Hence the tensor data type \mintinline{haskell}{Tensor n k v} contains additional type information consisting of a type-level natural number \mintinline{haskell}{n} representing the rank of the tensor, i.e., its number of indices and types \mintinline{haskell}{k} and \mintinline{haskell}{v} that encode the type of index the tensor takes and the type of values it stores. The data type then provides three constructors: \mintinline{haskell}{Scalar} constructs a rank 0 tensor, i.e., a scalar out of a given value, the constructor \mintinline{haskell}{Tensor} takes an ordered list of (index, sub tensor) pairs:
\begin{center}
\begin{cminted}{haskell}
type TMap k v = [(k,v)]
\end{cminted}
\end{center}
where the sub tensors have rank \mintinline{haskell}{n} and constructs from it a \mintinline{haskell}{Tensor (n+1) k v} with rank \mintinline{haskell}{n+1} and \mintinline{haskell}{ZeroTensor} simply represents a tensors of arbitrary rank with all components being identical zero. Thus a tensor is represented as ordered forest\footnote{A collection of disjoint ordered trees.} with additional type information regarding rank, index type, and stored values. Including an ordering of the individual sibling sets \footnote{We call a collection of nodes that have a common parent node a sibling set.} allows for faster insertion and lookup operations. All further functions that we define for the types \mintinline{haskell}{Tensor} and \mintinline{haskell}{TMap} will maintain this ordering.

To retrieve a value of a tensor of rank \mintinline{haskell}{n} we need to specify \mintinline{haskell}{n} values of the appropriate type \mintinline{haskell}{k}. It is important to understand that the distinction between the individual indices is not provided by some abstract labels that are attached to them but simply by their position. To provide an example, figure \ref{ExampleTens} displays the forest structure of a rank $2$ tensor that uses \mintinline{haskell}{Ind3} types, i.e., spacetime indices with potential values between $0$ and $3$ as index type and stores rational numbers as values.
\begin{figure}[hbt!]
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG!40, very thick, minimum size=7mm},]
\node[roundnode]  (I1) at (0,0) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (I2) at (0,-3) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (I3) at (0,-7) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J1) at (4,1) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (J2) at (4,-1) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J3) at (4,-3) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J4) at (4,-5) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J5) at (4,-7) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (J6) at (4,-9) {\mintinline{haskell}{Ind3 3}};
\node  (K1) at (8,1) {\mintinline{haskell}{Scalar 1 % 2}};
\node  (K2) at (8,-1) {\mintinline{haskell}{Scalar 3 % 1}};
\node  (K3) at (8,-3) {\mintinline{haskell}{Scalar -17 % 1}};
\node  (K4) at (8,-5) {\mintinline{haskell}{Scalar 1 % 3}};
\node  (K5) at (8,-7) {\mintinline{haskell}{Scalar 1 % 1}};
\node  (K6) at (8,-9) {\mintinline{haskell}{Scalar 1 % 2}};


\draw [-] (I1) -- (J1);
\draw [-] (I1) -- (J2);
\draw [-] (I2) -- (J3);
\draw [-] (I3) -- (J4);
\draw [-] (I3) -- (J5);
\draw [-] (I3) -- (J6);
\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);


\end{tikzpicture}
\caption{Forest Structure of Rank 2 Spacetime Tensor with Rational Values.}\label{ExampleTens}
\end{figure}
Providing the non-vanishing components of the displayed tensor forest explicitly the tensor would read:
\begin{align}
\begin{alignedat}{3}
T^{01} &= \frac{1}{2}, \ \  &  \ \ T^{02} &= 3, \ \  & \ \ T^{22} &= -17,\\
T^{30} &= \frac{1}{3}, & T^{31} &= 1, & T^{33} &= \frac{1}{2}.
\end{alignedat}
\end{align}

In order for tensors, represented this way to satisfy the usual tensor algebra, some restrictions on the possibly stored values are necessary. In particular, we need to be able to add and subtract the stored values as this is necessary when adding and subtracting tensors.
More precisely, the types that we might use as value must constitute an additive group. 
We collect such types that constitute a group and the corresponding group methods in the \mintinline{haskell}{TAdd} \textit{\textbf{type class}} displayed in (\ref{TAdd}).

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
class TAdd a where
    -- | Test whether the given element is zero, i.e., the neutral
    --   element.
    scaleZero :: a -> Bool
    -- | Addition of two elements.
    addS :: a -> a -> a
    -- | Maps an element to its additive inverse.
    negateS :: a -> a
    -- | Subtraction of two elements.
    subS :: a -> a -> a
    subS a b = a `addS` negateS b
\end{minted} 
\caption{Addition Type Class.}\label{TAdd}
\end{listing}

Furthermore, when forming the product of two tensors or also scaling a given tensor with a number, we also need to be able to compute products of the tensor values or between tensors values and scalars. This requirement is encoded in the additional \mintinline{haskell}{TProd} type class (\ref{Prod}).

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
class Prod v v' where
    -- | Type level function that returns the type of the result of
    --   @'prod'@.
    type TProd v v' :: *
    -- | Product function.
    prod :: v -> v' -> TProd v v'
\end{minted} 
\caption{Product Type Class.}\label{Prod}
\end{listing}

Note that the above type class not only requires the existence of a function \mintinline{haskell}{prod} that computes the products of any two stored values of types \mintinline{haskell}{v} and \mintinline{haskell}{v'}, it also requires its instances to provide a type level function that computes the new type of such a product \mintinline{haskell}{TProd v v'} once the types \mintinline{haskell}{v} and \mintinline{haskell}{v'} are specified.
The first instance that we are going to provide for the two introduced type classes is used to represent arbitrary number-types that allow for the required functions:
\begin{center}
\begin{cminted}{haskell}
newtype SField a = SField a deriving (Show, Eq, Ord)
\end{cminted} 
\end{center}
\mintinline{haskell}{SField} simply provides a wrapper for any number-type at wish.

Finally, the indices of any given tensor also must satisfy specific properties that can be described by class constraints that constraint their possible types. For instance, tensor indices should employ an order relation of their possible values, and also it should be possible to test two index values for equality. The necessary class constraints are combined in the \mintinline{haskell}{TIndex} type class:
\begin{center}
\begin{cminted}{haskell}
class (Eq a, Ord a, Enum a) => TIndex a where
\end{cminted} 
\end{center}



Restricting now to tensors with indices and values being instances of these type classes, we can define the usual tensor algebra operations.
Note that these operations provide the \mintinline{haskell}{Tensor} type with the structure of the usual graded\footnote{A graded algebra is an algebra whose elements can be labeled by elements of some monoid (such as positive integers equipped with addition) or group such that the algebra multiplication is consistent with the group operation of the labels. The label of an algebra element is called its grade.
For details see for instance \cite{bourbaki1998algebra} and also \cite{nlab:gradedAlg}.} tensor algebra on the type level. In other words, the grades of the individual tensors will be directly reflected by their type.
The source code for achieving the addition of two tensors is provided in listing (\ref{Addition}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
(&+) :: (TIndex k, TAdd v) => Tensor n k v -> Tensor n k v ->
                              Tensor n k v
(&+) (Scalar a) (Scalar b) = Scalar $ a `addS` b
(&+) (Tensor m1) (Tensor m2) = Tensor $ addTMaps (&+) m1 m2
(&+) t1 ZeroTensor = t1
(&+) ZeroTensor t2 = t2
\end{minted} 
\caption{Addition Function of Tensors.}\label{Addition}
\end{listing}
Here \mintinline{haskell}{addTMaps} (see listing \ref{addTmaps}) is a function that adds the two \mintinline{haskell}{TMaps k v} of the two tensors with a combiner function that is called to obtain the appropriate value if an index is present in both lists of type \mintinline{haskell}{TMaps k v}. It further ensures that the order of the indices is still valid when combining the two lists. In the case above the combiner function is again the addition of tensors, this time, however, only applied to the appropriate sub tensors. 

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
addTMaps :: (Ord k) => (v -> v -> v) -> TMap k v -> TMap k v ->
                                        TMap k v 
addTMaps f m1 [] = m1 
addTMaps f [] m2 = m2 
addTMaps f ((k1,v1):xs) ((k2,v2):ys) 
            | k1 < k2 = (k1,v1) : (addTMaps f xs ((k2,v2):ys))
            | k2 < k1 = (k2,v2) : (addTMaps f ((k1,v1):xs) ys)
            | k1 == k2 = (k1, f v1 v2) : (addTMaps f xs ys) 
\end{minted} 
\caption{Helper Function: Addition of Tensor Lists.}\label{addTmaps}
\end{listing}

Scalar multiplication (see listing \ref{ScalarProd}) and subtraction (see listing \ref{SubTens}) can now be implemented straight forwardly.
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
(&.) :: (TIndex k, Prod s v) => s -> Tensor n k v ->
                                     Tensor n k (TProd s v)
(&.) scalar = fmap (prod scalar)
\end{minted} 
\caption{Scalar Multiplication of Tensors.}\label{ScalarProd}
\end{listing}

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
(&-) :: (TIndex k, TAdd v) => Tensor n k v -> Tensor n k v ->
                              Tensor n k v
(&-) (Scalar a) (Scalar b) = Scalar $ subS a b
(&-) (Tensor m1) (Tensor m2) = Tensor $ addTMaps (&-) m1 m2
(&-) t1 ZeroTensor = t1
(&-) ZeroTensor t2 = negateS t2
\end{minted} 
\caption{Subtraction of Tensors.}\label{SubTens}
\end{listing}
Where \mintinline{haskell}{fmap} similar to the discussed case of mapping a function over a list of values maps a function over the values of a given tensor and is provided by the functor instance (see (\ref{Functor})) of the tensor data type:

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
instance Functor (Tensor n k) where 
        fmap f (Scalar x) = Scalar (f x)
        fmap f (Tensor m) = Tensor (mapTMap (fmap f) m)
        fmap f ZeroTensor = ZeroTensor 
\end{minted}
\caption{Functor Instance of Tensor Data Type.}\label{Functor}
\end{listing}

The product of two tensors displayed in (\ref{TensorProd}) can be implemented by appending the second tensor to each of the leaves of the first tensor that is specified. This is done such that the indices of the first tensor always are included in the result before the indices of the second tensor, i.e., the order of indices in the newly obtained tensor is the same as one would expect from naively writing down such a tensor product.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
(&*) :: (TIndex k, Prod v v') => Tensor n k v -> Tensor m k v' ->
                                 TProd (Tensor n k v) (Tensor m k v')
(&*) (Scalar x) (Scalar y) = Scalar $ prod x y
(&*) (Scalar x) t2 = fmap (prod x) t2
(&*) (Tensor m) t2 = Tensor $ mapTMap (&* t2) m
(&*) t1 ZeroTensor = ZeroTensor
(&*) ZeroTensor t2 = ZeroTensor
\end{minted}
\caption{Tensor Product Function.}\label{TensorProd}
\end{listing}
Here \mintinline{haskell}{mapTMap} maps a function over the values of the key value pairs \mintinline{haskell}{(k,v)} stored in the \mintinline{haskell}{TMap k v}.
Note that not only the rank of the resulting tensor depends on the ranks of the two input tensors --- The resulting rank is precisely given as the sum of the two input ranks. This is exactly where the grading of the tensor algebra is reflected on the type level --- also the types that are stored in it as values clearly depend on the respective types of the input tensors. This is precisely where the second type-class \mintinline{haskell}{TProd v v'} comes to use. 

When symmetrizing a given tensor, we need to be able to swap the position of some of the tensor's indices. This is done by the following function (\ref{TensorTrans}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
tensorTrans :: (TIndex k, TAdd v) => (Int,Int) -> Tensor n k v ->
                                     Tensor n k v
tensorTrans (0, j) t = fromListT l
                where
                    l = map (\(x,y) -> (swapHead j x, y)) $ toListT t
tensorTrans (i, j) (Tensor m) = Tensor $
                                mapTMap (tensorTrans (i-1, j-1)) m
tensorTrans (i ,j) ZeroTensor = ZeroTensor
\end{minted}
\caption{Transposition of Tensors in two Indices.}\label{TensorTrans}
\end{listing}
The two indices w.r.t. which the tensor is transposed are provided by the integer tuple, where the indices are labeled w.r.t. their position starting from 0. The function first traverses the tensor until the level\footnote{We will call the distance of a given node to the root node of the appropriate tree the level of the node.} specified by the first, i.e., the smaller integer is reached. Then all corresponding sub tensors are completely flattened to lists of indices value pairs. This is precisely what \mintinline{haskell}{toListT} does. Now in each of the thus obtained indices value lists of the various sub tensors by using the function \mintinline{haskell}{swapHead} the first and the $j$th entry of the indices are swapped. Finally, by using  \mintinline{haskell}{fromListT}, the indices value lists are again transformed into a tensor.  Applying this function to the previously provided rank $2$ example tensor with the integer pair given as \mintinline{haskell}{(0,1)} we obtain the following transposed rank $2$ tensors that are displayed in figure \ref{ExampleTensTrans}.

\begin{figure}[hbt!]
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG!40, very thick, minimum size=7mm},]
\node[roundnode]  (I1) at (0,0) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (I2) at (0,-2) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (I3) at (0,-4) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (I4) at (0,-6) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J1) at (4,0) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J2) at (4,-1) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J3) at (4,-2.5) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J4) at (4,-3.5) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J5) at (4,-5) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J6) at (4,-6) {\mintinline{haskell}{Ind3 3}};

\node  (K1) at (8,0) {\mintinline{haskell}{Scalar 1%3}};
\node  (K2) at (8,-1) {\mintinline{haskell}{Scalar 1%2}};
\node  (K3) at (8,-2.5) {\mintinline{haskell}{Scalar 1%1}};
\node  (K4) at (8,-3.5) {\mintinline{haskell}{Scalar 3%1}};
\node  (K5) at (8,-5) {\mintinline{haskell}{Scalar -17%1}};
\node  (K6) at (8,-6) {\mintinline{haskell}{Scalar 1%2}};


\draw [-] (I1) -- (J1);
\draw [-] (I2) -- (J2);
\draw [-] (I2) -- (J3);
\draw [-] (I3) -- (J4);
\draw [-] (I3) -- (J5);
\draw [-] (I4) -- (J6);
\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);


\end{tikzpicture}
\caption{Forest Structure of Transposed Rank 2 Tensor from Figure \ref{ExampleTens}.}\label{ExampleTensTrans}
\end{figure}
Now the corresponding values are given as:
\begin{align}
\begin{alignedat}{3}
T^{10} &= \frac{1}{2}, \ \  &  \ \ T^{20} &= 3, \ \  & \ \ T^{22} &= -17,\\
T^{03} &= \frac{1}{3}, & T^{13} &= 1, & T^{33} &= \frac{1}{2}.
\end{alignedat}
\end{align}

Similar to the transposition of two indices we can now obviously construct functions that transpose a given tensor in several indices or even completely reorder (\ref{resortTens}) the positions of the various indices. 
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
resortTens :: (KnownNat n, TIndex k, TAdd v) => [Int] -> Tensor n k v ->
                                                Tensor n k v
resortTens perm t = fromListT $
                    map (\(x,y) -> (resortInd perm x, y)) $ toListT t
\end{minted} 
\caption{General Reordering of Tensor Indices.}\label{resortTens}
\end{listing}
The function takes as argument a list of integers where the $i$th elements specifies the position on which the $i$th index in the tensor shall be sorted. For instance the integer list \mintinline{haskell}{[3,0,2,1]} sorts the 0th index of the tensor to position \mintinline{haskell}{3}, the first index on position \mintinline{haskell}{0}, the second index on position \mintinline{haskell}{2} and the third an last index on position \mintinline{haskell}{1}. Note that the length of the provided integer list must be the same as the number of indices in the given tensor.  The resorting is then achieved by flattening the tensor to a list of indices value pairs, resorting the indices as specified and then reconstructing the tensor from the newly obtained indices value list:
\begin{center}
\begin{cminted}{haskell}
resortTens [3,0,2,1] (fromListT' [([0,1,2,3],1)] :: Tensor 4 Ind3 Rational)
= (fromListT' [([1,3,2,0],1)] :: Tensor 4 Ind3 Rational)
\end{cminted}
\end{center}
Knowing how we can transpose and resort the indices of a given tensor we can easily construct arbitrary symmetrization functions, simply by rearranging the indices of a given tensor accordingly and then adding this newly obtained tensor to the previous one. We only provide the example of the standard symmetrization w.r.t. the exchange of too individual indices explicitly (\ref{symTens}).

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
symTensFac :: (TIndex k, TAdd v, Prod (SField Rational) v) => (Int,Int)
              -> Tensor n k v -> Tensor n k (TProd (SField Rational) v)
symTensFac inds t = (SField $ 1%2) &. symTens inds t
\end{minted} 
\caption{Pair Symmetrization of Tensors.}\label{symTens}
\end{listing}

Any other symmetrization can be constructed in similar ways.

Of course, we can also evaluate a given tensor by inserting a particular value for one of its indices. This can be achieved as displayed in listing (\ref{evalTens}).
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
evalTens :: (KnownNat (n+1), TIndex k, TAdd v) => Int -> k ->
            Tensor (n+1) k v -> Tensor n k v
evalTens ind indVal (Tensor m)
            | ind > size -1 || ind < 0 = error "wrong index to evaluate"
            | ind == 0 = fromMaybe ZeroTensor $ lookup indVal m
            | otherwise = fromMaybe ZeroTensor $
                          lookup indVal (getTensorMap newTens)
            where
                size = length $ fst $ head $ toListT' (Tensor m)
                l = [1..ind] ++ 0 : [ind+1..size -1]
                newTens = resortTens l (Tensor m)
\end{minted}
\caption{Evaluation Function for Tensors.}\label{evalTens}
\end{listing}
If the tensor is to be evaluated for its $0$th index, we simply look up the corresponding value in the top level of the forest. In any other case, we first shift the corresponding level of the forest that is to be evaluated to the front to then again proceed as described in the prior case. Evaluating a tensor for a particular value of one of its indices then return the appropriate sub tensor. 

Up to now, we only treated tensors with a single type of indices. For instance, in the example of figure \ref{ExampleTens}, the tensor only had contravariant spacetime indices.
Formulated more rigorously, we only considered the tensor algebra over one specific vector space $V$.
We can however easily generalize the above to the case of not only incorporating the dual to $V$, $V^{\ast}$ to obtain the usual notion of contravariant and covariant tensors, but we can even lift the above functionality to the free tensor algebra over finitely many vector spaces over the same field and their duals $\{V_1,...,V_n,V_1^{\ast},...,V_n^{\ast}\}$:
\begin{align}
    \mathfrak{T}(V_1,...,V_n) := \bigoplus_{r_1,s_1,...,r_n,s_n \geq 0}T^{r_1}_{s_1}V_1 \otimes ... \otimes T^{r_n}_{s_n}V_n.
\end{align}
The elements of this algebra are consequently tensors with $n$-different index types --- in the following formula the type of an index is denoted by an superscripted $(i)$ --- with the $i$th type running over $\mathrm{dim}(V_i)$ and an arbitrary number of indices of each of those types appearing both in contravariant and in covariant position:
\begin{align}
    \mathfrak{T}(V_1,...,V_n) \ni T^{A^{(1)}_1 ... A^{(1)}_{r_1} ... A^{(n)}_1 ... A^{(n)}_{r_n}}
    _{B^{(1)}_1 ... B^{(1)}_{s_1} ... B^{(n)}_1 ... B^{(n)}_{s_n}}.
\end{align}
The rank of such a tensor is represented as a $2n$-tuple of natural numbers. The grading of $\mathfrak{T}(V_1,...,V_n)$ is then provided by this n-tuples, more precisely the individual tensors are labeled by their rank and the operation of taking tensor products is reflected by the component wise addition of these labels. Most important, the rank label will be encoded in the type of such tensors.

We start by generalizing the relevant notions to the treatment of covariant indices.
This is simply achieved by additionally appending tensors of the same index type, that thus represent the particular covariant sub tensors, as values, i.e., as leafs
to the first tensor:
\begin{center}
\begin{cminted}{haskell}
type Tensor2 n1 n2 k v = Tensor n1 k (Tensor n2 k v)
\end{cminted}
\end{center}
Thus \mintinline{haskell}{Tensor2 n1 n2 k v} describes as before a contravariant tensor of rank \mintinline{haskell}{n1}, but this time with values being each provided by an additional covariant tensor of rank \mintinline{haskell}{n2}. In particular, the two index types are the same as of course, the new covariant indices run over the same index range as the contravariant counterparts. The pair \mintinline{haskell}{(n1,n2)} is then merely the usual rank of the tensor.

Note that in the definition of most of the above functions, the types that tensors might store as values were constrained to be instances of \mintinline{haskell}{TScalar} and \mintinline{haskell}{TAlgebra}. 
In order to be able also to use these functions for  \mintinline{haskell}{Tensor2}, i.e., for the case where the stored values are itself tensors, it is necessary that the \mintinline{haskell}{Tensor} type itself provides an instance of these two type classes. In other words, we must now define the necessary functions that make the data type
\mintinline{haskell}{Tensor n k v} an instance of \mintinline{haskell}{TAdd} (\ref{TensTAdd}) and \mintinline{haskell}{Prod} (\ref{TensProd}):

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
instance (TIndex k, TAdd v) => TAdd (Tensor n k v) where
    addS = (&+)
    negateS = negateTens
    scaleZero = \case
                    ZeroTensor -> True
                    _          -> False
\end{minted}
\caption{Addition Type Class Instance of The Tensor Type.}\label{TensTAdd}
\end{listing}

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
instance (TIndex k, Prod v v') => 
    Prod (Tensor n k v) (Tensor n' k v') where
        type TProd (Tensor n k v) (Tensor n' k v') = 
            Tensor (n+n') k (TProd v v')
        prod = (&*)
\end{minted} 
\caption{Product Type Class Instance of the Tensor Type.}\label{TensProd}
\end{listing}

Now given this data type \mintinline{haskell}{Tensor2 n1 n2 k v} that represents a standard tensor with not only contravariant indices but also covariant ones we can supplement the previous tensor algebra functions by implementing a notion of contracting such a tensor in two of its indices.
We specify the two indices that shall be contracted by their position in the list of contravariant and covariant indices respectively. The contraction can then be achieved as displayed in the listing \ref{TensorContr}.

\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
tensorContr :: (TIndex k, TAdd v) => (Int,Int) -> Tensor2 n1 n2 k v
               -> Tensor2 (n1-1) (n2-1) k v
tensorContr (0,j) t = fromListT tensList
    where
        l = map (\(x,y) -> (x, toListT y)) $ toListT t
        l2 = map (\(x,y) -> (tailInd x,mapMaybe (removeContractionInd j
            (headInd x)) y)) l
        l3 = filter (\(_,y) -> not (null y)) l2
        tensList = map (\(x,y) -> (x, fromListT y)) l3
tensorContr (i,j) (Tensor m) = Tensor $ mapTMap (tensorContr (i-1,j)) m
tensorContr inds ZeroTensor = ZeroTensor
tensorContr inds (Scalar s) = error "cannot contract scalar!"
\end{minted} 
\caption{Contraction of a Tensor.}\label{TensorContr}
\end{listing}

The integer pair \mintinline{haskell}{(Int,Int)} labels the two index positions. If the first integer is not zero, we traverse the tensor forest until we reach the appropriate level specified by it. Next, the remaining sub tensors are all flattened to lists of indices value pairs. In each such list, we filter those indices value pairs that admit equal values for the two indices that are to be contracted and hence contribute when during the contraction, the two indices are set equal. Then these two indices are removed such that the pairs with equal values of the two contraction indices now all have the same indices. This is all done by the function \mintinline{haskell}{removeContractionInd}. Finally, we reconstruct the individual sub tensors from the flattened lists ensuring that the values of these pairs with the same indices are summed up and thus yield the correct value of the contracted tensor. 

Note that with this framework, there are no limitations regarding the number of indices that one might possibly use. For instance, we can now easily construct a data type for tensors that are described by two different types of indices each one appearing in contravariant and covariant fashion by appending the two different \mintinline{haskell}{Tensor2} types:
\begin{center}
\begin{cminted}{haskell}
type AbsTensor4 n1 n2 n3 n4 k1 k2 v = AbsTensor2 n1 n2 k1 
                                     (Tensor2 n3 n4 k2 v)
\end{cminted}
\end{center}
where \mintinline{haskell}{AbsTensor2 n1 n2 k v = Tensor2 n1 n2 k v} is simply a type synonym. In the sparse-tensor library, we explicitly provided type synonyms for tensors that take up to 4 different indices each one appearing in contravariant and covariant position. This type is then called:
\begin{center}
\begin{cminted}{haskell}
type AbsTensor8 n1 n2 n3 n4 n5 n6 n7 n8 k1 k2 k3 k4 v
\end{cminted}
\end{center}
Further types can easily be defined once they are needed.
We also employed the convention that functions that are defined for tensors with n different index types, counting both covariant and contravariant indices, are labeled with the respective number of indices n as the last letter in the function name.

It is important to note that as the tensor type itself is an instance of \mintinline{haskell}{TAdd} and \mintinline{haskell}{Prod} the tensor algebra operations \mintinline{haskell}{(&+),(&-),(&.),(&*)} are always the same no matter how many different indices the tensor at hand uses. Furthermore, also the symmetrization, transposition and the contraction still work exactly the same the only thing that one additionally needs to specify now is for which index type the function should be applied.

This can be done by noting that \mintinline{haskell}{fmap} takes a function and applies it to the leaves of a given tensor. Thus applying fmap successively several times we can decent a fixed number of tensor levels in the forest. This can be used to apply functions to tensors that are stored as leaves of other tensors. We provide the following example (\ref{mapTo3}) of a function that precisely descents $3$ tensor levels.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
mapTo3 :: (v1 -> v2) -> AbsTensor3 n1 n2 n3 k1 k2 v1 -> 
                        AbsTensor3 n1 n2 n3 k1 k2 v2
mapTo3 = fmap . fmap . fmap
\end{minted}
\caption{Descending 3 Tensor Levels.}\label{mapTo3}
\end{listing}
Using this, we can, for instance, symmetrize a given tensor in the covariant indices of the second index type by the function (\ref{ASymDeep}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG!40]{haskell}
symATens5 :: (TIndex k1, TIndex k2, TIndex k3, TAdd v) =>
             (Int,Int) ->
             AbsTensor5 n1 n2 n3 n4 n5 k1 k2 k3 v ->
             AbsTensor5 n1 n2 n3 n4 n5 k1 k2 k3 v
symATens5 = mapTo4 . symTens
\end{minted} 
\caption{Anti-Symmetrization of Indices in the Fourth Tensor Level. }\label{ASymDeep}
\end{listing}
Similarly, all other functions that we have encountered so far can be defined for tensors of arbitrarily many different indices as well. In the sparse-tensor library we explicitly provided all involved functions for all possible index sets that are included in the \mintinline{haskell}{AbsTensor8} type. If one wishes even to use more different indices, functions that are defined for a single index type can easily be lifted to act on the various different indices by using \mintinline{haskell}{fmap} the appropriate number of times. 

This finally covers the most basic functionality that is provided by the sparse-tensor Haskell library. It, however, allows for much more tensor functions that can be used to treat a large number of tensor algebra related problems that arise in theoretical mathematical physics. 
Further details can be found in \cite{sparse-tensor} more specifically in the \textit{Tensor} sub module.

\section{Generation of Lorentz Invariant Basis Tensors}\label{LorentzGen}
Amongst the entire functionality provided by the developed Haskell library, the generation of Lorentz invariant basis tensors requires the most careful discussion. We have already seen that any such Lorentz invariant tensor must be constructed solely from the Minkowski metric and the Levi-Civita symbol both possibly occurring with upper and lower index position. For the following discussion, we restrict to the case where the Lorentz invariant tensor only possesses contravariant spacetime indices. Other cases can obviously be treated analogously.

Any general, such Lorentz invariant tensor is thus given by a linear combination of sums of products that are formed solely from $\eta^{ab}$ and $\epsilon^{abcd}$.
We will simply call the individual terms of such a linear combination \textit{\textbf{ansätze}}. Note that each ansatz necessarily features the same symmetry that is required from the Lorentz invariant tensor. The number of factors in the individual products that are included in the various ansätze is obviously solely determined by the required rank of the Lorentz invariant tensor.

Due to the well known identity $\epsilon^{abcd}\epsilon^{efgh} = 24 \eta^{[a\vert e}\eta^{\vert b \vert f}\eta^{\vert c \vert g}\eta^{\vert d] h}$ we only need to treat the two cases that in the individual products either feature  no contribution from $\epsilon^{abcd}$ or have precisely one $\epsilon^{abcd}$ included. Any other case then reduces to one of these as we can use the identity to pairwise eliminate Levi-Civita symbols by means of Minkowski metrics. 

As we do not only want to construct the most general Lorentz invariant tensor possible, by means of including all possible ansätze in the linear combination, but also want the individual ansätze to form a basis for the space of such Lorentz invariant tensors of given rank and symmetry, we obviously need to get rid of linear dependencies amongst the individual ansätze. Clearly, a set of ansätze is linearly dependent if we find a non-trivial linear combination of them that yields zero. 

There is, however, an additional dimension dependent mechanism that generates further linear dependencies between ansätze that are at first sight not as obvious. Such additional linear dependencies might occur for instance if due to the involved required symmetry of the ansätze we can construct linear combinations of ansätze that are not strictly zero but yield an expression that is totally antisymmetric in $5$ or more of its indices. As we are working in $4$ spacetime dimension, such an expression evaluates to zero on all possible index combinations. Thus the ansätze in consideration are in fact linearly dependent, although at first glance they might not seem to be so. 

In order to distinguish such additional linear dependencies that are generated in this fashion, from the former case we introduce the following terminology: We call a set of ansätze \textit{\textbf{algebraically linearly dependent}} if there exists a non-trivial linear combination of these that yields identical zero. In particular, when investigating algebraic linear dependencies, there is no need to evaluate the components of the expressions explicitly. Algebraic linear dependency is therefore completely  independent from the given dimension that we work in. We call it \textbf{\textit{numerically linearly dependent}} if there exists a non-trivial linear combination that vanishes when evaluated on all possible index combinations. This now clearly is a dimension dependent notion as for instance an expression that is totally antisymmetric in $5$ indices evaluates to zero on all possible index combinations when working in $4$ dimensions, it, however, does not so if working in $5$ dimensions or higher. Obviously, every algebraically linearly dependent set of ansätze is also numerically linearly dependent. The converse is, however, clearly not true. 

The above considerations are best seen when working out a particular example: We take the two expressions $\epsilon^{abcd} \eta^{pq}$ and $\epsilon^{abcp} \eta^{dq}$ and symmetrize s.t. the expressions admit the area metric symmetry in $abcd$, i.e., are antisymmetric in $ab$ and $cd$ and additionally obey the block symmetry $(ab) \leftrightarrow (cd)$. Doing so we get the two ansätze: 
\begin{itemize}
\item[(i)] $\epsilon^{abcd} \eta^{pq}$ 
\item[(ii)] $\epsilon^{abcp} \eta^{dq} - \epsilon^{abdp} \eta^{cq} + \epsilon^{cdap} \eta^{bq} - \epsilon^{cdbp} \eta^{aq}$.
\end{itemize}
Clearly, these two ansätze are not algebraically linearly dependent. Subtracting the second ansatz form the first one we nevertheless find that the result is totally antisymmetric in the indices $abcdp$ and thus evaluated for any possible index combination yields zero. Thus the two expressions are numerically linearly dependent.

Summing up, if the goal not only consists of computing the most general Lorentz invariant tensor of given symmetry but if the individual building blocks the ansätze are further required to be linearly independent it does not suffice to take into account algebraic linear dependencies, but we also have to ensure that the ansätze are numerically linearly independent.  

In order to achieve performance, it is essential to pick suitable data structures for representing a linear combination of ansätze. As usual, the data structures must be tailored towards the specific needs that are in this case, the symmetrization of such linear combinations but also their explicit evaluation at specific index values.  
The individual tensors $\eta^{ab}$ and $\epsilon^{abcd}$ can simply be represented as displayed in (\ref{EtaType}).
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
data Epsilon = Epsilon {-# UNPACK #-} !Int {-# UNPACK #-} !Int
               {-# UNPACK #-} !Int {-# UNPACK #-} !Int
               deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData)

data Eta = Eta {-# UNPACK #-} !Int {-# UNPACK #-} !Int 
           deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData)

data Var = Var {-# UNPACK #-} !Int {-# UNPACK #-} !Int 
           deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData )
\end{minted} 
\caption{Data Types for Minkowski Metric, Levi-Civita Symbol, and Variables.}\label{EtaType}
\end{listing}
Note that the integer values of the data types \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon} are used to label the abstract spacetime indices, i.e., the first index will simply be labels by $1$ the second index by $2$, etc. in particular they do not refer to values that these indices might admit.
Furthermore, we also defined a basic variable data type to encode the parameters that will later multiply the individual ansätze to form a linear combination. The \mintinline{haskell}{Var}
data type simply takes two integer values where the first one refers to an integer factor that multiplies the variable\footnote{When using factor less symmetrization (for details see the discussion following (\ref{ansatzExample})) it actually turns out that it suffices to use integers for representing the factors in front of the different variables.} and the second one provides an identifier that labels the different variables. 

Using these data types, we can encode a general linear combination of ansätze (\ref{AnsForest}).
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
data AnsatzForestEta = ForestEta (M.Map Eta AnsatzForestEta)| Leaf !Var
                       | EmptyForest 
                       deriving (Show, Read, Eq, Generic, Serialize)

type AnsatzForestEpsilon = M.Map Epsilon AnsatzForestEta
\end{minted} 
\caption{Data Type representing Linear Combinations of Ansätze.}\label{AnsForest}
\end{listing}
Here \mintinline{haskell}{AnsatzForestEta} is the datatype of a linear combination of ansätze that each solely involve $\eta^{ab}$ whereas the individual ansätze encoded by \mintinline{haskell}{AnsatzForestEpsilon} all involve exactly one $\epsilon^{abcd}$ in each of their individual product.

Clearly, there exist no algebraic linear dependencies between ansätze that involve an $\epsilon^{abcd}$ and those that do not. 
It is important to note that there also cannot exist numeric linear dependencies that mix the two types of ansätze. The reason for this is that for a product of $\eta^{ab}$ to yield non-vanishing contributions when evaluated on a list of index values, i.e., setting each individual index to a value in $\{0,1,2,3 \}$ each value must occur an even number of times as the individual $\eta^{ab}$ factors have only diagonal entries. In contrast to that, evaluating a product of several Minkowski metrics and one $\epsilon^{abcd}$ one additionally needs each possible value for a spacetime index precisely once such that the Levi-Civita symbol yields a non-vanishing contribution. Thus for the types of ansätze that include $\epsilon^{abcd}$ to yield a non zero contribution, each value of the indices must occur an odd number of times.

In total, this shows that whenever an ansatz that features no Levi-Civita symbol evaluates to a non zero contribution all possible ansätze that incorporate an $\epsilon^{abcd}$ evaluate to zero and vice versa. Thus the different types of ansätze are also mutually numerically linearly independent. In particular, we see that the problem of finding a list of ansätze that constitute a basis for the space of Lorentz invariant tensors of given rank and symmetry decouples into the two subproblems of finding those that incorporate an $\epsilon^{abcd}$ and those that do not. The two types of ansätze can hence be treated completely independently. To that end, we also chose to represent them using different data types. 

The data type \mintinline{haskell}{Map k v} that is used in the definition of the two ansatz forest types represents a finite map between keys and values that is internally implemented as binary tree\footnote{Details regarding the implementation can be found in \cite{adams_1993}. The \mintinline{haskell}{Map k v} \cite{HackageMap} and many more datatypes can be found in Haskell's central package archive \cite{Hackage}.}. Thus the \mintinline{haskell}{AnsatzForestEta} data type is a forest with nodes being provided by an \mintinline{haskell}{Eta} value and leafs given by a \mintinline{haskell}{Var} value. Similarly, the \mintinline{haskell}{AnsatzForestEpsilon} type is given by such a forest with the difference that compared to \mintinline{haskell}{AnsatzForestEta} now the first level nodes are occupied by \mintinline{haskell}{Epsilon} values. Note that the individual sibling sets of these forests themselves are not provided by a "linear" data structure s.t. lists but by the binary structure of the \mintinline{haskell}{Map k v}. This enhances performance as some of the ansatz forests that we will treat have up to several thousand elements in a single sibling set, and the binary structure allows for faster insertion and lookup of elements. Such a binary structure, for instance, features $\mathcal{O}(\mathrm{log}(n)$ lookup whereas a single linked list in the worst case only provides $\mathcal{O}(n)$ lookup. 

Furthermore, when constructing an \mintinline{haskell}{AnsatzForestEta}, we will always ensure that parent nodes are smaller than all of their children, where the order relation is obtained by comparing the individual integers that are contained in a value of type \mintinline{haskell}{Eta}, starting with the first. All further functions will always produce such sorted forests when evaluated on sorted forests as input. 

The thus obtained ordered forest structure is not only strikingly performant when inserting and looking up individual ansätze and therefore also when adding two forests it is also particularly suited for evaluating a given linear combination of ansätze on specific index values. Note that only 4 out of 16 possible index value pairs yield a non zero contribution to an $\eta^{ab}$ and only 24 out of 256 4 tuples contribute to an $\epsilon^{abcd}$. Thus when explicitly evaluating an ansatz forest on specific index values, a large number of nodes will actually evaluate to zero. Further, note that the forest structure roughly speaking represents a fully factored product. Thus whenever a given node in the forest evaluates to zero, we do not have to evaluate the subforest that is attached to it as the whole expression is then multiplied by zero anyway. This observation allows for a maximally efficient and therefore, rapid evaluation of ansatz forest when implemented by using such tree-based data structure. 

To provide an example of the data types that are used to encode such expressions, we consider the following linear combination of ansätze:
\begin{multline}\label{AnsatzExprEx}
3a_1 \cdot \left (\eta^{ab}\eta^{cd}\eta^{ef} + \eta^{ab}\eta^{ce}\eta^{df} + \eta^{ab}\eta^{cf}\eta^{de} \right ) + a_2 \cdot \left ( \eta^{ac} \eta^{bd} \eta^{ef} + \eta^{ac} \eta^{be} \eta^{df} -2 \eta^{ad} \eta^{be} \eta^{cf} \right ) \\
+ a_3 \cdot \left ( \eta^{ad} \eta^{bc} \eta^{ef} - \eta^{ad} \eta^{bf} \eta^{ce} \right ) + a_4 \cdot \left ( \epsilon^{abcd} \eta^{ef} + \epsilon^{abce} \eta^{df}  \right )   .
\end{multline}
The corresponding representation using the \mintinline{haskell}{AnsatzForestEta} and \mintinline{haskell}{AnsatzForestEta} data types can be seen in figure \ref{AnsatzExprExForest}.
\begin{figure}
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG!40, very thick, minimum size=7mm},]
\node  (I0) at (0,2) {\large{\mintinline{haskell}{AnsatzForestEta}}};
\node[roundnode]  (I1) at (0,-2) {\mintinline{haskell}{Eta 1 2}};
\node[roundnode]  (I2) at (0,-5) {\mintinline{haskell}{Eta 1 3}};
\node[roundnode]  (I3) at (0,-8) {\mintinline{haskell}{Eta 1 4}};

\node  (I02) at (0,-13) {\large{\mintinline{haskell}{AnsatzForestEpsilon}}};
\node[roundnode]  (I4) at (1,-14.5) {\mintinline{haskell}{Epsilon 1 2 3 4}};
\node[roundnode]  (I5) at (1,-16.5) {\mintinline{haskell}{Epsilon 1 2 3 5}};


\node[roundnode]  (J1) at (4,2) {\mintinline{haskell}{Eta 3 4}};
\node[roundnode]  (J2) at (4,0) {\mintinline{haskell}{Eta 3 5}};
\node[roundnode]  (J3) at (4,-2) {\mintinline{haskell}{Eta 3 6}};
\node[roundnode]  (J4) at (4,-4) {\mintinline{haskell}{Eta 2 4}};
\node[roundnode]  (J5) at (4,-6) {\mintinline{haskell}{Eta 2 5}};
\node[roundnode]  (J6) at (4,-8) {\mintinline{haskell}{Eta 2 3}};
\node[roundnode]  (J7) at (4,-10) {\mintinline{haskell}{Eta 2 5}};
\node[roundnode]  (J8) at (4,-12) {\mintinline{haskell}{Eta 2 6}};
\node[roundnode]  (J9) at (8,-14.5) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (J10) at (8,-16.5) {\mintinline{haskell}{Eta 4 6}};


\node[roundnode]  (K1) at (8,2) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K2) at (8,0) {\mintinline{haskell}{Eta 4 6}};
\node[roundnode]  (K3) at (8,-2) {\mintinline{haskell}{Eta 4 5}};
\node[roundnode]  (K4) at (8,-4) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K5) at (8,-6) {\mintinline{haskell}{Eta 4 6}};
\node[roundnode]  (K6) at (8,-8) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K7) at (8,-10) {\mintinline{haskell}{Eta 3 6}};
\node[roundnode]  (K8) at (8,-12) {\mintinline{haskell}{Eta 3 5}};


\node  (L1) at (12,2) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L2) at (12,0) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L3) at (12,-2) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L4) at (12,-4) {\mintinline{haskell}{Leaf $ Var 1 2}};
\node  (L5) at (12,-6) {\mintinline{haskell}{Leaf $ Var 1 2}};
\node  (L6) at (12,-8) {\mintinline{haskell}{Leaf $ Var 1 3}};
\node  (L7) at (12,-10) {\mintinline{haskell}{Leaf $ Var -2 2}};
\node  (L8) at (12,-12) {\mintinline{haskell}{Leaf $ Var -1 3}};
\node  (L9) at (12,-14.5) {\mintinline{haskell}{Leaf $ Var 1 4}};
\node  (L10) at (12,-16.5) {\mintinline{haskell}{Leaf $ Var 1 4}};




\draw [-] (I1) -- (J1);
\draw [-] (I1) -- (J2);
\draw [-] (I1) -- (J3);
\draw [-] (I2) -- (J4);
\draw [-] (I2) -- (J5);
\draw [-] (I3) -- (J6);
\draw [-] (I3) -- (J7);
\draw [-] (I3) -- (J8);
\draw [-] (I4) -- (J9);
\draw [-] (I5) -- (J10);




\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);
\draw [-] (J7) -- (K7);
\draw [-] (J8) -- (K8);



\draw [-] (K1) -- (L1);
\draw [-] (K2) -- (L2);
\draw [-] (K3) -- (L3);
\draw [-] (K4) -- (L4);
\draw [-] (K5) -- (L5);
\draw [-] (K6) -- (L6);
\draw [-] (K7) -- (L7);
\draw [-] (K8) -- (L8);
\draw [-] (J9) -- (L9);
\draw [-] (J10) -- (L10);




\end{tikzpicture}
%make Maps at each level visible??
\caption{Forest Structure of Ansatz Linear Combination (\ref{AnsatzExprEx}). }
\label{AnsatzExprExForest}
\end{figure}

Due to the forest data structure, the addition of two such expressions can be implemented quite performant. One simply uses the \mintinline{haskell}{unionWith} function provided by the Data.Map.Strict package \cite{HackageMap}. The function combines two Maps calling a specified combiner function if a key is present in both. The addition of two ansatz forests (\ref{ForestAdd}) can now be achieved by defining the addition of the leaf values in an obvious way and further defining the addition of two ansatz forests that themselves contain sub forests by combining the corresponding Maps with combiner function being again the addition of ansatz forests. Doing so, one eventually recurses over the whole structure. 
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
addForests :: AnsatzForestEta -> AnsatzForestEta -> AnsatzForestEta
addForests ans EmptyForest = ans
addForests EmptyForest ans = ans
addForests (Leaf var1) (Leaf var2)
        | isZeroVar newLeafVal = EmptyForest
        | otherwise = Leaf newLeafVal
        where
            newLeafVal = addVars var1 var2
addForests (ForestEta m1) (ForestEta m2)
        | M.null newMap = EmptyForest
        | otherwise = ForestEta newMap
         where
            newMap = M.filter (/= EmptyForest) $
                     M.unionWith addForests m1 m2

addForestsEpsilon :: AnsatzForestEpsilon -> AnsatzForestEpsilon ->
                     AnsatzForestEpsilon
addForestsEpsilon m1 m2 = M.filter (/= EmptyForest) $ M.unionWith
                          addForests m1 m2
\end{minted} 
\caption{Addition of Ansatz Forests.}\label{ForestAdd}
\end{listing}\\

Symmetrization (\ref{PairSymFor}) of such an expression can be achieved by merely swapping or permuting individual index identifiers of the various \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon} values in the ansatz forest and then adding the result to the former ansatz forest.
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
pairSymForestEta :: (Int,Int) -> AnsatzForestEta -> AnsatzForestEta
pairSymForestEta inds ans = addForests ans $ swapLabelFEta inds ans

pairSymForestEps :: (Int,Int) -> AnsatzForestEpsilon ->
                    AnsatzForestEpsilon
pairSymForestEps inds ans = addForestsEpsilon ans $ 
                            swapLabelFEps inds ans
\end{minted} 
\caption{Pair Symmetrization of Ansatz Forests.}\label{PairSymFor}
\end{listing}
As swapping labels of forest nodes (\ref{SwapF}) will, in general, destroy the sorting of the forest, we have to reinforce the correct ordering afterward. This is simply done by flattening the forest to a list that contains the individual branches as pairs with the first entry being given by a list containing the individual nodes of the given branch and the second entry being the corresponding value. One then sorts the node list in each such pair according to the required ordering and finally reconstructs the forest from the list of branches.
Haskells lazy evaluation then ensures that this straight forward implementation is surprisingly fast.
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
swapLabelFEta :: (Int,Int) -> AnsatzForestEta -> AnsatzForestEta
swapLabelFEta inds ans = sortForest.canonicalizeAnsatzEta $ swapAnsatz
        where
            f = swapLabelEta inds
            swapAnsatz = mapNodes f ans

swapLabelFEps :: (Int,Int) -> AnsatzForestEpsilon -> AnsatzForestEpsilon
swapLabelFEps inds ans = sortForestEpsilon.canonicalizeAnsatzEpsilon $ 
                        swapAnsatz
        where
            f = swapLabelEpsilon inds
            swapAnsatz = mapNodesEpsilon f $ M.map 
                                            (swapLabelFEta inds) ans
\end{minted}
\caption{Swap Function for Ansatz Forests.}\label{SwapF}
\end{listing}

Similarly, one can easily define symmetrization functions for arbitrary other kinds of symmetry. We define a \mintinline{haskell}{Symmetry} data type that collects the most used such, i.e., (pair symmetries, pair anti-symmetries, block symmetries, cyclic symmetries, cyclic block symmetries):
\begin{center}
\begin{cminted}{haskell}
type Symmetry = ([(Int,Int)],[(Int,Int)],[([Int],[Int])],[[Int]],[[[Int]]])
\end{cminted}
\end{center}
The various integers refer to the indices that are to be symmetrized. We now can collect the individual symmetrizer functions in one overall function as displayed in (\ref{SymAns}).
\begin{listing}[hbt!] 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
symAnsatzForestEta ::Symmetry -> AnsatzForestEta -> AnsatzForestEta
symAnsatzForestEta (sym,asym,blocksym,cyclicsym,cyclicblocksym) ans =
    foldr cyclicBlockSymForestEta (
        foldr cyclicSymForestEta (
            foldr pairBlockSymForestEta (
                foldr pairASymForestEta (
                    foldr pairSymForestEta ans sym
                ) asym
            ) blockSymMap
        ) cyclicsym
    ) cyclicblocksym
    where
        blockSymMap = map swapBlockLabelMap blocksym

symAnsatzForestEps :: Symmetry -> AnsatzForestEpsilon ->
                      AnsatzForestEpsilon
symAnsatzForestEps (sym,asym,blocksym,cyclicsym,cyclicblocksym) ans =
      foldr cyclicBlockSymForestEps (
          foldr cyclicSymForestEps (
              foldr pairBlockSymForestEps (
                  foldr pairASymForestEps (
                      foldr pairSymForestEps ans sym
                  ) asym
              ) blockSymMap
          ) cyclicsym
      ) cyclicblocksym
      where
        blockSymMap = map swapBlockLabelMap blocksym
\end{minted} 
\caption{General Ansatz Forest Symmetrizer Function.}\label{SymAns}
\end{listing}

This summarizes the data structures that we use to represent the tensorial expression that we are going to construct. Further details and additional functions that were omitted here can be found in \cite{sparse-tensor}.  

With this choice of data structures at hand, we now proceed with the construction of Lorentz invariant basis tensors.  
This process can, in general, be divided into three steps:
\begin{itemize}
    \item[(i)] Generating all possible products built either solely from $\eta^{ab}$ or including exactly one $\epsilon^{abcd}$ that feature the required index structure.
    \item[(ii)] Symmetrizing the individual expressions according to the demanded symmetry to obtain all possible ansätze.
    \item[(iii)] Reducing the resulting expressions w.r.t. algebraic and numeric linear dependencies.
\end{itemize}

We start with the first step, computing all possible products. Labeling the indices by integers \mintinline{haskell}{[1,...,n]} one readily obtains all possible such individual products be exhausting all possible ways the indices can be arranged, taking into account the symmetries of $\eta^{ab}$, $\epsilon^{abcd}$ if present and the symmetries that are induced by the product structure. We get the list of all possible index orders for the products that only involve $\eta^{ab}$ by the function \mintinline{haskell}{getEtaInds}.
Similar, the list of all possible indices for the expressions that involve one $\epsilon^{abcd}$ can be computed by \mintinline{haskell}{getEpsilonInds}. The two functions are displayed in the listing \ref{AllInds}.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
getEtaInds :: [Int] -> [[Int]]
getEtaInds [a,b] = [[a,b]]
getEtaInds (x:xs) = concatMap res firstEta
        where
            firstEta = map (\y -> ([x,y],delete y xs)) xs
            res (a,b) = (++) a <$> getEtaInds b 

getEpsilonInds :: [Int] -> [[Int]]
getEpsilonInds inds = allInds
        where
            i = length inds 
            epsInds = [ [a,b,c,d] | a <- [1..i-3], b <- [a+1..i-2],
                      c <- [b+1..i-1], d <- [c+1..i] ] 
            allInds = map (\x -> map (x ++) $
                      getEtaInds (inds \\ x) ) epsInds 
\end{minted} 
\caption{Computation of All Possible Index Orders.}\label{AllInds}
\end{listing}

Note that in \cite{sparse-tensor} we further reduced these index lists by filtering already here some of the linear dependencies that will later arise due to the symmetrization. This is not necessary as we will treat linear dependencies later anyway. It does, however, increase performance as the lists that are consumed by the algorithm are then smaller from the very beginning.  

We could now proceed with the next step, express each individual product by the appropriate chosen data structure, symmetrize all products individually and then at the very end, reduce occurring linear dependencies. Following these lines, however, bears the problem that intermediate results, i.e., the entirety of the thus constructed ansätze occupies an unreasonable amount of memory. Only at the very end when linearly dependent ansätze are removed this memory can be freed again. In fact, in most practical cases, the memory that would be needed for this approach by far exceeds resources that are typically available.

It is thus best to combine the steps (ii) and (iii) that are outlined above. The algorithm then consumes the list of individual products step by step. For each product a distinction is made: if the product is already present in the ansatz forest it is simply rejected if it is not it is symmetrized to obtain the corresponding ansatz and then added to the ansatz forest. Doing so the intermediate memory swelling is avoided as now from the very beginning, only linearly independent ansätze are added to the forest.

For this approach, it is incredibly important to note that two individual products when symmetrized to the respective ansätze are either identically, or completely disjoint in the sense that they do not share a single common summand. This can be seen from the fact that the permutations of indices that are involved in the symmetrization procedure constitute a subgroup of the permutation group of appropriate size $S_n$ that acts on the products via permutation of their indices. The individual products that contribute to a given symmetrized ansatz then precisely constitute the corresponding orbit under the action of the symmetry subgroup. As orbits of any group action are disjoint, so are the ansätze. 

Thus at a given step of the algorithm, when deciding if a new product should be added to the ansatz forest or discarded, we do not need to symmetrize the individual product. If it is already present in the forest and thus linearly dependent on it, already the non-symmetrized individual product will be present if the individual product is not present also all further expressions that are obtained from symmetrizing it will not be present. Hence the decision whether or not an individual product should be added to the forest can be made without ever having to symmetrize the individual product. Obviously, when such a product is added to the forest, we still have to symmetrize it, but all products that are rejected can now be rejected without requiring an explicit symmetrization.

The above allows for a decisive criterion that given a list of possible products only incorporates those into the ansatz forest that do not possess algebraic linear dependencies amongst each other. When further reducing the ansatz forest w.r.t. numeric linear dependencies there are in principle two approaches how this can be achieved. We can either first use the above technique to construct an ansatz forest that is algebraically linearly independent, then evaluate this ansatz forest for enough index combinations to retrieve all independent components, write these in a matrix where the columns label the individual variables that occur and then use standard linear algebra to compute a basis of these column vectors. Variables that are not present in this basis can then be removed from the ansatz forest as these multiply exactly those ansätze that are linearly dependent. 

Alternatively when constructing the ansatz forest from a list of individual products we can not only check for algebraic linear dependencies when deciding whether or not a new product should be added but if a given product is algebraically  independent from the ansatz forest then also test for numeric linear dependencies. Doing so we would construct from the very beginning an ansatz forest that consists of not only algebraically but also numerically linearly independent ansätze.
It turns out that the first approach results in slightly faster computation times\footnote{At least for the construction of ansätze that exceed a certain size, i.e., a certain number of individual expressions that are involved in them.
For ansätze below that size, the two computation methods take almost the same time.}, whereas the second approach is superior in memory usage, thus we have implemented both algorithms. 
We start provided details regarding the first approach. 

\subsection*{Reducing linear dependencies I:  The fast way}

The idea is straight forward. Representing the individual products involving only $\eta^{ab}$ or also including one $\epsilon^{abcd}$ as lists with the individual list elements representing the factors of the product, we need a function that decides whether or not such a product is already present in a given ansatz forest (\ref{ForestElem}). As we are always dealing with ordered forests and also sorted products we can already conclude that a given product is missing in the forest if any given node is missing in the appropriate level of the forest. Thus only when a product is actually present in the forest, we really need to decent up to the leaf values of the forest.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
isElem :: [Eta] -> AnsatzForestEta -> Bool
isElem [] (Leaf x) = True
isElem x EmptyForest = False
isElem  (x:xs) (ForestEta m) = case mForest of
                                Just forest -> xs `isElem` forest
                                _           -> False
            where
                mForest = M.lookup x m

isElemEpsilon :: (Epsilon, [Eta]) -> AnsatzForestEpsilon -> Bool
isElemEpsilon (eps,l) m = case mForest of
                            Just forest -> l `isElem` forest
                            _           -> False
            where
                mForest = M.lookup eps m  
\end{minted} 
\caption{Lookup Function for Ansatz Forests.}\label{ForestElem}
\end{listing}

Using these two functions, we simply define two functions that, when a product is missing, symmetrize it to obtain the corresponding ansatz and then add it to the forest, when the product is already present, it is dismissed. These functions are then folded over the list of products taking an empty forest as start value.  Doing this we obtain functions that take a list of products and a value of type \mintinline{haskell}{Symmetry} and constructs from it the two ansatz forests with ansätze being obtained by symmetrizing the various products that are provided by the list. Further, these two functions now ensure that all ansätze that are included in the forests are algebraically linearly independent. 

These two functions can then again be used to define the final functions that solely take the required symmetry and the rank of the to be constructed Lorentz invariant basis tensors as argument and then compute from it an ansatz forest of all possible algebraically linearly independent ansätze that feature the correct number of indices and symmetries. We simply use the priorly defined functions \mintinline{haskell}{getEtaInds} and \mintinline{haskell}{getEpsInds} to compute lists of all possible indices of products of either solely Minkowski metrics or also including one Levi-Civita symbol, all featuring the required number of indices. These two lists are then transformed to lists of the appropriate types \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon}, and each individual product is further combined with a different variable. The two lists are reduced as described above, by invoking the two functions \mintinline{haskell}{reduceAnsatzEta'} and \mintinline{haskell}{reduceAnsatzEpsilon'} (\ref{redFast}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
reduceAnsatzEta' :: Symmetry -> [([Eta],Var)] -> AnsatzForestEta
reduceAnsatzEta' sym = foldl' addOrRem' EmptyForest
        where
            addOrRem' f ans = if isElem (fst ans) f then f else
                              addForests f (symAnsatzForestEta sym 
                              $ mkForestFromAscList ans)

reduceAnsatzEpsilon' :: Symmetry -> [(Epsilon, [Eta], Var)] ->
                        AnsatzForestEpsilon
reduceAnsatzEpsilon' sym = foldl' addOrRem' M.empty
        where
            addOrRem' f (x,y,z) = if isElemEpsilon (x,y) f then f else
                                  addForestsEpsilon f 
                                  (symAnsatzForestEps sym 
                                  $ mkForestFromAscListEpsilon (x,y,z))  
\end{minted} 
\caption{Reduction of Ansatz Forests.}\label{redFast}
\end{listing}
Finally, the variables in the two ansatz forests that are thus constructed are relabeled. The two functions that construct the two ansatz forests with algebraic linear dependencies removed are displayed in listing \ref{ConstrFast}.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
getEtaForestFast :: Int -> Symmetry -> AnsatzForestEta
getEtaForestFast ord syms = relabelAnsatzForest 1 $ reduceAnsatzEta' syms 
                            allForests
            where
                allInds = getEtaInds [1..ord] syms
                allVars = mkAllVars
                allForests = zipWith mkEtaList' allVars allInds

getEpsForestFast :: Int -> Symmetry -> AnsatzForestEpsilon
getEpsForestFast ord syms = if ord < 4 then M.empty else
relabelAnsatzForestEpsilon 1 $ reduceAnsatzEpsilon' syms allForests
            where
                allInds = getEpsilonInds [1..ord] syms
                allVars = mkAllVars
                allForests = zipWith mkEpsilonList' allVars allInd
\end{minted} 
\caption{Construct Ansatz Forests: the "Fast" Way.}\label{ConstrFast}
\end{listing}

The remaining work consists of reducing the two constructed ansatz forests w.r.t. numeric linear dependencies. This is best achieved when explicitly retrieving the values that a given ansatz forest admits for all possible values that one might insert for its indices. It is important to note that we do not need to evaluate a given ansatz forest for all possible index values as due to symmetries that might be present we already know that evaluating the ansatz forest on two different lists of index values that are connected via the symmetry we will obtain the same value. Thus we can avoid much work by only evaluating the ansatz forest on one representative out of the equivalence classes that are generated by identifying index lists that are connected via a symmetry. 

We can further reduce the number of necessary evaluations by using the priorly noted fact that any product of etas and therefore, in particular, any \mintinline{haskell}{AnsatzForestEta} can only yield non zero values when evaluated on a list of index values with each value occurring an even number of times. In contrast to that, when evaluating an \mintinline{haskell}{AnsatzForestEpsilon}, we can safely restrict to those evaluation lists that have each index value appearing an odd number of times.

Finally, it is worth noting that the value that we retrieve when evaluating an ansatz forest on a specific index value list at most changes by a sign under arbitrary relabeling of the coordinate axes. Thus we might further reduce the number of necessary evaluations by selecting one index value list out of every set of index value lists that are mutually related by index relabeling. The reason why all ansätze admit this exceptional property, i.e., their values at most changes by a sign under arbitrary coordinate relabeling becomes obvious by recalling that all possible expressions that are built solely from $\eta^{ab}$ and $\epsilon^{abcd}$ are Lorentz invariant. Up to a sign the relabeling of coordinate axes clearly defines a Lorentz transformation, as obviously all those relabeling that keep $x_0$ fixed also keep $\eta_{ab}$ invariant, those that interchange $x_0$ and $x_{\alpha}$ keep eta invariant up to a sign where the sign depends on the precise number of swaps between $x_0$ and spatial coordinates. Hence the fact that the ansatz forests are up to a sign invariant under such coordinate relabeling is no surprise. 

Summing up all of the above considerations can be used to reduce the number of evaluations that are necessary to remove numerical linear dependencies. Clearly employing such techniques is not required as we could also simply evaluate the ansatz forest for all possible index value list. Using the above, however, severely reduces the computation time of the computer program. 
Storing the information regarding which index is set to which value in a \mintinline{haskell}{IntMap Int} \cite{HackageIntMap} (for details regarding the implementation see also \cite{Okasaki98fastmergeable}), i.e., a special of a map type that is tailored towards integer keys, the evaluation of a given forest can be obtained as described in (\ref{EvalOne}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
evalAnsatzForestEta :: I.IntMap Int -> AnsatzForestEta -> I.IntMap Int
evalAnsatzForestEta evalM (Leaf (Var x y)) = I.singleton y x
evalAnsatzForestEta evalM (ForestEta m) = M.foldlWithKey' foldF I.empty m
    where
        foldF b k a = let nodeVal = evalNodeEta evalM k
                      in if isNothing nodeVal then b
                         else I.unionWith (+)
                              (I.map (fromJust nodeVal *)
                              (evalAnsatzForestEta evalM a)) b
evalAnsatzForestEta evalM EmptyForest = I.empty

evalAnsatzForestEpsilon :: I.IntMap Int -> AnsatzForestEpsilon ->
                           I.IntMap Int
evalAnsatzForestEpsilon evalM = M.foldlWithKey' foldF I.empty
    where
        foldF b k a = let nodeVal = evalNodeEpsilon evalM k
                      in if isNothing nodeVal then b
                         else I.unionWith (+) 
                              (I.map (fromJust nodeVal *)
                              (evalAnsatzForestEta evalM a)) b  
\end{minted} 
\caption{Evaluation of Ansatz Forests.}\label{EvalOne}
\end{listing}

The \mintinline{haskell}{IntMap Int} that is returned as a result in the above functions encodes the linear combinations of variables that are obtained when evaluating an ansatz forest, i.e., the integer keys of the int map represent the variable labels and the values the corresponding factors in the linear combination. 
When evaluating a given ansatz forest for multiple index value lists, we can gain further performance improvements by noting that the list of all evaluation int maps can, in fact, be divided in chunks that then can be processed parallel. This is achieved by the functions \mintinline{haskell}{evalAllEta} and \mintinline{haskell}{evalAllEpsilon}. Details can be seen in \cite{sparse-tensor}

Once we have evaluated an ansatz forest for all necessary index value lists, we can store the retrieved values in the form of a matrix as described before with the columns labeling the individual variables and the rows labeling the index combination values that we evaluated the ansatz forest on. The ansatz forest can now be reduced w.r.t. numerical linear dependencies by removing linearly dependent column vectors from this matrix. To that end, we use Haskell bindings \cite{HackageEigen} to the C++ linear algebra library Eigen \cite{eigenweb}. Using Eigen subroutines, one can readily reduce linear dependencies amongst the column vectors a given matrix, for instance, by means of an LU decomposition. Finally, once the matrix is reduced, we can proceed by removing all branches with leaf variables that correspond to columns that have been removed from the matrix. This then also removes the numerically linearly dependent ansätze from the ansatz forest (\ref{RedNumLinFast}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
reduceLinDepsFastEta :: [I.IntMap Int] -> Symmetry ->
                        AnsatzForestEta -> AnsatzForestEta
reduceLinDepsFastEta evalM symL ansEta = newEtaAns
        where
            etaL = evalAllEta evalM ansEta
            etaVars = getPivots etaL
            allEtaVars = getForestLabels ansEta
            remVarsEta =  allEtaVars \\ etaVars
            newEtaAns = relabelAnsatzForest 1 $
                        removeVarsEta remVarsEta ansEta

reduceLinDepsFastEps :: [I.IntMap Int] -> Symmetry ->
                        AnsatzForestEpsilon -> AnsatzForestEpsilon
reduceLinDepsFastEps evalM symL ansEps = newEpsAns
        where
            epsL = evalAllEpsilon evalM ansEps
            epsVars = getPivots epsL
            allEpsVars = getForestLabelsEpsilon ansEps
            remVarsEps =  allEpsVars \\ epsVars
            newEpsAns = relabelAnsatzForestEpsilon 1 $
                        removeVarsEps remVarsEps ansEps 
\end{minted} 
\caption{Reduction of Numeric Linear Dependencies: the "Fast" Way.}\label{RedNumLinFast}
\end{listing}

This finishes the first, computation time optimized method of constructing a basis of the space of Lorentz invariant tensors of given rank and symmetry that we have implemented in the sparse-tensor library. In order to use the thus computed result, the basis of Lorentz invariant tensors together with the previously presented functionality of the sparse-tensor package we also provide functions that do not only construct the ansatz forests but also return the computed Lorentz invariant basis tensor in the form of a compatible tensor data type. We represent the occurring variables that then necessarily also occur in the components of the tensors as int maps:
\begin{center}
\begin{cminted}{haskell}
newtype AnsVar a = AnsVar (I.IntMap a) deriving (Show, Generic,Serialize)

type AnsVarR = AnsVar (SField Rational)
\end{cminted}
\end{center}
Further, we introduce type synonyms for the typical used tensors types, the usual spacetime tensors that feature contravariant and covariant indices ranging between $0$ and $3$:
\begin{center}
\begin{cminted}{haskell}
type STTens n1 n2 v = AbsTensor2 n1 n2 Ind3 v
\end{cminted}
\end{center}
Moreover, also a more general tensor type that we heavily used for the area metric computations. This tensor type features 3 different index types each appearing in contravariant and covariant position. The first index type ranges over the area metric degrees of freedom and thus ranges from $0$ to $20$, the second index type labels second derivative pairs, and hence runs form $0$ to $9$, and the last type is again the usual spacetime index:
\begin{center}
\begin{cminted}{haskell}
type ATens n1 n2 n3 n4 n5 n6 v = 
     AbsTensor6 n1 n2 n3 n4 n5 n6 Ind20 Ind9 Ind3 v
\end{cminted}
\end{center}
The final functions (\ref{mkAnsatzFast1}), (\ref{mkAnsatzFast2}), (\ref{mkAnsatzFast3}) that use the computation time optimized method and return the result as a triplet of the two ansatz forest and a tensor that collects all their values are.

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorFastSym :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                         [[Int]]-> (AnsatzForestEta, AnsatzForestEpsilon,
                         STTens n 0 (AnsVarR))
mkAnsatzTensorFastSym ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMaps symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens =
            evalToTensSym symmetries evalMEtaInds evalMEpsInds
                          ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 1.1: with Explicit Symmetrization.}\label{mkAnsatzFast1}
\end{listing}

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--without explicit symmetriization in tens
mkAnsatzTensorFast :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                      [[Int]]-> (AnsatzForestEta, AnsatzForestEpsilon,
                      STTens n 0 (AnsVarR))
mkAnsatzTensorFast ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMaps symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTens evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 1.2: without Explicit Symmetrization.}\label{mkAnsatzFast2}
\end{listing}

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--evaluation to a tensor that uses multiple index types
mkAnsatzTensorFastAbs :: Int -> Symmetry ->
                         [([Int], Int, [IndTupleAbs n1 0 n2 0 n3 0])] ->
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         ATens n1 0 n2 0 n3 0 (AnsVarR))
mkAnsatzTensorFastAbs ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMapsAbs symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTensAbs evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 1.3: Evaluation to Custom Indices.}\label{mkAnsatzFast3}
\end{listing}

The first function takes as arguments the order of the to be constructed Lorentz invariant basis, i. the number of indices, the symmetry and also a list of all index value lists that are necessary for the evaluation. The evaluation list must contain precisely one representative out of every set of index value lists that are the same under the present symmetry. Further reduction, as described above, must not be incorporated manually as this is achieved by the function \mintinline{haskell}{mkAllEvalMaps}.
The first function returns the Lorentz invariant basis as explicitly symmetrized spacetime tensors.

Note that for higher-ranked tensors and more complicated symmetries the explicit symmetrization of the tensors might be expensive. Furthermore often it suffices to only store one representative of each symmetry equivalence class in the tensor, for instance, if the tensor is contracted against symmetric objects that thus enforce the symmetries in later computations. To that end, we also provided a function that returns the tensors without explicit symmetrization.

Finally the last function does not require a list of index value lists as input but needs a list of triples each consisting of the index value list, the multiplicity of the given index value list that is computed as product of the individual multiplicities of the abstract indices used (for the definition of the multiplicity $\sigma$ see the discussion following definition \ref{interDef}), and also a list of all abstract index tuples that correspond to the present index value list. How these individual ingredients can be computed in detail will be discussed in the following section when we consider an example. The last function then computes a triple consisting of the two ansatz forest and an abstract tensor that uses multiple indices to store the corresponding values. 

Additionally, we also provide the first two of the above three functions in a form that does not require to specify the evaluation list explicitly. Paying the price of a slightly more expensive computation in the following two functions (\ref{mkAnsatzFast'1}) and (\ref{mkAnsatzFast'2}) the evaluation list is constructed fully automatically from the symmetries and the rank of the given tensor.

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorFastSym' :: forall (n :: Nat). SingI n => Int ->
                          Symmetry -> (AnsatzForestEta,
                          AnsatzForestEpsilon,
                          STTens n 0 (AnsVarR))
mkAnsatzTensorFastSym' ord symmetries = mkAnsatzTensorFastSym
                                            ord symmetries evalL
    where
        evalL = filter (`filterAllSym` symmetries) 
                        $ allList ord symmetries
\end{minted} 
\caption{Ansatz Construction 1.4: with Explicit Symmetrization, no Evaluation List Required.}\label{mkAnsatzFast'1}
\end{listing}

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorFast' :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                       (AnsatzForestEta, AnsatzForestEpsilon,
                       STTens n 0 (AnsVarR))
mkAnsatzTensorFast' ord symmetries = mkAnsatzTensorFast
                                        ord symmetries evalL
    where
        evalL = filter (`filterAllSym` symmetries) 
                        $ allList ord symmetries
\end{minted} 
\caption{Ansatz construction 1.5: without Explicit Symmetrization,  no Evaluation List Required.}\label{mkAnsatzFast'2}
\end{listing}

\subsection*{Reducing linear dependencies II:  The memory optimized way}

The main idea of this second way of reducing the ansatz forest w.r.t linear dependencies is to construct an ansatz forest that is not only algebraically but also numerically linearly independent from the very beginning. This can be achieved as follows: As before the algorithm consumes a list of all possible individual products and checks in any given step if the product at hand is missing in the ansatz forest and thus represents an ansatz that is algebraically linearly independent or is already present and can hence be rejected. In contrast to the previous approach now every time a product is missing, we immediately evaluate the corresponding ansatz for all necessary index value lists and check if the ansatz is numerically linearly dependent or not. Only if the new ansatz is also numerically linearly  independent from the present ansatz forest, we add it to the forest.

It is thus crucial that in each step we do not only provide input data that consists of the present ansatz forest which is needed for deciding whether or not the new ansatz is algebraically linearly independent but also include its fully evaluated matrix --- this time with rows labeling the individual variables that are present in the forest and columns labeling the several index value list.
Doing so when deciding whether or not the newly evaluated ansatz is numerically  independent from the ansatz forest, we do not have to evaluate the forest each time. Any time a  new ansatz is added to the forest, it is also added to the matrix that contains all independent components of the forest.

This total procedure is accomplished by the two functions displayed in (\ref{AddorDisc}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
addOrDiscardEtaEig :: Symmetry -> [I.IntMap Int] ->
                      (AnsatzForestEta, RankDataEig) -> 
                      [Eta] -> (AnsatzForestEta, RankDataEig)
addOrDiscardEtaEig symList evalM (ans,rDat) etaL
            | isElem etaL ans = (ans,rDat)
            | otherwise = case newRDat of
                               Nothing          -> (ans,rDat)
                               Just newRDat'    -> (sumAns,newRDat')
             where
                newAns = getNewAns symList etaL rDat
                newRDat = getNewRDat evalM newAns rDat
                sumAns = addForests ans newAns

addOrDiscardEpsilonEig :: Symmetry -> [I.IntMap Int] ->
                          (AnsatzForestEpsilon, RankDataEig) ->
                          (Epsilon,[Eta]) ->
                          (AnsatzForestEpsilon, RankDataEig)
addOrDiscardEpsilonEig symList evalM (ans,rDat) (epsL,etaL)
            | isElemEpsilon (epsL,etaL) ans = (ans,rDat)
            | otherwise = case newRDat of
                               Nothing          -> (ans,rDat)
                               Just newRDat'    -> (sumAns,newRDat')
             where
                newAns = getNewAnsEps symList epsL etaL rDat
                newRDat = getNewRDatEps evalM newAns rDat
                sumAns = addForestsEpsilon ans newAns
\end{minted} 
\caption{Add or Discard a new Ansatz.}\label{AddorDisc}
\end{listing}
The type alias \mintinline{haskell}{RankDataEig} represents the ansatz forest data that is necessary for deciding whether or not a new ansatz is numerically linearly dependent on the current ansatz forest and is given by a tuple that consists of one Eigen matrix and one sparse Eigen Matrix:
\begin{center}
\begin{cminted}{haskell}
type RankDataEig = (Mat.MatrixXd, Sparse.SparseMatrixXd)
\end{cminted}
\end{center}
Note that usually, the number of necessary evaluation index value lists by far exceeds the number of variables that are present in a given ansatz forest. Thus the chosen way of evaluating a given forest to a matrix yields a matrix with a large number of columns compared to a small number of rows. In the following, we will refer to this matrix as $M$. The second matrix that is included in \mintinline{haskell}{RankDataEig} is precisely this matrix $M$. To save memory, this matrix is stored in a sparse format. The first matrix in \mintinline{haskell}{RankDataEig} is given by $M M^t$. This matrix is used to achieve performance when computing whether or not a new ansatz is numerically linearly dependent on the current ansatz forest or not. To that end, note that $\mathrm{rank}(MM^t) = \mathrm{rank}(M)$ (see for instance "Bemerkung 2.57" in \cite{LAKnab}). Thus once we have evaluated a new ansatz to a row vector $v$, we can compute the rank of the new ansatz forest that would be obtained by adding the given ansatz to the current forest by:
\begin{align}\label{RankDatmat}
    \mathrm{rank}\left (\begin{bmatrix}
        M \\
        \cmidrule(lr){1-1} 
        v
    \end{bmatrix} \right )
    = \mathrm{rank} \left ( \begin{bmatrix}
        M \\
        \cmidrule(lr){1-1}
        v
    \end{bmatrix} \cdot \begin{bmatrix}
        M^t \ \vert \  v^t 
    \end{bmatrix} \right ) = \mathrm{rank} \left (\begin{bmatrix}
        MM^t & \vline & M v^t \\
        \cmidrule(lr){1-3}
        (Mv^t)^t & \vline & vv^t 
    \end{bmatrix}  \right ).
\end{align}
If the ansatz is numerically linearly independent from the forest appending its evaluated row vector to the current ansatz matrix will result in an increased rank.

Note that in each step of the algorithm once we have evaluated the new ansatz to the row vector $v$ we only have to compute its transpose $v^t$ the matrix-vector product $Mv^t$ and the corresponding transpose $(Mv^t)^t$ and the vector-vector product $v v^t$. In particular, the upper left block in the last matrix in (\ref{RankDatmat}) is already provided from the previous step and thus only needs to be computed newly once an additional ansatz is added to the ansatz forest. Hence we achieve the desired result with a minimum of expensive matrix-matrix operations involved. The rank computation itself is then as before carried out by relying on efficient Eigen subroutines.

The following function (\ref{checkNumLinDep}) takes as arguments the current rank data and the newly evaluated ansatz row vector and computes from it the new rank data, that is it returns the rank data provided by the first matrix in (\ref{RankDatmat}) if the new ansatz is numerically linearly independent from the ansatz forest in the form of a \mintinline{haskell}{Just} value and it returns \mintinline{haskell}{Nothing} if the new ansatz is numerically linearly dependent. 

\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
checkNumericLinDepEig :: RankDataEig -> Maybe Sparse.SparseMatrixXd ->
                         Maybe RankDataEig
checkNumericLinDepEig (lastMat, lastFullMat) (Just newVec)
    | eigenRank < maxRank = Nothing
    | otherwise = Just (newMat, newAnsatzMat)
     where
        newVecTrans = Sparse.transpose newVec
        scalar = Sparse.toMatrix $ Sparse.mul newVec newVecTrans
        prodBlock = Sparse.toMatrix $ Sparse.mul lastFullMat newVecTrans
        prodBlockTrans = Mat.transpose prodBlock
        newMat = concatBlockMat lastMat prodBlock prodBlockTrans scalar
        eigenRank = Sol.rank Sol.FullPivLU newMat
        maxRank = min (Mat.cols newMat) (Mat.rows newMat)
        newAnsatzMat = Sparse.fromRows $ 
                       Sparse.getRows lastFullMat ++ [newVec]
checkNumericLinDepEig (lastMat, lastFullMat) Nothing = Nothing
\end{minted} 
\caption{Check Numeric Linear Dependencies.}\label{checkNumLinDep}
\end{listing}

With these functions defined, we can now construct functions that construct algebraically and numerically linearly independent ansatz forests out of a given list of individual products, the given symmetry and the evaluation list (\ref{redMem}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
reduceAnsatzEtaEig :: Symmetry -> [[Eta]] -> [I.IntMap Int] ->
                      (AnsatzForestEta,Sparse.SparseMatrixXd)
reduceAnsatzEtaEig symL etaL evalM
    | null evalM = (EmptyForest, Sparse.fromList 0 0 [])
    | null etaL = (EmptyForest, Sparse.fromList 0 0 [])
    | otherwise = (finalForest, finalMat)
    where
        (ans1,rDat1,restEtaL) = mk1stRankDataEtaEig symL etaL evalM
        (finalForest, (_,finalMat)) = foldl' 
                                      (addOrDiscardEtaEig symL evalM)
                                      (ans1,rDat1) restEtaL

reduceAnsatzEpsilonEig :: Symmetry -> [(Epsilon,[Eta])] ->
                          [I.IntMap Int] ->
                          (AnsatzForestEpsilon,Sparse.SparseMatrixXd)
reduceAnsatzEpsilonEig symL epsL evalM
    | null evalM = (M.empty, Sparse.fromList 0 0 [])
    | null epsL = (M.empty, Sparse.fromList 0 0 [])
    | otherwise = (finalForest, finalMat)
    where
        (ans1,rDat1,restEpsL) = mk1stRankDataEpsilonEig symL epsL evalM
        (finalForest, (_,finalMat)) = foldl'
                                      (addOrDiscardEpsilonEig symL evalM)
                                      (ans1,rDat1) restEpsL
\end{minted} 
\caption{Reduce Linear Dependencies: the "Efficient" Way.}\label{redMem}
\end{listing}
Finally, now we can also provide functions (\ref{mkAnsatzEig1}), (\ref{mkAnsatzEig2}) and (\ref{mkAnsatzEig3}) that return a triplet consisting of two ansatz forests corresponding to a Lorentz invariant basis of given rank and symmetry and one tensor that includes all their values, this time employing the memory-optimized algorithm. 
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorEigSym :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                        [[Int]] -> (AnsatzForestEta, AnsatzForestEpsilon,
                        STTens n 0 (AnsVarR))
mkAnsatzTensorEigSym ord symmetries evalL = (ansEta, ansEps, tens)
        where
            (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
                mkAllEvalMaps symmetries evalL 
            (ansEta, ansEps, _, _) =
                getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
            tens = evalToTensSym
                   symmetries evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 2.1: with Explicit Symmetrization.}\label{mkAnsatzEig1}
\end{listing}
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorEig :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                     [[Int]] -> (AnsatzForestEta, AnsatzForestEpsilon,
                     STTens n 0 (AnsVarR))
mkAnsatzTensorEig ord symmetries evalL = (ansEta, ansEps, tens)
        where
            (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
                mkAllEvalMaps symmetries evalL 
            (ansEta, ansEps, _, _) =
                getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
            tens = evalToTens evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 2.2: without Explicit Symmetrization.}\label{mkAnsatzEig2}
\end{listing}
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--evaluation to a tensor that uses multiple abstract index types
mkAnsatzTensorEigAbs :: Int -> Symmetry ->
                        [([Int], Int, [IndTupleAbs n1 0 n2 0 n3 0])] ->
                        (AnsatzForestEta, AnsatzForestEpsilon,
                        ATens n1 0 n2 0 n3 0 (AnsVarR))
mkAnsatzTensorEigAbs ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMapsAbs symmetries evalL 
        (ansEta, ansEps, _, _) =
            getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTensAbs evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\caption{Ansatz Construction 2.3: Evaluation to Custom Indices.}\label{mkAnsatzEig3}
\end{listing}
And as before we also provide versions of the first two functions (\ref{mkAnsatzEig1'}) and (\ref{mkAnsatzEig2'}) that automatically construct the evaluation list from the rank and the symmetries specified.
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorEigSym' :: forall (n :: Nat). SingI n =>  Int ->
                         Symmetry ->
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         STTens n 0 (AnsVarR))
mkAnsatzTensorEigSym' ord symmetries = mkAnsatzTensorEigSym
                                       ord symmetries evalL
    where
        evalL =
            filter (`filterAllSym` symmetries) $ allList ord symmetries
\end{minted} 
\caption{Ansatz Construction 2.4: with Explicit Symmetrization,  no Evaluation List Required}\label{mkAnsatzEig1'}
\end{listing}
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorEig' :: forall (n :: Nat). SingI n =>  Int ->
                      Symmetry ->
                      (AnsatzForestEta, AnsatzForestEpsilon,
                      STTens n 0 (AnsVarR))
mkAnsatzTensorEig' ord symmetries = mkAnsatzTensorEig
                                    ord symmetries evalL
    where
        evalL =
            filter (`filterAllSym` symmetries) $ allList ord symmetries
\end{minted} 
\caption{Ansatz Construction 2.5: without Explicit Symmetrization, no Evaluation List Required.}\label{mkAnsatzEig2'}
\end{listing}

The developed sparse-tensor library also provides further functions that can be useful when working with the ansatz forest data types. Details can again be found in \cite{sparse-tensor}, specifically in the \textit{LorentzGenerator} submodule. 

\section{A Step-By-Step Example}
In this section, we are going to consider an example that shall nicely illustrate how the sparse-tensor package can be used to deal with the kind of problems that arise in the perturbative approach to Constructive Gravity. To that end, we are going to consider the two linear-order perturbative equivariance equations (\ref{order1}) in the context of area metric gravity. 

The first step is determining the different indices and thus, the abstract tensor type that is needed for the treatment of perturbative area metric gravity. 
We need one index type that runs over area metric degrees of freedom, one index type for second-order spacetime derivatives and one index type for treating spacetime indices. The corresponding tensor type was already introduced before:

\begin{center}
\begin{cminted}{haskell}
type ATens n1 n2 n3 n4 n5 n6 v = 
     AbsTensor6 n1 n2 n3 n4 n5 n6 Ind20 Ind9 Ind3 v
\end{cminted}
\end{center}

Here the different index types are all defined in the same fashion. As an illustrative example, we discuss \mintinline{haskell}{Ind3} in more detail:
\begin{center}
\begin{cminted}{haskell}
newtype Ind3 =  Ind3 {indVal3 :: Int}
    deriving (Ord, Eq, Show, Read, Generic, NFData, Serialize)
\end{cminted}
\end{center}
We choose to use the \mintinline{haskell}{newtype} declaration in order to ensure that functions that are defined for one particular index type cannot be applied to any other index type and thus preventing possible errors that might occur when mixing up index types.
Note that in order for the tensor algebra functions in sparse-tensor to work for the \mintinline{haskell}{ATens} type, the index types must all be instances of \mintinline{haskell}{TIndex}. This can be simply achieved by defining the function \mintinline{haskell}{toEnum}, \mintinline{haskell}{fromEnum} making them instances of the typeclass \mintinline{haskell}{Enum}: 
\begin{center}
\begin{cminted}{haskell}
instance Enum Ind3 where
    toEnum = Ind3
    fromEnum = indVal3
\end{cminted} 
\end{center}
and then simply making them also an instance of \mintinline{haskell}{TIndex}:

\begin{center}
\begin{cminted}{haskell}
instance TIndex Ind3 where
\end{cminted} 
\end{center}

When taking a closer look at the equations (\ref{order1}), we see that albeit of the three Lorentz invariant basis tensors $a^{A}$, $a^{AI}$ and $a_0$ we further need the two area metric intertwiners $I^I_{abcd}$, $J_I^{abcd}$ with components being displayed in (\ref{AreaI}) and (\ref{AreaJ}), the J-intertwiner of a symmetric index pair whose components can be found in (\ref{interJMet})  , the flat background area metric $N_A$ and the 4 dimensional Kronecker delta $\delta^a_b$. 
The Kronecker delta can readily be obtained (\ref{Kronecker}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
delta3 :: ATens 0 0 0 0 1 1 (SField Rational)
delta3 = fromListT6 $ zip
         [(Empty, Empty, Empty, Empty,
         singletonInd (Ind3 i), singletonInd (Ind3 i)) | 
         i <- [0..3]] (repeat $ SField 1)
\end{minted} 
\caption{Construction of Kronecker Delta.}\label{Kronecker}
\end{listing}
The three intertwiners can be constructed according to (\ref{ConstrInterJ2}), (\ref{ConstrINterIArea}) and (\ref{ConstrInterJArea}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
interJ2 :: ATens 0 0 0 1 2 0 (SField Rational)
interJ2 = fromListT6 $ fmap (fmap SField) $ filter (\(i,k) -> k /= 0) $
          map (\x -> (x,f x)) inds
    where
        trian2 = trianMap2
        inds = [ (Empty, Empty, Empty, singletonInd $ Ind9 a,
        Append (Ind3 b) $ singletonInd $ Ind3 c, Empty) |
        a <- [0..9], b <- [0..3], c <- [0..3]]
        f (_, _, _, ind1, ind2, _)
            | ind1 == (M.!) trian2 (sortInd ind2) = jMult2 ind2
            | otherwise = 0
\end{minted} 
\caption{Construction of Symmetric Index Pair I Intertwiner.}\label{ConstrInterJ2}
\end{listing}
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
interIArea :: ATens 1 0 0 0 0 4  (SField Rational)
interIArea = fromListT6 $ fmap (fmap SField) $ filter (\(i,k) -> k /= 0) $
             map (\x -> (x,f x)) inds
    where
        trianArea = trianMapArea
        inds = [(singletonInd (Ind20 a), Empty, Empty, Empty, Empty,
               Append (Ind3 b) $ Append (Ind3 c) $ Append (Ind3 d) 
               $ singletonInd $ Ind3 e) |
               a <- [0..20], b <- [0..3], c <- [0..3], d <- [0..3],
               e <- [0..3], not (b == c || d == e)]
        f (ind1, _, _, _, _, ind2)
            | ind1 == (M.!) trianArea indArea = s
            | otherwise = 0
            where
                (indArea, s) = canonicalizeArea ind2
\end{minted} 
\caption{Construction of Area Metric I Intertwiner. }\label{ConstrINterIArea}
\end{listing}
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
interJArea :: ATens 0 1 0 0 4 0 (SField Rational)
interJArea = fromListT6 $ fmap (fmap SField) $ filter (\(i,k) -> k /= 0) $
             map (\x -> (x,f x)) inds
    where
        trianArea = trianMapArea
        inds = [(Empty, singletonInd $ Ind20 a, Empty, Empty,
               Append (Ind3 b) $ Append (Ind3 c) $ Append (Ind3 d)
               $ singletonInd $ Ind3 e, Empty) |
               a <- [0..20], b <- [0..3], c <- [0..3], d <- [0..3],
               e <- [0..3], not (b == c || d == e)]
        f (_, ind1, _, _, ind2, _)
            | ind1 == (M.!) trianArea indArea = s * jMultArea indArea
            | otherwise = 0
            where
                (indArea, s) = canonicalizeArea ind2
\end{minted} 
\caption{Construction of Area Metric J Intertwiner.}\label{ConstrInterJArea}
\end{listing}
Here \mintinline{haskell}{trianMap2} is a Map that relates the indices of a symmetric pair of spacetime indices to the corresponding abstract index of type \mintinline{haskell}{Ind9}, \mintinline{haskell}{trianMapArea} is a Map that relates the $4$ spacetime indices of a given area metric to the corresponding abstract index, \mintinline{haskell}{canonicalizeArea} is a function that brings a set of $4$ spacetime indices obeying the area metric symmetries to canonical order taking into account possible signs that might occur during the resorting, \mintinline{haskell}{jMult2} is a function that computes the $\sigma$ multiplicity of the symmetric index pair and \mintinline{haskell}{jMultArea} computes the $\sigma$ multiplicity (see discussion following definition \ref{interDef} for the precise definition of these multiplicities $\sigma$) of a given set of $4$ spacetime indices that correspond to one area metric.

From the two area metric intertwiners, we can compute the constant tensor $C^{Am}_{Bn}$ according to (\ref{areaGotayMInter}):

\begin{center}
\begin{cminted}{haskell}
interArea :: ATens 1 1 0 0 1 1 (SField Rational)
interArea = (-4) &. contrATens3 (1,1) (contrATens3 (2,2) $
            contrATens3 (3,3) $ interIArea &* interJArea)
\end{cminted}
\end{center}
The flat background area metric can be constructed as (\ref{ConstrFlatArea}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
flatArea :: ATens 0 1 0 0 0 0 (SField Rational)
flatArea = fromListT6 $ map (\(i,v) -> ( (Empty, singletonInd $ Ind20 i,
           Empty, Empty, Empty, Empty), SField v))
           [(0,-1),(5,-1),(6,-1),(9,1),(11,-1),
           (12,-1),(15,1),(18,1),(20,1)]
\end{minted} 
\caption{Construction of Flat Area Metric.}\label{ConstrFlatArea}
\end{listing}

The next step is the computation of the three bases of Lorentz invariant tensors. Clearly, $a_0$ is simply a single constant. As such, it can be incorporated in our tensor framework by:
\begin{center}
\begin{cminted}{haskell}
let ans0 = fromListT6' [(([],[],[],[],[],[]), AnsVar $ 
           I.fromList [(1,1)] )] :: ATens 0 0 0 0 0 0 (AnsVarR)
\end{cminted}
\end{center}
The second expansion coefficient is given by $a^A = I^A_{abcd} a^{abcd}$, where the indices $(abcd)$ feature the area metric symmetries. In order to use our functions for constructing the appropriate Lorentz invariant basis tensors we need to provide the symmetry at hand as a value of type \mintinline{haskell}{Symmetry} and also need to provide a list of index value lists that are necessary for the evaluation. Labeling the indices $(abcd)$ by the integers $(1,2,3,4)$ the symmetry is given as:
\begin{center}
\begin{cminted}{haskell}
symList4 :: Symmetry
symList4 = ([], [(1,2),(3,4)], [([1,2],[3,4])], [], [])
\end{cminted}
\end{center}
This simply corresponds to pair anti symmetries in the indices $(1,2)$ and $(3,4)$ and a block symmetry w.r.t. exchange of the two index pairs $(1,2)$ and $(3,4)$. The evaluation list can be constructed as (\ref{ConstrAreaList4}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
areaList4 :: [([Int], Int, [IndTupleAbs 1 0 0 0 0 0])]
areaList4 = list
      where
          trianArea = trianMapArea
          list = [ let a' = (I.!) trianArea a in (a', areaMult a',
                 [(singletonInd (Ind20 $ a-1), Empty, Empty, Empty,
                 Empty, Empty)]) | a <- [1..21] ]
\end{minted} 
\caption{Construction of Area Metric Evaluation List 1.}\label{ConstrAreaList4}
\end{listing}
As before \mintinline{haskell}{trianMapArea} is a Map that relates a block of 4 spacetime indices with area metric symmetries to an abstract area metric index and \mintinline{haskell}{areaMult} is a function that computes the multiplicity of a given set of 4 spacetime indices with area metric symmetry. Note that we do not only need to provide the index value list that is necessary for the evaluation of the ansatz forests, but also need to include the multiplicities of the index values and the corresponding abstract indices as we want to make use of the function \mintinline{haskell}{mkAnsatzTensorEigAbs} to evaluate the ansatz forests to the chosen abstract tensor type \mintinline{haskell}{ATens}. 

We can now construct the basis for the expansion coefficient $a^{A}$ by invoking:
\begin{center}
\begin{cminted}{haskell}
let (eta4,eps4,ans4) = mkAnsatzTensorEigAbs 4 symList4 areaList4 :: 
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         ATens 1 0 0 0 0 0 (AnsVarR))
\end{cminted}
\end{center}
If we wish to check the constructed ansatz forest quickly we can do so by using the functions \mintinline{haskell}{drawAnsatzEta} and \mintinline{haskell}{drawAnsatzEpsilon} that return a simple ASCII drawing of the forest structure. Doing so we get:
\begin{center}
\begin{BVerbatim}
(1,3)
|
`---- (2,4) * (4) * x[1]

(1,4)
|
`---- (2,3) * (-4) * x[1]

(1,2,3,4) * (8) * x[2]
\end{BVerbatim}
\end{center}
Proceeding along the same lines for the ansatz $a^{AI} = I^A _{abcd} I^I_{pq} a^{abcdpq}$ we first provide the symmetry:
\begin{center}
\begin{cminted}{haskell}
symList6 :: Symmetry
symList6 = ([(5,6)], [(1,2),(3,4)], [([1,2],[3,4])], [], [])
\end{cminted}
\end{center}
This simply corresponds to an additional pair symmetry in the derivative indices $(5,6)$. The evaluation list can be obtained as (\ref{ConstrAreaList6}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
areaList6 :: [([Int], Int, [IndTupleAbs 1 0 1 0 0 0])]
areaList6 = list
      where
          trian2 = trianMap2
          trianArea = trianMapArea
          list = [ let (a',i') = ((I.!) trianArea a, (I.!) trian2 i) in
                 (a' ++ i', areaMult a' * iMult2 i', 
                 [(singletonInd (Ind20 $ a-1), Empty,
                 singletonInd (Ind9 $ i-1),
                 Empty, Empty, Empty)]) | a <- [1..21], i <- [1..10]]
\end{minted} 
\caption{Construction of Area Metric Evaluation List 2.}\label{ConstrAreaList6}
\end{listing}

We now can again construct the ansatz forests:
\begin{center}
\begin{cminted}{haskell}
let (eta6,eps6,ans6) = mkAnsatzTensoreigAbs 6 symList6 areaList6 :: 
                       (AnsatzForestEta, AnsatzForestEpsilon,
                       ATens 1 0 1 0 0 0 (AnsVarR))
\end{cminted}
\end{center}
Note that when constructed, the variables that are included in any pair of ansatz forests start from one. When one uses several ansatz forests in one equation, it is thus necessary to relabel certain variables. The number of variables in a given pair of ansatz forest can, for instance, be obtained by applying the function \mintinline{haskell}{tensorRank6'} on the corresponding tensor. This function arranges the independent components of the tensor in a matrix with the variables labeling the columns and then invokes Eigen subroutines to compute the rank of the matrix. for instance applying it on the ansatz tensor \mintinline{haskell}{ans4} we get \mintinline{haskell}{tensorRank6' ans4 = 2}. Hence we can use it to shift the labels of the variables that are included in the three tensors appropriately:
\begin{center}
\begin{cminted}{haskell}
let ans0' = shiftLabels6 5 ans0
let ans4' = shiftLabels6 3 ans4 
\end{cminted}
\end{center}

Now the first 3 variables are those contained in \mintinline{haskell}{ans6} the next 2 are those in \mintinline{haskell}{ans4} and the last variable is those from \mintinline{haskell}{ans0}.
We can now finally construct the two equations in (\ref{order1}). The source code is displayed in (\ref{ConstrEqn}).
\begin{listing}[hbt!]
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG!40]{haskell}
eqn1 :: ATens 0 0 0 0 0 0 (AnsVarR) ->
        ATens 1 0 0 0 0 0 (AnsVarR) ->
        ATens 0 0 0 0 1 1 (AnsVarR)
eqn1 ans0 ans4 = contrATens1 (0,0) (ans4 &* flatInter) &+
                 (ans0 &* delta3)
                 
eqn2 :: ATens 1 0 1 0 0 0 (AnsVarR) ->
        ATens 0 0 0 0 3 1 (AnsVarR)
eqn2 ans6 = contrATens2 (0,0) $ contrATens1 (0,0) $ ans6 &*
            contrATens1 (0,1) (interEqn5 &* flatArea)
    where 
        interEqn5 = cyclicSymATens5 [0,1,2] $
                    interJ2 &* interMetric
\end{minted} 
\caption{Construction of Area Metric Perturbative Equivariance  Equations.}\label{ConstrEqn}
\end{listing}

We can now, for instance, check the rank of the equations. This can be achieved by first computing the two equations:
\begin{center}
\begin{cminted}{haskell}
let eqn1Area = eqn1 ans0' ans4' 
let eqn2Area = eqn2 ans6  
\end{cminted}
\end{center}
Then we collect the two equations in a list:
\begin{center}
\begin{cminted}{haskell}
let tList = eqn2Area &.&> (singletonTList6 eqn1Area ::
            TensList6 Ind20 Ind9 Ind3 (AnsVarR)) 
\end{cminted}
\end{center}
Then we compute the rank of the collective tensor equations that are included in such a list to find:
\begin{center}
\begin{cminted}{haskell}
tensorRank6 tList = 2 
\end{cminted}
\end{center}
We can also extract the information given by the two tensor equations in the form of a matrix with as usual columns labeling the 6 variables and rows labeling independent equations. The sparse-tensor function \mintinline{haskell}{toMatList6} for instance returns this matrix in terms of a standard sparse matrix format \mintinline{haskell}{[((Int,Int),(SField Rational))]}, where the pair of integers label row and column and the rational number is the corresponding value. Applying this function we find:
\begin{center}
\begin{cminted}{haskell}
toMatList6 tList = [((1,1),1152 % 1),((1,2),576 % 1),((1,3),(-2304) % 1),
                   ((2,4),(-96) % 1),((2,5),192 % 1),((2,6),1 % 1)]
\end{cminted}
\end{center}
Which in matrix corresponds to:
\begin{align}
    \begin{bmatrix}
    1152 & 576 & -2304 & 0 & 0 & 0 \\
    0 & 0 & 0 & -96 & 192 & 1
    \end{bmatrix}.
\end{align}
Note that the relatively large values are a result of the used factor less symmetrization method. The thus generated output can now easily be used as input for standard computer algebra systems such as Maple or Mathematica that then can be used to solve the perturbative equivariance equations.
The further required tensors that were used to derive the second-order solution displayed in Appendix \ref{AppArea}
are also included in the sparse-tensor sub modules \textit{Gravity} and \textit{Gravity.DiffeoSymEqns} which can also be found in \cite{sparse-tensor}. 
