\documentclass[a4paper,12pt, DIV=14, BCOR=5mm, twoside, headsepline]{scrbook}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage[cal=boondoxo]{mathalfa}
\usepackage{relsize}
\usepackage[boxruled, linesnumbered]{algorithm2e}
\usepackage{stmaryrd}
\usepackage{fancyvrb}

\usepackage{booktabs} % required for the first solution



%for code included
\usepackage{minted}
\usepackage[dvipsnames]{xcolor}
\definecolor{LG}{rgb}{0.863, 0.863, 0.863}


%for centered minted environment 
\usepackage{xpatch,letltxmacro}
\LetLtxMacro{\cminted}{\minted}
\let\endcminted\endminted
\xpretocmd{\cminted}{\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}}{}{}

\usemintedstyle{vs}


\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{tikz}
\usetikzlibrary{shapes}


\tikzset{every picture/.style=remember picture}

\newcommand{\mathnode}[1]{%
   \mathord{\tikz[baseline=(#1.base), inner sep = 0pt]{\node (#1) {$\scriptstyle{#1}$};}}}
   


\usepackage[utf8]{inputenc}

\setcounter{MaxMatrixCols}{20}

\pagestyle{headings}

\renewcommand*{\dictumwidth}{0.65\textwidth}
\renewcommand*{\raggeddictum}{\centering}
\renewcommand*{\raggeddictumtext}{}
\setkomafont{dictumtext}{\normalfont\normalcolor\itshape\setlength{\parindent}{2em}\noindent}

\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{example}{Example}[chapter]


\usepackage[backend=biber, style=alphabetic, sorting=ynt]{biblatex}
\addbibresource{references.bib}

\AtEveryBibitem{%
  \ifentrytype{misc}{%
  }{%
    \clearfield{url}%
  }%
}

\usepackage{xkeyval}

%citation nasa ads macros

\def\apjl{\textit{ApJ}}                % Astrophysical Journal, Letters

\def\prd{\textit{Phys.~Rev.~D}}        % Physical Review D

\def\prl{\textit{Phys.~Rev.~Lett.}}    % Physical Review Letters


\title{Covariant Constructive Gravity}
\author{Tobias Reinhart }
\date{April 2019}

\begin{document}
 
\begin{titlepage}
	\centering

	{\Huge\bfseries Covariant Constructive Gravity\par}
	\vspace{2cm}
	
	{\scshape\Large Masterthesis \par}
	\vspace{1cm}
	{ submitted by \par}
	
	
	
	{\bfseries Tobias Reinhart\par}
		{ 4. September 2019\par}
	\vspace{2cm}
	
	\includegraphics[width=0.5\textwidth]{fau-siegel.pdf}\par
	\vspace{1cm}Department of Physics\par
	Friedrich-Alexander-Universität Erlangen-Nürnberg\par
	Supervisor: Erstgutachter , Dr. Frederic P. Schuller
	
	\vfill





\end{titlepage}

\tableofcontents

\chapter{Introduction: The Aim of Constructive Gravity}

%in introduction: main idea of constructive gravity and reasons for the constructive approach. Maybe also current state of the field (+ lovelock, HKT, bootstrap).

%check labeling of equations !!!


Before we start with the main part we quickly illustrate particularities of the chosen \textit{\textbf{notation}}. Throughout this thesis even if not stated explicitly all objects are considered to be smooth, i.e. we work with smooth maps defined on smooth manifolds. 

Furthermore as the relevant construction presented in this work are entirely local we will denote maps between manifolds without being to rigorous with the description of the appropriate domain and codomain respectively. Hence we might for instance write $x^a : M
\rightarrow \mathbb{R}$ for a chart map on a manifold $M$ where we should more properly write $x^a : M \supset U : \rightarrow x(U) \subset \mathbb{R} $. 

Lower case latin indices $a,b,c,...$ are used as spacetime indices and hence run from 0 to 3, spatial indices between 1 and 3 are denoted by lower case greek letters $\alpha,\beta,\gamma,...$. In addition to that we will use further types of indices to describe coordinate functions on additional bundles that we are going to construct over the spacetime manifold $M$. Upper case latin letters from the middle of the alphabet $I_k,J_k,L_k,...$ will be used to describe derivative indices of order $k$ and hence run from $0$ to $\binom{b+k-1}{n-1}-1$ where $n$ is the dimension of the basespace w.r.t. which the derivatives are taken. As we will work extensively over some second order jet bundle we will sometimes drop the labels that specify the order of such indices and hence use $I,J,K,...$ to solely describe such second derivative order indices if this does not cause confusion.

There will also be situations with two different types if derivative indices appearing in the same equation, describing higher derivatives w.r.t. the coordinates of the spacetime manifold and w.r.t. coordinates of some additional bundle. We will then label the derivative coordinates of the latter kind by caligraphic letters $\mathcal{I}_k, \mathcal{J}_k,...$ to allow for distinction.

Upper case latin letters from the beginning of the alphabet $A,B,C,D,...$ are used to label fibre coordinate functions on the space of fields, whenever we are dealing with a second field that belongs to a different space we label the coordinates by $\tilde{A}, \tilde{B},...$ . The respective range obviously depends on the specific field at hand.\\

In this work we will deal with bundles of smooth manifolds to quite some extend. When we introduce a new bundle we will denote it as triple $(F,\pi_F,M)$ consisting of the total space $F$ the bundle projection $\pi_F$ and the base space $M$. Priorly introduced bundles might also be simply denoted by their total space. Morphisms between bundles $(F_1,\pi_1,M_1)$ and $(F_2,\pi_2,M_2)$ are denoted as pairs of maps $(f,h)$, with $f$ being the map between total spaces that covers $h$. Bundle morphisms that cover the identity or such morphisms that are constructed from a given map between base spaces will be referred simply by the map on the total space.

Given a vector space $V$, a linear map $\phi$, a vector bundle $(E,\pi,N)$ or any other object with a notion of duality, a superscripted $\ast$ on the relevant object will denote the corresponding dual object, i.e. the dual vector space $V^{\ast}$, dual linear map $\phi^{\ast}$, dual vector bundle $(E^{\ast}, \pi^{\ast},N)$. In particular given a smooth map between manifolds $f : M \rightarrow N$ we denote its pushforward by $f_{\ast} : Tm \rightarrow TN$ and its pullback by $f^{\ast} : T^{\ast}N \rightarrow T^{\ast}M$.

We denote tensor products by $\otimes$ and the corresponding space of $(m,n)$ tensors over $M$ by $T^m_nM$. The space of n-forms over $M$ is denoted by $\Lambda^nM$. Given a bundle $(F,\pi_{F},M)$ we write $G \in \Gamma(F) $ for a section of this bundle. We denote direct sums of vectorspaces or the whitney sum of vector bundles by $\oplus$ respectively. \\

The jet bundle of order q over a given bundle $(F,\pi_F,M)$ will be denoted by $J^qF$ and any associated jet prolongation map, either in the sense of prolonging sections of $F$ to sections of $J^qF$ or in the sense of prolonging vectorfields on $M$ to vectorfields on $J^qF$ will be referred to as $j^q$. 

In general we will label bundle maps that take values in the bundle of volume forms over some base manifold letters in calligraphic font $\mathcal{L}, \mathcal{H}$ and the corresponding representation in terms of the chart induced volume form by the corresponding standard font letter $\mathcal{L}=L \mathrm{d}^4x, \mathcal{H}= H \mathrm{d}^3x$.

As usual $\left[ \  ,   \  \right]$ denotes the commutator of vector fields and $\left \{  \  ,   \   \right \}$ the poisson bracket of phase space functions/functionals. 

\chapter{Diffeomorphism Invariant Field Theory}

\dictum{
We provide the basic mathematical tools necessary for a precise formulation of classical Lagrangian field theory. Furthermore we study implications of the requirement that the Lagrangian be infinitesimally equivariant w.r.t. spacetime diffeomorphisms and derive from it an equivalent set of first order partial differential equations. Lastly we investigate consequences of the diffeomorphism invariance for the Hamiltonian formulation of classical field theory.
}

\section{Field Bundles Jet Bundles and Prolongations}
In order to develop a concise framework for classical Lagrangian field theory it is insightful to recall the situation in standard classical mechanics. There one encodes the possible configurations a given system might adopt at given time as finite dimensional manifold, the so called configuration space $Q$. The objects of interest are curves $\gamma : \mathbb{R} \rightarrow Q $ that represent the systems path in this configuration space, i.e. the particular system configuration dependent on some parameter time. Given such a curve, by adjoining its velocity $\dot{\gamma}$ at each parameter value one can lift the curve to the tangent bundle $TQ$. The dynamics of the system can then be described by a Lagrangian function $L : TQ \rightarrow \mathbb{R}$. Composing $L$ with a lifted curve one obtains a real valued function. As such one can compute its integral. This procedure defines a local functional on the space of curves:
\begin{align}
S[\gamma] = \int \mathrm{d}\lambda L(\gamma (\lambda), \dot{\gamma} (\lambda)).  
\end{align}
Fixing start and end configuration and velocity of the system for all curves in consideration, the physical curves are those that extremize this action functional, or equivalently those that solve the corresponding Euler-Lagrange-Equations. \\

%We conclude that in order to obtain a mathematical precise formulation of classical mechanics one needs a manifold that represents possible system configurations, %a notion of lifting curves in this configuration space by adjoining its velocities and a Lagrangian function that encodes the dynamics of the system.\\

Once the transition from standard particle mechanics to field theories is made the system is no longer described by curves in some finite dimensional configuration space representing the systems configuration at each parameter time, but by the values a given field attains at different spacetime points. Hence instead of curves one deals with sections of some bundle $(F,\pi_F,M)$ over the 4-dimensional spacetime manifold $M$. The notion of adjoining velocities to a curve is replaced by adjoining derivatives to a section. The appropriate setting for this procedure is the \textit{\textbf{jet bundle}} over $F$. Just as the tangent bundle $TQ$ is obtained from a coordinate independent description of derivatives of curves, the jet bundle over $F$ is constructed by a coordinate independent description of derivatives of sections of $F$. As before the Lagrangian is then simply a function on this jet bundle. 
In the following we outline how these ideas can be cast into precise mathematical language. \\

We follow \cite{1998physics...1019G} to some extend. Further information concerning the geometry of jet bundles can be found in \cite{saunders_1989} and \cite{seiler2009involution}. A more sophisticated treatment with the focus lying on the naturallity\footnote{Loosely speaking in the sense that certain constructions do not require further structure or additional choices than what is already provided.} of certain constructions that we are going to meet in the next chaptes can be found in \cite{kolar1993natural}. Many details regarding the involved standard differential geometry are concisely collected in \cite{doi:10.1142/3867}  Let $M$ be an oriented 4-dimensional manifold that represents spacetime. We restrict our attention to tensor fields over $M$. Such fields are \textit{\textbf{sections}} of a bundle $(F,\pi_F,M)$, i.e. smooth maps $G : M \rightarrow F $ that satisfy $\pi_F \circ G = \mathrm{id}_M$. In the following we will refer to the bundle $F$ as $\textit{\textbf{field bundle}}$.  As we want to consider tensor fields the field bundle is required to be given by a vector sub bundle of some $T^m_n M$. We denote adapted coordinates\footnote{Recall that adapted coordinates $(x^m,v_A)$ on a fibre bundle $(F, \pi_F, M)$ are defined by the requirement that there exits a coordinate chart on $M$, $(y^m)$, s.t. $y^m = x^m \circ \pi_F$, and hence we denote the first $dim(M)$ coordinate functions of both charts with the same label. It should be clear from the context which coordinates are meant, those on $M$ or those on $F$.} on $F$ by $(x^m,v_A)$, where $A$ runs from $0$ to $n - 1$ with $n$ being the fibre dimension of $F$, i.e. $n := \pi_F^{-1}(p)$ for some $p \in M$. Note that as $F$ is assumed to be a vector bundle we can always chose adapted coordinates s.t. the $v_A$ are linear on the fibres. Details regarding this remark can be found in \cite{saunders_1989}. In the following we will only regard such adapted coordinates. The corresponding coordinate representation of sections is denoted by $G_A(x^m(p)) = v_A \circ G (p)$ where we will simply write $G_A$ most of the time. Hence writing $G_A$ we refer to the coordinate representation of a section, whereas $v_A$ simply denotes fibre coordinates on $F$. \\
%check coordinate expression 

Given a field bundle $(F, \pi_F, M)$ we can define the corresponding $\textit{\textbf{dual field bundle}}$ $(F^{\ast}, \pi_{F^{\ast}},M)$ as its vector bundle dual. We quickly recall the definition of the vector bundle dual and the dual of a linear map.
\begin{definition}[dual linear map] \label{dual}
Given vectorspaces $V$ and $W$ and a linear map, i.e. a morphism of vectorspaces $f : V \rightarrow W$, the dual linear map $f^{\ast} : W^{\ast} \rightarrow V^{\ast}$ is defined by the requirement: 
\begin{align}
    \forall v \in V, \forall \omega \in W^{\ast} : \omega (f(v)) = f^{\ast}(\omega) (v).
\end{align}
\end{definition}
Sometimes the dual of a linear map is also called its adjoint. With the definition of the vector space dual and the dual of a linear map we can now lift these notions to the case of vector bundles and vector bundle morphisms.
\begin{definition} [vector bundle dual]
Given a vector bundle $(F, \pi_F,M)$. Its vector bundle dual is the vector bundle $(F^{\ast}, \pi_{F^{\ast}},M)$ with fibres $\pi_{F^{\ast}}^{-1}(p) = (\pi_F^{-1}(p))^{\ast} $ for all $p$ in $M$ and the obvious projection.  
\end{definition}
In other words one obtains the dual to a given vector bundle by implementing the dual construction fibre by fibre.
Given adapted coordinates on the field bundle $(x^m, v_A)$ with the coordinate functions $v_A$ being linear on the fibres, we can obtain \textbf{\textit{dual coordinates}} on the dual field bundle by $(x^m, ((v_A)^{\ast})^{-1})$, i.e. by taking the inverse dual of the fibre coordinates $v_A$ (see \cite{saunders_1989}). Note that we do not simply take the dual of the $v_A$ but its inverse as the dual construction reverses the direction of linear maps\footnote{In the language of category theory: duality defines a contravariant functor \cite{MacLane:205493}.} In the following we denote the fibre coordinates dual to $v_A$ by $v^A$. With these coordinates we get $ v_A(v^B) = \delta_A^B$. From now on given a chart on the field bundle we always work with the corresponding dual chart on the dual field bundle.\\
%check this last statement

Note that as the field bundle is required to be given by a vector sub bundle of some $T^m_n M$ one usually works directly on $T^m_n M$ and uses chart induced basis fields $\mathrm{d}x^{a_1}\otimes ... \otimes \mathrm{d}x^{a_m} \otimes \frac{\partial}{\partial x^{b_1}} \otimes ... \otimes \frac{\partial}{\partial x^{b_n}}$ to complete a chart on $M$ to one on $T ^m _ n M$. In the following we denote such coordinates on $T^m_n M$ as $(x^m, v^{a_1 ... a_m}_{b_1 ... b_n})$. The coordinate representation of sections is then given as $G^{a_1 ... a_m}_{b_1 ... b_n}$. Although this is the standard way of representing tensor fields in coordinates it has two significant disadvantages. Firstly the notation gets increasingly complicated once products and contractions of higher ranked fields are involved. Secondly if the field bundle is a true sub bundle of $T^m_nM$ and hence has fibres with dimension less than $4^{(m+n)}$ the chart induced fields simply are to many functions to provide a valid chart on this sub bundle. This is for instance the case considering the field bundle for a metric tensor. The total space of this particular bundle is the space of symmetric rank $(0,2)$ tensors $S^2(T^{\ast}M)$. The typical fibre of the field bundle has dimension $10$. Nevertheless one usually uses the $16$ chart induced basis fields $ \frac{\partial}{\partial x^a}  \otimes \frac{\partial}{\partial x^b}$ to obtain coordinate expressions $g_{ab} = g(\frac{\partial}{\partial x^a},\frac{\partial}{\partial x^b})$ for a section of this field bundle. These component functions then necessarily satisfy $g_{ab} = g_{ba}$ which reduces the fibre dimension from $16$ to $10$. \\

As we are going to deal with the bundle $(F, \pi_F, M)$ in a rather abstract setting we do not follow this practise but stick to true coordinates $(x^m,v_A)$ on $F$. 
In order to relate the thus obtained intrinsic description of $F$ to the one that is provided by considering $F$ as being an embedded sub bundle in $T^m_n M$ we construct vector bundle isomorphisms that allow us to identify the two perspectives. We quickly recall the definition of a \textit{\textbf{(vector-)bundle morphism}}:
\begin{definition}[bundle morphism]
Given two bundles $(F_1, \pi_{F_1}, M)$ and $(F_2, \pi_{F_2}, N)$, a bundle morphism covering $h : M \rightarrow N$ is a smooth map $f : F_1 \rightarrow F_2$ such that the following diagram commutes.
\begin{center}
\begin{tikzpicture}
\node (M) at (0,0) {$M$};
\node (N) at (4,0) {$N$};
\node (F1) at (0,3) {$F_1$};
\node (F2) at (4,3) {$F_2$};
\draw [->] (M) -- node[pos=0.5, below] {$h$} (N);
\draw [->] (F1) -- node[pos=0.5, above] {$f$} (F2);
\draw [->] (F1) -- node[pos=0.5, left] {$\pi_{F_1}$} (M);
\draw [->] (F2) -- node[pos=0.5, right] {$\pi_{F_2}$} (N);
\end{tikzpicture}
\end{center}
\end{definition}
\begin{comment}
\begin{remark}
We normally denote a bundle morphism $f$ that covers $h$ as pair $(f,h)$. For the special case $M=N$ and $h=\mathrm{id}_M$ we shorten the notation to simply $f$. Also we might refer to a bundle morphism by the total space function $f$ if $f$ and $h$ are related via some specified construction, i.e. if $f$ can be obtained uniquely once $h$ is given. 
\end{remark}
\end{comment}
We often consider the case where the bundles carry additional structure on their fibres, for instance in the case of $(F, \pi_F, M)$ being a vector bundle an additional vector space structure. Then one is in particularly interested in those bundle morphisms that preserve the given structure. As an example we define a vector bundle morphism between vector bundles $(F_1, \pi_{F_1}, M)$ and $(F_2, \pi_{F_2}, N)$ as a bundle morphism $(f,h)$ that restricts to a linear map on the fibres of $F_1$, i.e. $\forall p \in M$ the restriction $f \vert_{\pi_{F_1}^{-1}(p)}$ defines a linear map.
With these definitions at hand we can now specify the maps that allow for the transition between the description of the field bundle $F$ as abstract, independent vector bundle and the one obtained from considering it as being embedded in some tensor bundle over $M$.
%Clarification needed
\begin{definition}[Intertwiner]\label{interDef}
Let $(F,\pi_F,M)$ be a vector bundle. We call a pair of vector bundle morphisms $(I,J)$ that cover $id_M$,
$I: F \rightarrow T^m_n M$ and $J: T^m_n M \rightarrow F $ that satisfy  $J \circ I = \mathrm{id}_F$ a pair of \textbf{\textit{intertwiners}} for the bundle $(F, \pi_F, M)$.
\end{definition}
The last requirement, namely the existence of a leftinverse to $I$ ensures that $\tilde{I} : F \rightarrow I(F) \subset T^m_nM$ defines a vector bundle isomorphism, and thereby allows us to relate the two distinct points of view.
\begin{comment}
\begin{remark}
Note that the situation is similar to the soldering of a vector bundle to a manifold, where the necessary data is a vector bundle isomorphism between the vector bundle and the tangent bundle of the manifold. The soldering then allows one to identify the vector bundle with the tangent bundle of the manifold. By requiring the existence of $J$, a left inverse of $I$ in the definition presented here, we ensure that $I$ is injective and hence defines an isomorphism if $T^m_n M$ is restricted to the image of $I$. The image of $I$ is obviously a vector sub bundle of $T^m_n M$. In total given the two maps $I$ and $J$ we can identify the field bundle $F$, not considered as being embedded in $T^m_n M$ but as abstract bundle  and with independent coordinate charts $(x^m, v_A)$, with a sub bundle of $T^m_n M$, where we can choose the usual chart induced coordinates $(x^m,v^{a_1 ... a_m}_{b_1 ... b_n})$.
\end{remark}
\end{comment}
%rewrite the above !!!!!
%
%
%
%
%
As the two intertwiner maps are linear maps, they can be understood as $I \in F^{\ast} \otimes T^m_n M$ and $J \in (T^m_nM)^{\ast} \otimes F \cong T^n_m M \otimes F$, where $F^{\ast}$ denotes the total space of the vector bundle dual. 
Once we specify intertwiners $(I,J)$ for the field bundle $F$ we immediately obtain intertwiners for the dual field bundle by means of the standard dual construction. We first take the dual of the intertwiner $I$, $I^{\ast}$. As taking the dual reverse the direction one usually proceeds by inverting $I^{\ast}$. Here this can not be done as $I$ is not invertible but only admits a left inverse. Hence its dual is not invertible. It is however well known that the existence of a left inverse for $I$ implies the existence of a right inverse for $I^{\ast}$. 
%
%add citation here ??
%
In the following we denote this map simply by $(I^{\ast})^{-1}$ but keep in mind that it only provides a right inverse of $I^{\ast}$. Vice versa the existence of a right inverse of $J$ guarantees the existence of a left inverse $(J^{\ast})^{-1}$ of its dual. From the definition of the dual map we further find that $(J^{\ast})^{-1} \circ (I^{\ast})^{-1} = id_{F^{\ast}}$. In total given a pair of intertwiners for the field bundle $(I,J)$ we can obtain a pair of intertwiners for the dual fiedl bundle via this construction, i.e. by $((I^{\ast})^{-1}, (J^{\ast})^{-1})$. \\

We can now choose adapted coordinates $(x^m,v_A)$ on the field bundle and the corresponding dual coordinates $(x^m, v^A)$ on the dual field bundle and chart induced coordinates\footnote{We denote coordinate induced tangent vectors by $\frac{\partial}{\partial x^m}$, the coordinate representation of the action of tangent vectors on functions is denoted as $\partial_m f$. } $(x^m, v^{a_1 ... a_m}_{b_1 ... b_n})$ on $T^m_n M$ and the dual coordinates $(x^m, v^{b_1 ... b_n}_{a_1 ... a_m})$ on its dual $T^n_mM$. Using these charts the maps $I$ and $J$ are represented as follows:
\begin{align} \label{interAbs}
    \begin{aligned}
    &I = I^{A a_1 ... a_m}_{b_1 ... b_n} \cdot v_A \otimes  v^{b_1 ... b_n}_{a_1 ... a_m}\\
    &J = J^{b_1 ... b_n}_{A a_1 ... a_m} \cdot v^A \otimes  v^{a_1 ... a_m}_{b_1 ... b_n}.
    \end{aligned}
\end{align}
Computing similar coordinate expressions for the pair of intertwiners of the dual field bundle $((I^{\ast})^{-1}, (J^{\ast})^{-1})$ one finds that:
\begin{align} \label{dualInterAbs}
    \begin{aligned}
         &(I^{\ast})^{-1} = J^{b_1 ... b_n}_{A a_1 ... a_m} \cdot v^A \otimes  v^{a_1 ... a_m}_{b_1 ... b_n}.\\
         &(J^{\ast})^{-1} = I^{A a_1 ... a_m}_{b_1 ... b_n} \cdot v_A \otimes  v^{b_1 ... b_n}_{a_1 ... a_m}.
    \end{aligned}
\end{align} 
Hence although the constructed intertwiners for the dual field bundle define different maps, their components w.r.t. a given chart are already given by the intertwiners of the field bundle $(I,J)$ if one agrees on chosing the appropriate dual coordinates with a given choice of coordinates. Therefore in the following we will drop the distinction of intertwiners for the field bundle and its dual.  This should not cause confusion, as when working with dual coordinates on the dual field bundle already the index position of the fibre coordinates $v_A$ and $v^{A}$ respectively contains the information regarding which intertwiner relates the these coordinates to those one $T^m_n M$ and $T^n_mM$ respectively. In total we find the following relations connecting the two ways of coordinatising the field bundle and its dual:  
\begin{align} \label{interRel}
    \begin{aligned}
    v^{a_1 ... a_m}_{b_1 ... b_n} = I^{A a_1 ... a_m}_{b_1 ... b_n} \cdot v_{A} \ \ &, \ \  
    v_A = J^{b_1 ... b_n}_{A a_1 ... a_m} \cdot v^{a_1 ... a_m}_{b_1 ... b_n}, \\
    v^{b_1 ... b_n}_{a_1 ... a_m} = J^{b_1 ... b_n}_{A a_1 ... a_m} \cdot v^A \ \ &, \ \ 
    v^A = I^{A a_1 ... a_m}_{b_1 ... b_n} \cdot v^{b_1 ... b_n}_{a_1 ... a_m}, \\
    I^{A a_1 ... a_m}_{b_1 ... b_n} &\cdot J^{b_1 ... b_n}_{B a_1 ... a_m} = \delta^A _ B.
    \end{aligned}
\end{align}
In particular we find that intertwiners defined in this way keep contractions between elements of $F$ and $F^{\ast}$ and all possible higher tensor products that can be obtained from these invariant. In other words, it is equivalent if we compute expressions using the chart induced coordinates or, by means of the intertwiners $(I,J)$, using $(x^m, v_A)$ and the corresponding dual coordinates. \\

In practise one can specify the coordinate expressions $I^{A a_1 ... a_m}_{b_1 ... b_n}$ and $J^{b_1 ... b_n}_{A a_1 ... a_m}$ in order to define coordinates $v_A$ in terms of the chart induced coordinates $v^{a_1 ... a_m}_{b_1 ... b_n}$, i.e. one constructs a valid pair of intertwiners with the desired properties and then defines the fibre coordinates on $F$ by
\begin{align}
    \begin{aligned}
    v_A := J^{b_1 ... b_n}_{A a_1 ... a_m} \cdot v^{a_1 ... a_m}_{b_1 ... b_n}\\
    v^A := I^{A a_1 ... a_m}_{b_1 ... b_n} \cdot  v^{b_1 ... b_n}_{a_1 ... a_m} .
    \end{aligned}
\end{align}
This approach can for instance be used if the field bundle is defined in terms of symmetries the tensor field has to satisfy. We use these symmetries to define an equivalence relation on the set of $4^{m+n}$ fibre coordinate functions $\left \{ v^{a_1 ... a_m}_{b_1 ... b_n} \ \vert \  a_1,...,a_m,b_1...b_n \in \{0,...,3 \} \right \}$ on $T^m_nM$. We call two such fibre coordinates $v^{a_1 ... a_m}_{b_1 ... b_n}$ and $v^{c_1 ... c_m}_{d_1 ... d_n}$ if once they are restricted to $F \subset T^m_nM$ they only differ by a sign:
\begin{align}\label{equivCord}
v^{a_1 ... a_m}_{b_1 ... b_n} \cong v^{c_1 ... c_m}_{d_1 ... d_n} : \iff \exists \epsilon \in \{-1,+1 \} : v^{a_1 ... a_m}_{b_1 ... b_n}\vert _F = \epsilon \cdot  v^{c_1 ... c_m}_{d_1 ... d_n}\vert_F.
\end{align}
Obviously removing all coordinate functions that restrict to identical zero on $F$ one then sorts the set of equivalence classes according to some ordering that can be specified at wish.  Note that the number of equivalence classes that we obtain in this way precisely corresponds to the fibre dimension of $F$. In the following we label the $A-th$ equivalence classes  starting from 0 by $[A]$. Next we select a representative out of each equivalence class.  The selection of a representative $v^{a_1 ... a_m}_{b_1 ... b_n}$ of a given equivalence class $[A]$ allows us to split this equivalence class into the subset $[A]_+$ containing those fibre coordinates that restrict to $+1 \times v^{a_1 ... a_m}_{b_1 ... b_n}$  on $F$ and the remaining class $[A]_-$ with thus according to (\ref{equivCord}) necessarily restricts to $-1 \times v^{a_1 ... a_m}_{b_1 ... b_n}$ on $F$.
With each equivalence class $[A]$ we further associate its \textbf{\textit{multiplicity}} $\sigma(A) = \vert [A] \vert$ which is simply given by the number of its elements.
Now we can define
\begin{align}\label{defI}
    I^{A a_1 ... a_m}_{b_1 ... b_n} = \begin{cases} +1 \ \text{if} \  v^{a_1 ... a_m}_{b_1 ... b_n} \in [A]_+ \\
    -1 \ \text{if} \  v^{a_1 ... a_m}_{b_1 ... b_n} \in [A]_-  \\
                       0 \  \text{otherwise}.
                            \end{cases}
\end{align}
$J^{b_1 ... b_n}_{A a_1 ... a_m}$ is then defined such that the pair of intertwiners satisfies the last condition in (\ref{interRel}), i.e. $J$ is chosen as left inverse of $I$. On readily finds that
\begin{align}\label{defJ}
    J^{b_1 ... b_n}_{A a_1 ... a_m} = \begin{cases}  +\frac{1}{\sigma(A)} \  \text{if} \  v^{a_1 ... a_m}_{b_1 ... b_n} \in [A]_+\\
    -\frac{1}{\sigma(A)} \  \text{if} \  v^{a_1 ... a_m}_{b_1 ... b_n} \in [A]_- \\ 
    0   \ \text{otherwise}.
    \end{cases}
\end{align}
Considering again the metric tensor as an example, we have $v_{ab} = v_{ba}$ as only symmetry of the $16$ coordinate functions on $T^0_2M$. We sort the equivalence classes as 
\begin{align}
    \left[[v_{00}], [v_{01}], [v_{02}], [v_{03}], [v_{11}], [v_{12}], [v_{13}], [v_{22}], [v_{23}], [v_{33}]\right ].
\end{align}
Proceeding along the lines outlined above we now obtain the intertwiner $I$ in terms of the components of the following matrix: 
\begin{equation}\label{interIMet}
I^A_{ab} = \begin{bmatrix}
                1 & & & & & & & & & \\
                & 1 & & & & & & & & \\
                & & 1 & & & & & & & \\
                & & & 1 & & & & & & \\
                & 1 & & & & & & & & \\
                & & & & 1 & & & & & \\
                & & & & & 1 & & & & \\
                & & & & & & 1 & & & \\
                & & 1 & & & & & & & \\
                & & & & & 1 & & & & \\
                & & & & & & & 1 & & \\
                & & & & & & & & 1 & \\
                & & & 1 & & & & & & \\
                & & & & & & 1 & & & \\
                & & & & & & & & 1 & \\
                & & & & & & & & & 1 
            \end{bmatrix}.
\end{equation}
Here the rows run over $(a,b)={(0,0),(0,1),(0,2),...,(3,3)}$ and the columns run from $0$ to $9$. Only non vanishing components are displayed. The inverse intertwiner is then given as: 
\begin{equation}\label{interJMet}
J^{ab}_{A} = \begin{bmatrix} 
                1 & & & & & & & & & & & & & & & \\
                & \frac{1}{2} & & & \frac{1}{2} & & & & & & & & & & &  \\
                & & \frac{1}{2} & & & & & & \frac{1}{2} & & & & & & &  \\
                & & & \frac{1}{2} & & & & & & & & & \frac{1}{2} & & &  \\
                & & & & & 1 & & & & & & & & & &  \\
                & & & & & & \frac{1}{2} & & & \frac{1}{2} & & & & & &  \\
                & & & & & & & \frac{1}{2} & & & & & & \frac{1}{2} & &  \\
                & & & & & & & & & & 1 & & & & &  \\
                & & & & & & & & & & & \frac{1}{2} & & & \frac{1}{2} &  \\
                & & & & & & & & & & & & & & & 1  
            \end{bmatrix},
\end{equation}
now with columns running over $(ab)={(0,0),(0,1),(0,2),...,(3,3)}$ and rows running from $0$ to $9$. By this definition we now have for the metric tensor field $v_1 \cong v_{01}, v_2 \cong v_{0,2},...$.
\begin{remark}
It might be tempting to define the pair of intertwiners in a way such that the matrix representation of $I$ equals the matrix transpose of the matrix representing $J$. This can be achieved by not defining $I$ without fractions and introducing the factors that result from the multiplicities $\sigma$ in $J$, but by splitting them evenly and introducing factors of $\frac{1}{\sqrt{\sigma}}$ both in the definition of $I$ and $J$. Although this seems advantageous there is one particular problem with this definition. The usage of squareroot factors might introduce irrational numbers into the intertwiners. As we are going to rely heavily on computer algebra later and the appearance of irrational numbers severely obstructs the computational performance we stick to the squareroot free approach.  
\end{remark}
%
%reference to problem of constructive gravity in introduction
In the end we want to describe the gravitational field as sections of such a field bundle and construct for it a Lagrangian and the corresponding equations of motion. For that reason it is necessary to introduce a notion of \textit{\textbf{deriving}} such sections. 

It is important to keep in mind that one of the aims of the constructive gravity approach is allowing for a deviation from metric gravity theories. Hence we are not dealing with spacetimes where the usual metric structures are present. In particular there is a priori no distinguished connection on the frame bundle and therefore no covariant derivative that can be used to define derivatives of tensor fields in a coordinate independent fashion. Note however that already in standard general relativity those notions are strictly speaking not necessary. Although there, the fundamental object, the Riemann tensor might be interpreted as curvature 2-form associated to the metric compatible connection on the frame bundle, in coordinates it can be fully written down in terms of the metric tensor its inverse and up to second partial derivatives of the metric without ever mentioning any deeper structure that these expressions might define. The same holds true for the Einstein-Hilbert-Lagrangian. 

We therefore proceed as follows: We keep the involved structures at a minimum, aiming to construct a Lagrangian solely in terms of the gravitational field and partial derivatives of the gravitational field up to some finite order, just as it is the case for the Einstein-Hilbert-Action\footnote{In other words we are not trying to construct structural analogies to general relativity or some other known theory of gravity.}. In order to nevertheless be able to describe partial derivatives of the gravitational field in a well defined, i.e. coordinate independent fashion we construct the $\textit{\textbf{jet bundle}}$ over the field bundle.  \\

The jet bundle over a given bundle can be intrinsically defined in quite abstract fashion. Such an approach nicely reveals its intrinsic properties. This is however not the route we are going to take her as a coordinate based approach to jet bundles is intuitively often more accessible. Furthermore such an approach emphasizes similarities to the construction of the tangent bundle of a manifold. Details regarding the more abstract access to jet bundles can be found in particular in the first chapters of \cite{seiler2009involution}. 
%remark on mathematical precision in the introduction, neglect global properties, etc.
We start by defining the \textbf{\textit{1-jet}} of a \textbf{\textit{section}}.
\begin{definition}[1-jet] 
Given a bundle $(F, \pi_F, M)$, with adapted coordinates $(x^m, v_A)$ and sections $G,H \in \Gamma(F)$, we call $G$ and $H$ 1-equivalent at $p \in M$, if $G(p) = H(p)$ and $\partial_i G_A \vert_p = \partial_i H_A \vert_p$. The corresponding equivalence class of a section $G$ at $p$ is then called the 1-jet of $G$ at $p$ and denoted by $j^1_pG$.
\end{definition}
Here $G_A$ and $H_A$ denote as before the chart representations of the two sections w.r.t. the specified adapted coordinates. One can easily check that this indeed defines an equivalence relations and is independent of the chosen coordinate chart, i.e. well defined. Details can be found in \cite{saunders_1989}. From this definition one immediately sees the similarity to the construction of tangent vectors as equivalence classes of curves. Just as in the case of tangent vectors, where one collects all tangent vectors to obtain the tangent bundle, we can now collect all such 1-jets of sections of $F$ to obtain the first order jet bundle $J^1F$ over $F$. 
\begin{definition}[first jet bundle]
Let $(F, \pi_F, M)$ be a bundle, the first jet bundle over $F$, $(J^1F,\pi_1,M)$ is the bundle with total space $J^1F := \{j^1_pG \ \vert \  p \in M, G \in \Gamma(F)\}$ and bundle projection 
\begin{align}
    \begin{aligned}
\pi_1 : &J^1F \longrightarrow M \\
&j^1_pG \longmapsto p.
    \end{aligned}
\end{align}
\end{definition}
\begin{remark}
Note that by this definition $J^1F$ is a bundle over the base manifold $M$. We can also equip $J^1F$ with the projection 
\begin{align}
    \begin{aligned}
    \pi_{1,0} : &J^1F \longrightarrow F \\
    &j^1_pG \longmapsto G(p)
    \end{aligned}
\end{align}
By doing so we additionally provide $J^1F$ with the structure of a bundle over $F$, namely $(J^1F,\pi_{1,0},F)$. From the definition of the two projection we furthermore find that $\pi_1 = \pi_F \circ \pi_{1,0}$. 
\end{remark}
Given adapted coordinates $(x^m,v_A)$ on $F$ we get induced adapted coordinates $(x^m, v_A, v_{Ai})$ on $J^1F$ by defining as always $x^m(j^1_pG) := x^m(p)$, $v_A(j^1_pG) := v_A(G(p))$ and now specifying the new $4 \cdot n$ coordinate functions $v_{Ai}$ by $v_{Ai}(j^1_pG) := \partial_iG_A \vert_p$.
We could now define \textit{\textbf{higher order jet bundles}} in similar fashion, by calling two sections of $F$ \textit{\textbf{q-equivalent}} or equivalent in order $q$ at $p \in M$ if in any coordinate system covering $p$ they agree in their first $q$ partial derivatives evaluated at $p$. There are however arguments why we do not have to work with jet bundles higher than second order if we wish to define a physically meaningful Lagrangian field theory.

As we want to use the jet-bundle formalism to define a Lagrangian and then obtain equations of motion (in short EOM) for the gravitational field as the corresponding Euler-Lagrange Equations the order of the jet bundle is ultimately connected to the order of the highest derivative appearing in the EOM. In the following we want to restrict to cases where the EOM are of no higher than second derivative order. The reason for this restriction is the famous \textbf{\textit{Ostrogratsky}} construction \cite{Ostrogradsky:1850fid}. 
% review this statement 

Simply stated starting from Lagrangians that generate equations of motion with higher than second order time derivatives one obtains instabilities in the associated Hamiltonian formulation. From the perspective of covariant Lagrangian field theory there is no distinct time direction and hence the only way of preventing the theory from these kind of instabilities once we pass to the Hamiltonian formulation is by restricting the Lagrangian such that the generated EOM are of second derivative order altogether. For further information see \cite{2015arXiv150602210W}. 

The appropriate setting for such Lagrangians is the \textbf{\textit{second order jet bundle}}. Although completely analog to the definitions involved in the construction of the first order jet bundle we quickly define the necessary ingredients. 
\begin{definition}[2-jet]
Given a bundle $(F, \pi_F, M)$ with adapted coordinate chart $(x^m, v_A)$ and sections $G,H \in \Gamma(F)$, we call $G$ and $H$ 2-equivalent at $p \in M$, if $G(p) = H(p)$, $\partial_i G_A \vert_p = \partial_i H_A \vert_p$ and $\partial_i \partial_j G_A \vert_ p = \partial_i \partial_j H_A \vert_ p $. The corresponding equivalence class of a section $G$ at $p$ is then called the 2-jet of $G$ at $p$ and denoted by $j^2_pG$.
\end{definition}
Again one can show that this is well defined \cite{saunders_1989}. We collect all such 2-jets for all sections of $F$ and all points in $M$ to obtain the second order jet bundle over $F$.
\begin{definition}[second jet bundle]
Let $(F, \pi_F, M)$ be a bundle, the second jet bundle $(J^2F,\pi_2,M)$ is the bundle with total space $J^2F := \{j^2_pG \  \vert \  p \in M, G \in \Gamma(F)\}$ and bundle projection 
\begin{align}
    \begin{aligned}
\pi_2 : J^2F \longrightarrow M \\
j^2_pG \longmapsto p.
    \end{aligned}
\end{align}
\end{definition}
\begin{remark}
One can again show that this really defines a bundle over the base manifold $M$. In the case of the second order jet bundle we can obtain two further bundle projections 
\begin{align}
    \begin{aligned}
    \pi_{2,1} : &J^2F \longrightarrow J^1F \\
    &j^2_pG \longmapsto j^1_pG ,\\
    \pi_{2,0} : &J^2F \longrightarrow F \\
    &j^2_pG \longmapsto G(p).
    \end{aligned}
\end{align}
These bundle projections provide $J^2F$ with the additional structure of a bundle over $J^1F$ with projection $\pi_{2,1}$ or a bundle over $F$ with projection $\pi_{2,0}$. Note that from the various definitions we find $\pi_{2} = \pi_F \circ \pi_{2,0} = \pi_F \circ \pi_{1,0} \circ \pi_{2,1}$.
\end{remark}
As before we can obtain adapted coordinates $(x^m, v_A, v_{Ai},v_{Aij})$ of $J^2F$ from adapted coordinates $(x^m,v_A)$ on $F$ by defining $x^m(j^2_pG) := x^m(p)$, $v_A(j^2_pG):= v_A(G(p))$, $v_{Ai}(j^2_pG) := v_{Ai}(j^1_pG)$ and $v_{Aij}(j^2_pG) := \partial_i \partial_j G_A \vert _p$. 
At this point however we run into similar troubles as before, when we tried to use coordinate induced basis vectors to construct a chart on $F$. 

We have specified $4^2\cdot n$ new coordinate functions $v_{Aij}$ to construct adapted coordinates on $J^2F$. Due to the commutativity of the partial derivatives that are involved in the definition the $v_{Aij}$ actually only constitute $4\cdot (4+1)/2 \cdot n = 10\cdot n$ independent functions. To cope with such problems the standard approach is the introduction of multiindices for higher order jet bundles. We will follow a slightly different approach by making use of the same intertwiner technique that we developed before. By doing so it is easier for us to relate the derivative coordinates $v_{Aij}$ and possible higher jet coordinates with partial derivatives of sections in standard physics notation. 

We simply proceed along the same lines as before when we constructed a pair of intertwiners for the bundle of symmetric $(0,2)$ tensor fields, i.e. we provide a pair of intertwiners and thereby define $10$ new indices that label second order derivative in terms of the $16$ possible pairs of spacetime indices:
\begin{align}
    \begin{aligned}
    v_{Aij} = I^I_{ij} v_{AI}\\
    v_{AI} = J_I^{ij} v_{Aij}.
    \end{aligned}
\end{align}
We divide the $16$ coordinates that label symmetric second order derivatives $(ij)$ into the $10$ equivalence classes 
\begin{align}
    \left [[(00)],[(01)],[(02)],[(03)],[(11)],[(12)],[(13)],[(22)],[(23)],[(33)] \right ].
\end{align}
We then define the intertwiners as before by (\ref{defI}) and (\ref{defJ}). 
This choice is given by the intertwiners specified by the matrices displayed in (\ref{interIMet}) and (\ref{interJMet}) that were obtained as pair of intertwiners for a metric tensor field. The reason for this coincidence is clear as in both cases we are dealing with symmetric index pairs. \\

\begin{remark}
As stated above during this thesis we will in fact never perform explicit computations on a jet bundle that is of higher than second order. In order to nevertheless be able to provide some of the relevant definitions on jet bundles of arbitrary order we will in the following denote adapted coordinates on $J^qF$ by 
\begin{align}
(x^m,v_A,v_{Am},v_{AI},v_{AI_{3}},...,v_{AI_{q}}),
\end{align}
where we introduced new indices $I_k$ that range from $0$ to $\binom{4+k-1}{k}-1$ and label the independent combinations of partial derivatives of order $k$. In the follwoing when displaying formulae that include such indices $I_k$ we might write $v_{AI_0} \equiv v_m, v_{AI_1} \equiv v_A$ and $v_{AI_2} \equiv v_{AI}$, such that we can concisely write expressions that involve sums involving all such indices:
\begin{align}
    u^mv_m + u^Av_A + u^{AI}v_{AI} + ... + u^{AI_q}v_{AI_q} = \sum _{k = 0}^q u^{AI_k}v_{AI_k}.
\end{align}
Note that we use no summation convention over $k$.
As before we denote the corresponding intertwiners mapping between the representation of derivative in terms of such indices and the spacetime representation as
\begin{align}
    \begin{aligned}
    v_{Ai_1...i_k} = I^{I_k}_{i_1..i_k} v_{AI_k}\\
    v_{AI_k} = J_{I_k}^{i_1...i_k} v_{Ai_1...i_k},
    \end{aligned}
\end{align}
with the obvious intertwiners for $k=0,..2$.
They can in fact be obtain completely analog to the above example we will however never need their explicit expression. 
The projections that project from a given $q$-th order jet bundle to the $k$-th order jet bundle with $k< q$ are denoted as before by $\pi_{q,k}$ with $\pi_q = \pi_F \circ \pi_{q,0}$.
\end{remark}

%rethink notation
There are certain operation that naturally can be performed when working in the context of the jet-bundle formalism.
Given a section of the field bundle $G \in \Gamma(F)$ we can \textit{\textbf{prolong}} it to a section of $J^1F$, $J^2F$ or in fact any higher order jet bundle $J^qF$ by at each point $p \in M$ taking its 1-jet $j^1_pG$, 2-jet $j^2_pG$ or  q-jet $j^q_pG$ respectively. The section of the q-th order jet bundle $J^qF$ that is obtained this way is called the q-th \textbf{\textit{jet prolongation}} of $G$ and is denoted as $j^q(G)$. In adapted coordinates this is simply achieved by appending the appropriate derivatives of the section, for instance the coordinate representation of $j^2(G)$ is given by the map $x^m \mapsto (x^m, G_A, \partial_i G_A, \partial_I G_A)$, where $\partial_I := J_I^{ij} \partial_i \partial_j$.

Given a function $f \in C^{\infty}(F)$, with coordinate expression $f(x^m, v_A)$ we can compute its \textit{\textbf{total derivative}} as $D_if = \partial_i f + v_{Ai}\cdot \partial^A f$, which yields a function on $J^1F$. Here the expression $\partial^A f$ denotes the action of the coordinate induced vector fields $\frac{\partial}{\partial v_A}$ on the coordinate expression of $f$. Similarly if $h \in C^{\infty}(J^1F)$ its total derivative $D_i h = \partial_i h + v_{Ai} \partial^A h + v_{AI}I^I_{ij} \partial ^{Aj} h$ defines a function on $J^2F$. In the same way we can compute total derivatives for functions on any higher $q$-th order jet bundle $J^qF$ to obtain a function on $J^{q+1}F$:
\begin{align}\label{totDer}
    D_j g = \sum _{k = 0}^{q}  v_{AI_{k+1}}I^{I_{k+1}}_{ji_1...i_k}J_{I_k}^{i_1...i_k}\partial^{AI_k} g.
\end{align}
 

Lastly, with the definition of the total derivative at hand, given a function on some finite order jet bundle $f \in C^{\infty}(J^qF)$ with coordinate expression $f(x^m,v_A,v_{Am},v_{AI},v_{AI_{3}},...,v_{AI_{q}})$ we define its \textit{\textbf{variational derivative}} as 
\begin{align}\label{varDer}
\frac{\delta f}{\delta v_A} := \partial^{A}g + \sum _{k = 1}^q (-1)^k D_{I_k}(\partial^{AI_k}f),
\end{align}
where $D_I := J_I ^{ij} D_i D_j$ and similar $D_{I_k} := J_{I_k}^{i_1...i_k} D_{i_1} ... D_{i_k}$.
Note that in particular the variational derivative of a function on the $q$-th order jet-bundle in general yields a function on the $2q$-th order jet bundle.\\

Now we have finally developed the necessary structure to give a precise definition of a \textbf{\textit{Lagrangian}} for a field theory whose dynamical objects are sections of the field bundle $(F, \pi_F,M)$.
\begin{definition}[Lagrangian]
A second order Lagrangian on $(F,\pi_F,M)$ is a bundle map $\mathcal{L} : J^2F \rightarrow \Lambda^4M$ that covers $id_M$.
\end{definition}
Here $\Lambda^4 M$ denotes the volume bundle on $M$. Alternatively we could have required $\mathcal{L}$ to take values in the bundle of scalar densities of weight 1 over $M$. In coordinates we have 
\begin{align}
    \mathcal{L}(x^m,v_A,v_{Ai},v_{AI}) = L(x^m,v_A,v_{Ai},v_{AI}) \mathrm{d}x^0 \wedge \mathrm{d}x^1 \wedge \mathrm{d}x^2 \wedge \mathrm{d}x^3 = L \mathrm{d}^4x.
\end{align} 

As the Lagrangian is a volume form valued bundle map we can take any section $G \in \Gamma(F)$, prolong it to $j^2(G) \in \Gamma(J^2F)$ and compose the result with $\mathcal{L}$ to obtain section of the volume bundle over $M$, $\mathcal{L}(j^2(G)) \in \Gamma(\Lambda^4M)$ which thus can be integrated over spacetime regions. Given a Lagrangian this procedure defines a \textit{\textbf{local action functional}} on the space of fields $\Gamma(F)$:
\begin{align}
\begin{aligned}
    &\mathcal{S}_{\mathcal{L}} : \Gamma(F) \longrightarrow \mathcal{R} \\
    &G \longmapsto \mathcal{S}_{\mathcal{L}}[G] := \int \mathcal{L}(j^2(G)).
\end{aligned}
\end{align}
The total situation with the various maps that are involved in this construction is displayed in diagram (\ref{diagram1}). 
\begin{figure}[ht]
\centering
\begin{tikzpicture}
\node (M) at (0,0) {$M$};
\node (F) at (0,2) {$F$};
\node (J1) at (0,4) {$J^1F$};
\node (J2) at (0,6) {$J^2F$};
\node (Vol) at (6,6) {$\Lambda^4M$};
\draw [->] (F) -- node[pos=0.35, right] {$\pi_F$} (M);
\draw [->] (J1) -- node[pos=0.35, right] {$\pi_{1,0}$}  (F);
\draw [->] (J2) -- node[pos=0.35, right] {$\pi_{2,1}$}  (J1);
\draw [->] (Vol) -- node[pos=0.4, left] {$\pi_{\Lambda^4M}$}  (M);
\draw[->] (M) .. controls (-0.75,0.5) and (-0.75,1.5) .. node[pos=0.5, left] {$G$} (F);
\draw[->] (M) .. controls (-3,1) and (-3,5) .. node[pos=0.5, left] {$j^2(G)$} (J2);
\draw [->] (J2) -- node[pos=0.5, above] {$\mathcal{L}$}  (Vol);
\draw[->] (M) .. controls (3,0.5) and (6,3) .. node[pos=0.5, right] {$\mathcal{L}\circ j^2(G)$} (Vol);
\end{tikzpicture}
\caption{Commutative diagram for Lagrangian field theory on $J^2F$} \label{diagram1}
\end{figure}
Note that by giving a proper definition of the Lagrangian as a bundle map between $J^2F$ and $\Lambda^4M$ where are able to fully encode the properties of the corresponding action functional in terms of a smooth map between finite dimensional manifold. At no place we actually have to work over infinite dimensional spaces of sections and functionals defined on them. In particular when working with the Lagrangian $\mathcal{L}$ we can apply the standard techniques of differential geometry. 
\section{Lagrangian Equivariance Equations}
One of the key requirements that constructive gravity poses on the to-be-constructed dynamics for the gravitational field is their invariance under spacetime diffeomorphisms. In the following we show how this requirement can be incorporated in the previously developed framework of Lagrangian field theory. We start by computing the transformation behaviour of 2-jets $j_p^2G \in J^2F$ under spacetime diffeomorphisms.
%Comment on the necessarity of this requirement ?? 

The action of diffeomorphisms on $M$ naturally lifts to any tensor space over $M$ by the usual push-forward pull-back construction. To be more precise given $\phi \in \mathrm{Diff}(M)$ we can obtain vector bundle isomorphisms $\phi_{\ast} : TM \rightarrow TM $ and $\phi^{\ast} : T^{\ast}M \rightarrow T^{\ast}M$ that cover $\phi$ by taking its pushforward and pullback respectively. As the pull back is a contravariant functor, i.e. $(\phi \circ \psi)^{\ast}=\psi^{\ast}\circ \phi^{\ast}$ given $\phi \in \mathrm{Diff}(M)$ we simply take the pullback along $\phi^{-1}$. The action of diffeomorphisms on tensor spaces over $M$, $T^m_nM$ can then be obtained by tensoring m copies of the pushforward action and n copies of the inverse pull back action. In other words given $\phi \in \mathrm{Diff}(M)$ we get a vector bundle isomorphism on any $T^m_nM$ by:
\begin{align}
\begin{aligned}
    &\underbrace{\phi_{\ast} \otimes ... \otimes \phi_{\ast}}_{\substack{m \  times}} \otimes \underbrace{(\phi^{-1})^{\ast} \otimes ... \otimes (\phi^{-1})^{\ast} }_{\substack{n \  times}} : T^m_nM \longrightarrow T^m_nM \\
    u_1 \otimes ... \otimes u_m &\otimes \omega_1 \otimes ... \otimes \omega_n \longmapsto \phi_{\ast} (u_1) \otimes ... \otimes \phi_{\ast}(u_m) \otimes (\phi^{-1})^{\ast}(\omega_1) \otimes ... \otimes (\phi^{-1})^{\ast}(\omega_n).
\end{aligned}
\end{align}
The action is then defined on all other elements of $T^m_nM$, i.e. on general linear combinations of product elements, by extending it linearly.\\

Note that there is a slight duplication in our notation: Before a superscripted $\ast$ denoted the dual of a linear map, the dual of a vector-space/bundle, etc. Here the superscripted $\ast$ denotes the pull back of a given diffeomorphism. The connection between the to notions and henceforth the reason for the apparent duplication in the notation can be explained regarding the standard definition of the pullback: Let $\phi \in \mathrm{Diff}(M)$, $p \in M$, $u \in T_pM$, and  $\omega \in T^{\ast}_{\phi(p)}M$ then pushforward and pullback of $\phi$ restrict to linear maps $(\phi_{\ast}) _p :  T_pM \rightarrow T_{\phi(p)}M$ and $\phi^{\ast}_{\phi(p)} : T^{\ast}_{\phi(p)}M \rightarrow T^{\ast}_pM$, where the pullback is defined by the requirement $\phi^{\ast}_{\phi(p)}\omega (u) := \omega ((\phi_{\ast})_p u)$. Comparing this with definition \ref{dual} the dual of a linear map we see that the pull back really defines the fibre wise dual of the push forward. 

To shorten the notation we will refer to actions of $\mathrm{Diff}(M)$ on the various tensor spaces $T^m_nM$ that can be obtained by means of this pushforward pullback construction simply as pullback actions and denote the corresponding bundle morphisms that cover a given $\phi \in \mathrm{Diff}(M)$ collectively by $\phi_{\ast}$. Which precise tensorbundle is meant can normally be inferred from the context quite easily.\\

Given $\phi \in \mathrm{Diff}(M)$, by this construction we can obtain the corresponding action by bundle isomorphisms on any tensor bundle over $M$. As we have identified the field bundle $F$ with a subbundle of some $T^m_nM$ we can restrict the bundle isomorphisms to $F$ to get an action of $\mathrm{Diff}(M)$ on the field bundle in particular. 
In order to further lift the action of $\mathrm{Diff}(M)$ to the first jet bundle $J^1F$ and the second jet bundle $J^2F$ respectively we need to develop additional techniques, the main tool being the \textbf{\textit{prolongation of bundle morphisms}}. 
\begin{definition}[prolongation of morphisms]
Let $(F_1,\pi_{F_1},M)$ and $(F_2,\pi_{F_2},N)$ be bundles, $\phi : M \rightarrow N$ a diffeomorphism, $f : F_1 \rightarrow F_2$ a bundle morphism covering $\phi$ and $J^1F_1$, $J^1F_2$ the first order jet bundles over $F_1$ and $F_2$ respectively. The first jet bundle lift or prolongation of the bundle morphism $(f,\phi)$ is given by the unique map\footnote{Just as it is the case with bundle morphisms being denoted by pairs $(f,\phi)$ their prolongation should be denoted by triples $(j^1(f),f,\phi)$. However we will lighten their notation to simply $j^1(f)$ if the two remaining maps are clear from the given context.} $j^1(f)$ that lets the following diagram commute.
\begin{center}
\begin{tikzpicture}
\node (M) at (0,0) {$M$};
\node (N) at (5,0) {$N$};
\node (F1) at (0,3) {$F_1$};
\node (F2) at (5,3) {$F_2$};
\node (JF1) at (0,6) {$J^1F_1$};
\node (JF2) at (5,6) {$J^1F_2$};
\draw [->] (M.10) -- node[pos=0.5, above] {$\phi$} (N.170);
\draw [<-] (M.350) -- node[pos=0.5, below] {$\phi^{-1}$} (N.190);
\draw [->] (F1) -- node[pos=0.5, above] {$f$} (F2);
\draw [->] (F1) -- node[pos=0.5, left] {$\pi_{F_1}$} (M);
\draw [->] (F2) -- node[pos=0.5, right] {$\pi_{F_2}$} (N);
\draw [->] (JF1) -- node[pos=0.5, left] {$(\pi_1)_{1,0}$} (F1);
\draw [->] (JF2) -- node[pos=0.5, right] {$(\pi_2)_{1,0}$} (F2);
\draw [->] (JF1) -- node[pos=0.5, above] {$j^1(f)$} (JF2);
\end{tikzpicture}
\end{center}
\end{definition}
The explicit expression for the prolongation of $(f,\phi)$ is then given by
\begin{align}
    j^1(f)(j^1_pG) = j^1_{\phi(p)}(f\circ G).
\end{align}
We can now investigate how the prolongation $j^1(f)$ acts on the prolongation of sections of $F_1$.
Given a section $G \in \Gamma(F_1)$ we can define a section of $F_2$ by making use of the required invertibility of $\phi$: 
\begin{align}
\Gamma(F_2) \ni \widetilde{f}(G) := f \circ G \circ \phi^{-1}.
\end{align}
Aside from that we can also prolong the section to obtain a section $j^1(G) \in \Gamma(J^1F_1)$. Obviously we can now apply the same construction to the prolonged section to obtain 
\begin{align}
\Gamma(J^1F_2) \ni \widetilde{j^1(f)}(G) := j^1(f) \circ j^1(G) \circ \phi^{-1}.
\end{align}
From the definition of the prolongation of bundle morphisms we then obtain:
\begin{align}
    \widetilde{j^1(f)}(G) = j^1(\widetilde{f}(G)).
\end{align}
In other words the induced action of bundle morphisms on sections commutes with the operation of prolonging the involved section and bundle morphisms respectively. We can either first map a section of $F_1$ by using the bundle morphisms $(f,\phi)$ to a section of $F_2$ and then prolong it to a section of $J^1F_2$ or we prolong it first to a section of $J^1F_1$ and then map it with the prolonged bundle morphism $(j^1(f),f,\phi)$ to a section of $J^1F_2$, the result is the same.  \\

We can prolong bundle morphisms to any higher order jet bundle by an equivalent definition, i.e. by defining $j^k(f)$ s.t. the following diagram commutes:
\begin{center}
\begin{tikzpicture}
\node (M) at (0,0) {$M$};
\node (N) at (5,0) {$N$};
\node (F1) at (0,3) {$F_1$};
\node (F2) at (5,3) {$F_2$};
\node (JF1) at (0,6) {$J^kF_1$};
\node (JF2) at (5,6) {$J^kF_2$};
\draw [->] (M.10) -- node[pos=0.5, above] {$\phi$} (N.170);
\draw [<-] (M.350) -- node[pos=0.5, below] {$\phi^{-1}$} (N.190);
\draw [->] (F1) -- node[pos=0.5, above] {$f$} (F2);
\draw [->] (F1) -- node[pos=0.5, left] {$\pi_{F_1}$} (M);
\draw [->] (F2) -- node[pos=0.5, right] {$\pi_{F_2}$} (N);
\draw [->] (JF1) -- node[pos=0.5, left] {$(\pi_1)_{k,0}$} (F1);
\draw [->] (JF2) -- node[pos=0.5, right] {$(\pi_2)_{k,0}$} (F2);
\draw [->] (JF1) -- node[pos=0.5, above] {$j^k(f)$} (JF2);
\end{tikzpicture}
\end{center}
One can in particular show that the jet prolongations of bundle morphisms behaves functorial w.r.t. their composition: $j^k(f\circ g) = j^k(f) \circ j^k (g)$. Details regarding the prolongation of bundle morphisms can be found in \cite{saunders_1989}.\\

Now we are in a position where we can formulate what we mean by requiring the gravitational theory to be invariant under spacetime diffeomorphisms in precise manner. We start by recalling the definition of an \textit{\textbf{equivariant}} map. Further information regarding this can be found in \cite{doi:10.1142/3867}.
\begin{definition}[equivariance]
Let $M$ and $N$ be some spaces that both carry an action of a group $\mathcal{G}$, $\rho : \mathcal{G} \rightarrow \mathrm{Aut}(M)$ and $\sigma : \mathcal{G} \rightarrow \mathrm{N}$. We call a function $f : M \rightarrow N$ equivariant w.r.t. $\rho$ and $\sigma$ if it holds for all $g \in \mathcal{G}$ that: $f \circ \rho(g) = \sigma(g) \circ f(m)$, i.e. if the following diagram commutes for all $g \in \mathcal{G}$:
\begin{center}
\begin{tikzpicture}
\node (M) at (0,0) {$M$};
\node (N) at (4,0) {$N$};
\node (M2) at (0,3) {$M$};
\node (N2) at (4,3) {$N$};
\draw [->] (M) -- node[pos=0.5, below] {$f$} (N);
\draw [->] (M2) -- node[pos=0.5, above] {$f$} (N2);
\draw [<-] (M) -- node[pos=0.5, left] {$\rho(g)$} (M2);
\draw [<-] (N) -- node[pos=0.5, right] {$\sigma(g)$} (N2);
\end{tikzpicture}
\end{center}
\end{definition}
Finally we provide a definition for a \textit{\textbf{diffeomorphism invariant Lagrangian field theory}}.
\begin{definition}
A Lagrangian field theory described by a second order Lagrangian $\mathcal{L} : J^2F \rightarrow \Lambda^4 M$ is called diffeomorphism invariant if $\mathcal{L}$ is equivariant w.r.t. the action of $\mathrm{Diff}(M)$ that is obtained by prolonging diffeomorphisms to bundle isomorphisms of $J^2F$ and the pullback action of diffeomorphisms on $\Lambda^4M$, i.e. if it holds for all $\phi \in \mathrm{Diff}(M)$ that: 
\begin{align}
     \mathcal{L}\circ j^2(\phi_{\ast}) = \phi_{\ast} \circ \mathcal{L}.
\end{align}
\end{definition}

\textit{\textbf{Infinitesimally}}, on the \textbf{\textit{Lie algebra}} level, diffeomorphisms are described by vector fields $\xi \in \Gamma(M)$ with Lie bracket being provided by the commutator of vector fields. 
%comment on suttleties of diff(M)?? -> sign difference ??
As usual given a Lie group action of some Lie group $\mathcal{G}$ on some manifold $M$, i.e. $\rho : \mathcal{G} \rightarrow \mathrm{Aut}(M)$, one defines for all $p \in M$ the the orbit map:
\begin{align}
    \begin{aligned}
    \rho_p : &\mathcal{G} \longrightarrow M \\
    &g \longmapsto \rho(g)(p).
    \end{aligned}
\end{align}
This map is differentiable. Its push forward at the identity of $\mathcal{G}$ yields for each element of the Lie algebra of $\mathcal{G}$ a tangent vector at $p$ . Repeating this process for each $p \in M$ one obtains a vector field on $M$. It can then be shown that mapping each $\xi \in \mathrm{Lie}(\mathcal{G})$ to the negative of this vector field defines a morphism of Lie algebras $\mathrm{Lie}(\mathcal{G}) \rightarrow \Gamma(TM)$. Given $\xi \in \mathrm{Lie}(\mathcal{G})$ the image under this map is called the fundamental vector field corresponding to $\xi$. For details see for instance \cite{boothby1989} and \cite{doi:10.1142/3867}.\\

Hence also for the special case of the pullback action of diffeomorphism on tensor bundles $T^m_nM$ over $M$ we can construct a \textit{\textbf{Lie algebra morphism}} $\Gamma(TM) \rightarrow \Gamma(T(T^m_nM))$ as described above. Given a $\xi \in \Gamma(TM)$ the induced action of the corresponding vector field on $T^m_nM$ obtained via the outlined construction on sections $G \in \Gamma(T^m_nM)$ is usually called the \textit{\textbf{Lie derivative}} and denoted by $G \mapsto \mathcal{L}_{\xi}G$. \\
%Check this section !!!!!!!!!!!!!!

Applying this construction to the pullback actions of diffeomorphisms on the field bundle in coordinates $x^m$ on $M$, adapted coordinates $(x^m, v_A)$ on $F$ and the corresponding induced coordinates on $TM$ and $TF$ respectively, this Lie algebra morphism takes the following form:
\begin{align}\label{LieF}
\begin{aligned}
    \mathcal{f} : \Gamma(TM) &\longrightarrow \Gamma(TF)\\
    \xi &\longmapsto \xi_F .
\end{aligned}
\end{align}
And in adapted coordinates we obtain the following general expression for $\xi_F$:
\begin{align}
   \xi_F = \xi^m \frac{\partial}{\partial x^m} + \xi^A \frac{\partial}{\partial v_A} = \xi^m \frac{\partial}{\partial x^m} + C_{An}^{Bm} v_B \partial_m \xi ^n \frac{\partial}{\partial v_A}. 
\end{align}
%additionaly state how the tensors can be calculated ???
Here the constant tensor $C_{An}^{Bm}$ depend on the specific nature, i.e. rank and symmetries of the tensor fields in consideration.
It can most easily be computed by making use of the standard rules of computing Lie derivatives in coordinates and further using:
\begin{align}
    \mathcal{L}_{\xi} G_A = \partial_m G_A \xi^m + C_{An}^{Bm} G_B \partial_m \xi ^n.
\end{align}
The only feature that all various $C_{An}^{Bm}$ have in common, irrespective of the specific field bundle at hand is that they will always be given in a way that ensures that $\mathcal{f}$ defines a Lie algebra morphism, i.e 
\begin{align}
\mathcal{f}\left ( \left [\xi, \tilde{\xi} \right ]_{\Gamma(TM)}\right ) = \left [ \mathcal{f}\left (\xi\right ), \mathcal{f}\left (\tilde{\xi}\right ) \right]_{\Gamma(TF)}
\end{align}
Similarly we can now computed the Lie algebra action that corresponds to the prolonged actions on $J^1F$ and $J^2F$. By doing this we obtain \textbf{\textit{prolonged}} vector fields that describe the infinitesimal diffeomorphism action on the jet bundle. In coordinates $(x^m,v_A,v_{Ai})$ on $J^1F$ we obtain:
\begin{align}
    \begin{aligned}
    j^1(\mathcal{f}) : \Gamma(TM) &\longrightarrow \Gamma(TJ^1F)\\
    \xi & \longmapsto \xi_{J^1F}.
    \end{aligned}
\end{align}
Where we can compute the thus prolonged vector field $j^1(\xi)$ to be given by 
\begin{multline}\label{LieJ1}
    \xi_{J^1F} = \xi^m \frac{\partial}{\partial x^m} + C_{An}^{Bm} v_B \partial_m \xi ^n \frac{\partial}{\partial v_A}\\
    + C_{An}^{Bm} \partial_m \xi^n v_{Bi} \frac{\partial}{\partial v_{Ai}} - v_{An} \partial_m \xi ^n \frac{\partial}{\partial v_{Am}} + C_{An}^{Bm} v_B \partial_m \partial_p \xi^n \frac{\partial}{\partial v_{Ap}}.
\end{multline}
In exactly the same way one can now construct prolongations of vectorfield to higher order jet bundles as for instance to the second order jet bundle as it is relevant for our treatment of Lagrangian field theory.
\begin{align}
    \begin{aligned}
    j^2(\mathcal{f}) : \Gamma(TM) &\longrightarrow \Gamma(TJ^2F)\\
    \xi & \longmapsto \xi_{J^2F}.
    \end{aligned}
\end{align}
Performing a similar computation as before in adapted coordinates $(x^m,v_A,v_{Ai}, v_{AI})$ on $J^2F$ we find:
\begin{multline}\label{LieJ2}
    \xi_{J^2F} = \xi^m \frac{\partial}{\partial x^m} + C_{An}^{Bm} v_B \partial_m \xi ^n \frac{\partial}{\partial v_A}
    + C_{An}^{Bm} \partial_m \xi^n v_{Bi} \frac{\partial}{\partial v_{Ai}}\\
    - v_{An} \partial_m \xi ^n \frac{\partial}{\partial v_{Am}} + C_{An}^{Bm} v_B \partial_m \partial_p \xi^n \frac{\partial}{\partial v_{Ap}} 
    + C_{An}^{Bm} v_{BI} \partial_m \xi ^n \frac{\partial}{\partial v_{AI}}\\
    - 2 v_{BJ} I^J_{an}J^{am}_I \partial_m \xi^n \frac{\partial}{\partial v_{AI}} + 2 C_{An}^{Bm} v_{Ba}J^{ap}_I \partial_m \partial_p \xi^n \frac{\partial}{\partial v_{AI}}\\
    - v_{An} J^{pm}_I \partial_m \partial_p \xi^n\frac{\partial}{\partial v_{AI}} + C_{An}^{Bm} v_B J^{pq}_I \partial_m \partial_p \partial_q \xi^n \frac{\partial}{\partial v_{AI}}.
\end{multline}
These two expression describe the infinitesimal action of a diffeomorphism that is induced by a vector field $\xi$ on the first and second jet bundle over the field bundle respectively. 
As the involved maps $j^1(\mathcal{f})$ and $j^2(\mathcal{f})$ or in fact any prolongation to any higher jet bundle $j^q(\mathcal{f})$ define Lie algebra morphisms and hence one in particular finds that they preserve the commutator:
\begin{align}
j^q (\mathcal{f})\left ( \left [\xi, \tilde{\xi} \right ]_{\Gamma(TM)}\right) = \left [ j^q(\mathcal{f})(\xi), j^q(\mathcal{f})(\tilde{\xi}) \right ]_{\Gamma(TJ^qF)}
\end{align}
Using the prolongation of vector fields to the jet bundle we can now state one of the main results regarding the implications of the diffeomorphism equivariance of bundle maps on the jet bundle. Namely we present a first order PDE system that the Lagrangian of any diffeomorphism invariant field theory necessarily has to solve. 
\begin{theorem}[equivariance equations]
Let $(F,\pi_f,M)$ be the field bundle and $J^2F$ denote the second order jet bundle over $F$ as before. Furthermore let $(E, \pi_E, M)$ be a second bundle over $M$ with adapted coordinates $(x^m, u_{\tilde{A}})$ that carries an action of $\mathrm{Diff}(M)$ which is infinitesimally described by a Lie alegbra morphism $\mathcal{e}: \Gamma(TM) \rightarrow \Gamma(TE)$ with coordinate expression $\mathcal{e}(\xi) = \xi^m \frac{\partial}{\partial x^m} + K_{\tilde{A}n}^{\tilde{B}m} u_{\tilde{B}} \partial_m \xi^n \frac{\partial}{\partial u_{\tilde{A}}}$. Assume that we are given a bundle morphism $f : J^2F \rightarrow E$ covering $id_M$ that is equivariant w.r.t. the lifted action of $\mathrm{Diff}(M)$ on $J^2F$ and the action $\mathcal{e}$ on $E$ then in coordinates $f^{\tilde{A}}$ satisfies the following set of linear first order partial differential equations:
\begin{align}
\begin{aligned}
    0 &= f^{\tilde{C}:m} \\
    0 &= f^{\tilde{C}:A} C_{An}^{Bm} v_B + f^{\tilde{C}:Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + f^{\tilde{C}:AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ} - f^{\tilde{A}}K_{\tilde{A}n}^{\tilde{C}m}\\
    0 &= f^{\tilde{C}:A(p\vert}C_{An}^{B \vert m)} v_B + f^{\tilde{C}: AI} \bigl[ C_{An}^{B(m\vert} 2 J_I^{\vert p) q} - \delta^B_A J_I ^{pm} \delta_n^q \bigr] v_{Bq} \\
    0 &= f^{\tilde{C}:AI} C_{An}^{B(m\vert} v_B J_I^{\vert p q )}.
\end{aligned}
\end{align}
\end{theorem}
Here we introduced the notation $h^{:A} := \partial^A h$. The four equations describe a system of first order partial differential equations (PDEs) for the coordinate representation of $f$. 
\begin{proof}
The statement follows immediately from the definition of equivariance. Evaluating this definition infinitesimaly, i.e. on the level of Lie algebra actions for some vector field $\xi$ we obtain an equality with left hand side being given by the application of the prolonged vector field $\xi_{J^2F}$ and right hand side $f^{\tilde{A}}K_{\tilde{A}n}^{\tilde{C}m} \partial_m \xi^n$. As this equation has to hold for aritrary vector fields $\xi$ can in particular chose vector fields that allow us to specify the different derivatives of $\xi$ arbitrarily. Hence we can equate the coefficients of different derivatives of $\xi$ to zero independently which then yields the desired PDE system and therefore proves the statement.  
\end{proof}
The PDE system deduced from the equivariance of $f$ can obviously be projected to a PDE system encoding equivariance for functions on $J^1F$. This is simply done by discharging all terms that contain derivatives w.r.t. second order jet coordinates $v_{AI}$.  
Note that from the last theorem we can conclude in particular that the Lagrangian of diffeomorphisms invariant field theories, in local coordinates given by $\mathcal{L} = L \mathrm{d}^4x$ on $J^2F$ satisfies the following linear first order PDE system:
\begin{align}\label{DiffeoEqn}
\begin{aligned}
    0 &= L^{:m} \\
    0 &= L^{:A} C_{An}^{Bm} v_B + L^{:Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + L^{:AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ} + L \delta^m_n \\
    0 &= L^{:A(p\vert}C_{An}^{B \vert m)} v_B + L^{: AI} \bigl[ C_{An}^{B(m\vert} 2 J_I^{\vert p) q} - \delta^B_A J_I ^{pm} \delta_n^q \bigr] v_{Bq} \\
    0 &= L^{:AI} C_{An}^{B(m\vert} v_B J_I^{\vert p q )}.
    \end{aligned}
\end{align}
Note that the only ingredient that explicitly depends on the particular nature of the field is the constant tensor $C^{Bm}_{An}$.
The remaining objects in (\ref{DiffeoEqn}) take exactly this form no matter what concrete field theory we wish to consider.

Considering field theories on $J^1F$ this was already presented in \cite{Gotay1992StressEnergyMomentumTA}, where the authors used a similar PDE system for the Lagrangian to construct a universal energy momentum tensor that is conserved for any diffeomorphism invariant field theory.

In the context of constructive gravity it is worth to mention that as we required diffeomorphism invariance of the gravitational theory any Lagrangian that might describe it neccessarly needs to solve the PDE system (\ref{DiffeoEqn}). To put it differently by finding the general solution to the Lagrangian equivariance PDE for a given gravitational field bundle we would obtain the largest possible set of candidates for the Lagrangian of this theory of gravity. Without further requirements of the gravitatinal theory finding Lagrangians that describe the diffeomorphism invariant gravitational dynamics of a given gravitational field therefore boils down to solving the above PDE system (\ref{DiffeoEqn}). 

In particular from the diffeomorphism equivariance equations we easily see that in order to simplify the task of solving the System we might proceed as follows:
We first solve the corresponding \textit{\textbf{invariance equation}}, the system (\ref{DiffeoEqn}) with the second equation being replaced by the corresponding homogeneous equation
\begin{align}
    0 &= L_{scal}^{:A} C_{An}^{Bm} v_B + L_{scal}^{:Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + L_{scal}^{:AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ}.
\end{align}
A solution to this system describes a map $L_{scal}: J^2F \rightarrow \mathbb{R}$ that is invariant under diffeomorphisms, i.e. for all $\phi \in \mathrm{Diff}(M)$ it holds that:
\begin{align}
    L_{scal} \circ j^2(\phi) = L_{scal}.
\end{align}
Once we obtain the general solution to these difeomorphism invariance equations $L_{scal}$ multiplying by any particular solution of (\ref{DiffeoEqn}) yields a solution of the diffeomorphism equivariance equations. We might for instance take this particular solution to only depend on the coordinates of $F$ and not on the derivative coordinates. Such solutions can normaly be found more easily as then all terms in (\ref{DiffeoEqn}) that contain derivatives w.r.t. the derivative coordinates on $J^2F$ drop out. Conversely given any two solution to the equivariance equations taking their quotient yields a solution to the invariance equations. These arguments show that there really is a one to one correspondence between solutions of the equivariance and invariance equations. If solving the invariance equation really gives advantages over solving the PDE (\ref{DiffeoEqn}) can only be decided with concrete examples at hand. Details regarding these considerations can be found in theorem (\ref{GeneralSol}) and the corresponding proof. \\

Note that the first equation in the PDE system (\ref{DiffeoEqn}) simply requires the Lagrangian to be independent of the coordinates of the spacetime manifold. This is of course a requirement posed on the Lagrangian as a bundle map on $J^2F$ not on the corresponding composition with a given section of $J^2F$ as all such compositions are necessary $x^m$-dependent. The first equation can hence be trivially solved and we might use its implications in the following. For instance we can now write $L(v_A.v_{Ai},v_{AI})$ for the coordinate expression of the Lagrangian, discharging the explicit $x^m$-dependency.\\

Before proceeding with the next section we quickly deduce the implications for the EOM that are generated by diffeomorphism invariant Lagrangians. The EOM of a given Lagrangian $\mathcal{L}$ can be obtained by taking its variational derivative, in coordinates $E^A := \frac{\delta \mathcal{L}}{\delta v_A}$, composing it with the prolongation of a section of the field bundle and then equating to zero. In the following we will also refer to $E^A$ as equations of motion. If the Lagrangian is given as a funcntion on the second jet bundle then the EOM are generally functions on the fourth jet bundle $J^4F$. As mentioned before we are however particularly interested in the case where the Lagrangian is degenerated in the sense that also the EOM are functions on $J^2F$ and hence a meaningful Hamiltonian formulatione exists.
\begin{theorem}[diffeomorphism equivariant EOM]
Given a Lagrangian on $J^2F$ that describes a diffeomorphism invariant field theory with second order EOM, in the usual coordinates: $E^A := \frac{\delta \mathcal{L}}{\delta v_A}$, then the equations of motion satisfy the following linear first order PDE system:
\begin{align}\label{EOM}
    \begin{aligned}
    0 &= E^{C:m} \\
    0 &= E^{C:A} C_{An}^{Bm} v_B + E^{C:Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + E^{C:AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ}\\
    &+ E^C \delta^m_n + E^A C_{An}^{Cm}  \\
    0 &= E^{C:A(p\vert}C_{An}^{B \vert m)} v_B + E^{C: AI} \bigl[ C_{An}^{B(m\vert} 2 J_I^{\vert p) q} - \delta^B_A J_I ^{pm} \delta_n^q \bigr] v_{Bq} \\
    0 &= E^{C:AI} C_{An}^{B(m\vert} v_B J_I^{\vert p q )}.
    \end{aligned}
\end{align}
\end{theorem}
\begin{proof}
Again we start with Lie algebra version of the equivariance condition for $\mathcal{L}$ and an arbitrary vector field $\xi$. This time the statement can be proven by first computing the variational derivative, i.e. acting with $\frac{\delta}{\delta v_A}$ on the whole equation and then equating the components in front of different derivatives of $\xi$ individually to zero. 
\end{proof}
Similar equation where obtained in the context variational calculus and gauge symmetries of a given Lagrangian in \cite{article}. In the context of GR, although not in the form of a PDE system, Lovelock used akin implications deduced from the required diffeomorphism invariance to prove that the Einstein-Hilbert-Lagrangian is unique in the sense that it is the only Lagrangian producing second derivative order EOM that solves these implications \cite{Lovelock1969}, \cite{doi:10.1063/1.1666069} and \cite{doi:10.1063/1.1665613}. 

If we loosen our requirements of the gravitational theory to the point where we are only interested in the EOM and not the Lagrangian we could also work with the PDE system (\ref{EOM}). By solving this system we obviously directly obtain EOM. Such an approach is described in \cite{TobiR}.\\

Summing up we have derived linear first order PDE systems for the Lagrangian or the equations of motion respectively that encode the required diffeomorphism invariance of a given field theory. Any theory that is invariant under spacetime diffeomorphisms has to solve the two PDEs. Conversely we might construct difeomorphism invariant theories as solutions to either the system (\ref{DiffeoEqn}) on the level of the Lagrangian or the system (\ref{EOM}) on the level of the EOM. This is a huge improvement as now the notoriously difficult requirement of diffeomorphism invariance that we have posed on the gravitational theory that we wish to construct is translated into the rather simply quest of solving a linear first order PDE system. 



\section{Canonical Kinematics and Hypersurface Deformations}
In the following two sections we are going to investigate implication of the required diffeomorphism invariance for the \textit{\textbf{canonical formulation}} of a given field theory. For that purpose we mainly concern field theories that are described by a \textit{\textbf{first order}} Lagrangian, i.e. a function on $J^1F$. Working on $J^2F$ as before we would necessary have to consider the case where the Lagrangian is degenerate such that the EOM are nevertheless of second order and the Hamiltonian formulation is free of Ostrogratsky instabilities. The degeneracy of the Lagrangian would then however impede the canonical formulation of the theory\footnote{We remark that although posing further technical difficulties we expect a generalisation of the following derivations to the case of degenerate Lagrangians on $J^2F$ to be straight forward.}.

The main idea that is used when transferring from a given Lagrangian field theory to the associated Hamiltonian formulation is that of decomposing the spacetime manifold $M$ into a disjoint set of 3-dimensional manifolds that are smoothly parametrized by a time parameter. As a consequence one can then accordingly decompose any structure that is prescribed on $M$. In order to achieve this we follow \cite{2004math.ph..11032G} and introduce the notion of a \textit{\textbf{slicing}}, sometimes also called a \textit{\textbf{3+1-split}}, of the spacetime manifold $M$.
\begin{definition}[slicing]
A slicing of the spacetime manifold $M$ is a diffeomorphism $\phi : \Sigma \times \mathbb{R} \rightarrow M$.
\end{definition}
By this definition for each $\lambda \in \mathbb{R}$ we obtain an embedding of the 3-dimensional manifold $\Sigma$:
\begin{align}
\begin{aligned}
\phi_{\lambda}: &\Sigma \longrightarrow M \\
&s \longmapsto \phi_{\lambda}(s) := \phi(s,\lambda).
\end{aligned}
\end{align}
We define the images of this one-parameter family of embeddings as $\Sigma_{\lambda} := \phi_{\lambda}(\Sigma)$. Hence we can think of the 3-dimensional manifolds $\Sigma_{\lambda}$ as modelling space at given parameter time $\lambda$.

Note that it is a priori not clear whether or not such a slicing exists for a given spacetime $M$. For the situation in constructive gravity there are however certain results that guaranty its existence. If we require the given matter theory on $M$ to be predictive in the sense that the corresponding EOM are of hyperbolic type\footnote{We will provide a rigorous version of this statement in the second chapter. }, then one can show that there necessarily exists a slicing of spacetime with the embedded spaces $\Sigma_{\lambda}$ being initial data hypersurfaces for the hyperbolic EOM. In the context of GR see for instance \cite{2003CMaPh.243..461B} and also \cite{1996gere.conf...19G} for a more general context. \\
%Refererrence !!!!!!!
%
%
%
%
%
%clarify matter theory statement

In addition to the smoothly parameterized family of embeddings $\phi_{\lambda}$ a slicing on $M$ immediately induces a \textit{\textbf{time vector field}} and a \textit{\textbf{time differential}} one form on $M$. The one form can be obtained as differential $\mathrm{d}t$ of the function $t:=\pi_{\mathbb{R}} \circ \phi^{-1}$, where $\pi_{\mathbb{R}}$ denotes the canonical project of $\Sigma \times \mathbb{R}$ onto the second factor: 
\begin{center}
\begin{tikzpicture}
\node (S) at (0,0) {$\Sigma \times \mathbb{R}$};
\node (M) at (4,0) {$M$};
\node (R) at (0,-4) {$\mathbb{R}$};
\draw [<-] (S) -- node[pos=0.5, above] {$\phi^{-1}$} (M);
\draw [->] (S) -- node[pos=0.5, left] {$\pi_{\mathbb{R}}$} (R);
\draw [->] (M) -- node[pos=0.5, right] {$t$} (R);
\end{tikzpicture} .
\end{center}
The time vector field can be constructed by first defining for all $s \in \Sigma$ the following map 
\begin{align}
\begin{aligned}
    \phi_s : &\mathbb{R} \longrightarrow M \\
    &\lambda \longmapsto \phi(s,\lambda).
\end{aligned}
\end{align}
These maps define curves on $M$ and hence each provide us with tangential vectors along the curves. Using this we can define a vector field on $M$ by defining the value at $p \in M$ to be given by
\begin{align}
\frac{\partial}{\partial t}\bigg \vert_p = \phi^{\prime}_{\pi_{\Sigma}\circ \phi^{-1}(p)} \left (\pi_{\mathbb{R}}\circ \phi^{-1}(p)\right ).
\end{align}
One readily finds that $\frac{\partial}{\partial t}$ and $\mathrm{d}t$ are dual objects, i.e. $\mathrm{d}t(\frac{\partial}{\partial t}) = 1$ and hence the notation. In the following we call $t$ time function, $\mathrm{d}t$ time differential and $\frac{\partial}{\partial t}$ time vector field.

Given the fact that $\mathrm{d}t$ and $\frac{\partial}{\partial t}$ were obtained from the given slicing this immediately raises the question how the time differential and vector field and in fact all further objects that will subsequently be defined in terms of these change under a change of slicing. If $\phi : \Sigma \times \mathbb{R} \rightarrow M $ and $\psi : \Sigma \times \mathbb{R} \rightarrow M$ are two slicings then we obtain a diffeomorphism on $M$ by taking $\psi \circ \phi^{-1}$. Conversely given a slicing $\phi$ any diffeomorphism $\rho \in \mathrm{Diff}(M)$ defines a second slicing by $\rho \circ \phi $. Hence changing the given slicing is in one to one correspondence with diffeomorphisms on $M$. \\

In addition to the time differential and time vector field on $M$ a slicing also induces a \textbf{\textit{direct sum split}} of the tangent bundle over $M$. Given a slicing $\phi : \Sigma \times \mathbb{R} \rightarrow M$ we can compute its push forward to obtain a vector bundle isomorphism
\begin{align}
\phi_{\ast}: T(\Sigma \times \mathbb{R}) \longrightarrow TM.
\end{align}
One can now show that $T(\Sigma \times \mathbb{R}) \cong \pi_{\Sigma}^{\ast}T\Sigma \oplus \pi_{\mathbb{R}}^{\ast} T\mathbb{R}$. And hence in total we have: $TM \cong \pi_{\Sigma}^{\ast}T\Sigma \oplus \pi_{\mathbb{R}}^{\ast} T\mathbb{R}$.
Here $F\oplus E$ denotes the direct sum or whitney sum (for details see the first chapter of \cite{nla.cat-vn705150}) of the vector bundles over the common base space $M$, i.e. the vector bundle with fibre at $p \in M$ given by the direct sum of vectorspaces $\pi_F^{-1}(p) \oplus \pi_E^{-1}(p)$. $\pi_{\Sigma}^{\ast}T\Sigma$ and $\pi_{\mathbb{R}}^{\ast}T\mathbb{R}$ denotes the pullback\footnote{Given a bundle $(F,\pi_F,M)$ and a map $h: N \rightarrow M$, the pullback of the bundle $F$ along $h$ is the bundle over $N$ with total space $h^{\ast}F := \{ (n,f) \in N \times F \ \vert \  h(n) = \pi_F(f)\}$, and bundle projection $\pi_{h^{\ast}F}(n,f) = n$ (see \cite{doi:10.1142/3867}). Note that this construction makes the map $\tilde{h}: f^{\ast}F \rightarrow F$ defined by $\tilde{h}(n,f) = f$ a bundle morphism covering $h$, i.e the following diagram commutes. 
\begin{center}
\begin{tikzpicture}
\node (M) at (0,0) {$N$};
\node (N) at (4,0) {$M$};
\node (M2) at (0,2) {$f^{\ast}F$};
\node (N2) at (4,2) {$F$};
\draw [->] (M) -- node[pos=0.5, below] {$h$} (N);
\draw [->] (M2) -- node[pos=0.5, above] {$\tilde{h}$} (N2);
\draw [<-] (M) -- node[pos=0.5, left] {$\pi_{h^{\ast}F}$} (M2);
\draw [<-] (N) -- node[pos=0.5, right] {$\pi_F$} (N2);
\end{tikzpicture}
\end{center}} of vector bundles $T\Sigma$ and $T\mathbb{R}$ over $\Sigma$ and $\mathbb{R}$ to the common basespace $\Sigma \times \mathbb{R}$ by making use of the two canonical projections. In the following we write
\begin{align}
    TM = \mathcal{T}\Sigma \oplus \mathcal{V}\Sigma
\end{align} 
for this direct sum decomposition of the tangent bundle and call the summands \textit{\textbf{tangential}} and \textit{\textbf{vertical subbundle}} respectively.  

In particular vector fields on $M$ can uniquely be \textit{\textbf{decomposed}} w.r.t. this split. Given a vector field $\xi \in \Gamma(TM)$ we write $\xi = \xi_{\parallel} + \xi_{\perp} $ for this decomposition, with $\xi_{\parallel} \in \Gamma(\mathcal{T}\Sigma)$ and $\xi_{\perp} \in \Gamma(\mathcal{V}\Sigma)$ called tangential and vertical part of $\xi$.
%is the following really true, probably only up to canonical isomorphisms
%Conversely, from the construction of the split we see that any tangential vector field $\xi_{\parallel} \in \Gamma(\mathcal{T}\Sigma)$ can be obtained as pushforward of a vector field in $\Gamma(\pi_{\Sigma}^{\ast}T\Sigma)$ along $\phi$. Any vertical vector field $\xi_{\perp} \in \Gamma(\mathcal{V}\Sigma)$ is the pushforward of a vector field in $\Gamma(\pi_{\mathbb{R}}^{\ast}T\mathbb{R})$.
The vectors in $\mathcal{T}\Sigma$ satisfy: $\mathrm{d}t(\xi_{\parallel})=0$, the vectors in $\mathcal{V}\Sigma$ are parallel to $\frac{\partial}{\partial t}$  . Note that the direct sum split of $TM$ naturally induces a direct some split of the Lie algebra actions on $F$ and $J^1F$ (and also $J^2F$ if we would work on the second order jet bundle) as the previously constructed Lie algebra morphisms (\ref{LieF}) and (\ref{LieJ1}) are in particular linear maps on the fibres of $TM$ and hence respect the direct sum structure.\\

We call a coordinate system $(x^m)$ on $M$ \textit{\textbf{adapted}} to the slicing $\phi$ if there exists an adapted coordinate system $(y^{\alpha},z)$ on $\Sigma \times \mathbb{R}$ s.t. $x^0 = z \circ \phi^{-1}$ and $x^{\alpha} = y^{\alpha} \circ \phi^{-1}$. Therefore in adapted coordinates the embedded hypersurfaces are characterized by $x^0 = const$ and the chart induced vector fields satisfy $\frac{\partial}{\partial t} = \lambda \cdot \frac{\partial}{\partial x^0}$ for some $\lambda$ that only depends on $x^0$ and $\mathrm{d}t\left(\frac{\partial}{\partial x^{\alpha}}\right) = 0$.
Note that in adapted coordinates we thus have in particular that the induced spatial coordinate fields $\frac{\partial}{\partial x^{\alpha}}$ are sections of the tangential subbundle $\mathcal{T}\Sigma$ whereas $\frac{\partial}{\partial x^0}$ defines a section of the vertical subbundle $\mathcal{V}\Sigma$. Hence in such coordinates we can express the decomposition of vector fields as 
\begin{align}
    \xi = \xi_{\perp} + \xi_{\parallel} = N  \frac{\partial }{\partial t} + N^{\alpha} \frac{\partial}{\partial x^{\alpha}} = N \cdot \lambda \frac{\partial}{\partial x^0} + N^{\alpha} \frac{\partial}{\partial x^{\alpha}},
\end{align}

%denote the following differently ???
In the following we work mainly over one embedded hypersurface. Given a slicing $\phi$ take some $t_0 \in \mathbb{R}$ and consider the corresponding embedded hypersurface $\Sigma_{t_0}$ with embedding $\phi_{t_0}$. To this end we restrict all functions, fields, etc. on $M$ to $\Sigma_{t_0}$. These restrictions are in the following called \textbf{\textit{hypersurface quantities}}. In order to keep the notation concise we might not explicitly denote the restriction every time. 
%Note that on this hypersurface we have $t \vert _{\Sigma_{t_0}} = t_0$.

Furthermore we find that the restriction of the direct sum split of the tangent bundle to $\Sigma_{t_0}$\footnote{It is important to observe the difference between the restricted tangent bundle $TM\vert_{\Sigma_{t_0}}$ and the tangent bundle of the embedded hypersurface $T\Sigma_{t_0}$. While the former contains a 4-dimensional vector space at each hypersurface point the latter only has 3-dimensional fibres. The difference lies precisely in the vertical bundle over the hypersuface $\mathcal{V}\Sigma_{t_0}$.} is given by $TM \vert _{\Sigma_{t_0}} = T\Sigma _{t_0} \oplus \mathcal{V}\Sigma _{t_0}$. Note that the restriction of the tangential bundle $\mathcal{T}\Sigma_{t_0}$ is precisely given by the tangent bundle over the embedded hypersurface $T\Sigma_{t_0}$.

We are then in particular interested in the question how hypersurface quantities change under a change of slicing $\phi$ and hence an induced change  of the embedding $\phi_{t_0}$. By previous arguments changing the slicing corresponds to the action of a diffeomorphism. Infinitesimaly this is described by the Lie algebra action of a vector field $\xi$. Note that whether we can take the standard action of diffeomorphism and vector fields on $M$ or have to take the previously computed lifted actions (\ref{LieF}) on $F$, (\ref{LieJ1}) on $J^1F$, etc. depends on the kind of the objects that we act on.  Given $\xi$ we decompose it into its tangential and vertical part $\xi_{\parallel}$ and $\xi_{\perp}$ to obtain derivative operators that describe the infinitesimal tangential and vertical change of hypersurface quantities.  In adapted coordinates we obtain such tangential and vertical vector fields by specifying component functions
 $N^a : M \rightarrow \mathbb{R}$ and $N^a : M \rightarrow \mathbb{R}$. We define: 
\begin{align}
    \begin{aligned}
    \mathcal{D}(N^{\alpha}) &:= N^{\alpha} \frac{\partial}{\partial x^{\alpha}} \\
    \mathcal{H}(N) &:= N \frac{\partial}{\partial t} = N \cdot \lambda \frac{\partial}{\partial x^0}.
    \end{aligned}
\end{align} 
Note that we can interpret $\mathcal{D}(N^\alpha)$ and $\mathcal{H}(N)$ as \textit{\textbf{tangential}} and \textit{\textbf{vertical deformation operators}} that when acting on functions compute their infinitesimal change under a tangential respectively vertical deformation by an amount specified by the component functions $N^{\alpha}$ and $N$.
We are now going to compute the commutator algebra that such tangential and vertical vector fields satisfy. Doing this we obtain:
\begin{align}
    \begin{aligned}
    \left [ \mathcal{D}(N^{\alpha}), \mathcal{D}(M^{\alpha}) \right] &= \mathcal{D}(N^\alpha \partial_{\alpha}M^{\beta} - M^{\alpha} \partial_{\alpha} N^{\beta}) \\
    \left[ \mathcal{D}(N^{\alpha}), \mathcal{H}(M) \right] &= \mathcal{H}(N^{\alpha} \partial_{\alpha} M) - \mathcal{D}(M \lambda \partial_0 N^{\alpha})\\
    \left[ \mathcal{H}(N), \mathcal{H}(M) \right ] &= \mathcal{H}(N \lambda \partial_0 M - M \lambda \partial_0N)
    \end{aligned}
\end{align}
We now want to describe the change in tangential and vertical direction that is computed by the operators $\mathcal{D}(N^{\alpha})$ and $\mathcal{H}(N)$ intrinsically, in terms of quantities that only depend on the coordinates of $\Sigma_{t_0}$.
Hence we restrict the involved component functions $N^{\alpha}$ and $N$ and the deformation operators defined in terms of them such that they locally only depend on the coordinates $x^{\mu}$ and not on $x^0$.
\begin{comment}
The corresponding tangential and vertical vector fields given by $\mathcal{D}(N^{\alpha})$ and $\mathcal{H}(N)$ can then be interpreted as tangential and vertical deformation operators defined on $\Sigma_{t_0}$. Acting on hypersurface quantities they compute the infinitesimal change in tangential and vertical direction. Note that the action of vector fields on hypersurface quantities describes their infinitesimal change under diffeomorphisms. Hence by the arguments provided before the action of vector fields on hypersurface quantities describes their infinitesimal change under a change of slicing. The deformation operators $\mathcal{D}(N^{\alpha})$ and $\mathcal{H}(N)$ defined on $\Sigma_{t_0}$ encode contributions to this change in tangential and vertical direction.
\end{comment}
As a consequence contributions from $\partial_0$ derivatives of $N^{\alpha}$ and $N$ now drop out.
We thus obtain the following commutator algebra: 
\begin{align}\label{Alg}
    \begin{aligned}
    \left[ \mathcal{D}(N^{\alpha}), \mathcal{D}(M^{\alpha}) \right] &= \mathcal{D}(\mathcal{L}_{\vec{N}}M^{\beta}) \\
    \left[ \mathcal{D}(N^{\alpha}), \mathcal{H}(M) \right] &= \mathcal{H}(\mathcal{L}_{\vec{N}}M)\\
    \left[ \mathcal{H}(N), \mathcal{H}(M) \right] &= 0,
    \end{aligned}
\end{align}
where we denoted $\mathcal{L}_{\Vec{N}}M^{\beta} = N^\alpha \partial_{\alpha}M^{\beta} - M^{\alpha} \partial_{\alpha} N^{\beta}$ and $\mathcal{L}_{\Vec{N}}M = N^{\alpha} \partial_{\alpha} M$ as Lie derivative along the tangential hypersurface vector field $\vec{N} = N^{\alpha} \frac{\partial}{\partial x^{\alpha}}$ to display the commutator algebra in concise form.
We observe that the commutator of two tangential vector fields again yields a tangential vector field. The tangential vector fields hence constitute a Lie sub algebra. 
The commutator of a tangential and a vertical vector field yields a vertical vector field.
Finally any two vertical vector fields commute. This commutator algebra encodes the change of hypersurface quantities on $\Sigma_{t_0}$ under a change of the slicing of spacetime. It is therefore often called the \textit{\textbf{hypersurface deformation algebra}}.\\

\begin{remark}
Note that in traditional terms the hypersurface deformation algebra is mostly obtained by computing commutators of vectorfields on the infinite dimensional manifold of embeddings of a given $\Sigma$ in $M$ (see \cite{HOJMAN197688} and \cite{doi:10.1063/1.522976}). We deliberately decided for an alternative approach here, as already in the formulation of Lagrangian field theory we saw how working with rigorously defined objects constructed over the spacetime manifold instead of a heuristical treatment involving infinite dimensional function spaces led to a much clearer description that essentially involved the same information. In other words if working on infinte dimensional spaces can be avoided we simply avoid it. We will encunter this guideline again later on when we construct poisson brackets to encorporate dynamics in terms of the Hamiltonian formulation of field theory.
\end{remark}

One often encounters the situation where the direct sum split of the tangent bundle is not obtained from the time vector field $\frac{\partial}{\partial t}$ that is induced by the given slicing but by means of some physically motivated notion that at each point of the embedded hypersurface $\Sigma_{t_0}$ distinguishes a vertical vector. For instance this can be achieved by making use of a given \textit{\textbf{observer definition}} for the field theory at hand. For the embedded hypersurface $\Sigma_{t_0}$ one constructs the direct sum split by defining the vertical subbundle as span of the "time" vector field seen as time direction by observers that travel through $p \in \Sigma_{t_0}$ and have a spatial frame lying in the tangent plane $T_p\Sigma_{t_0}$.

Taking the canonical formulation of General Relativity\footnote{For further information regarding some of the provided examples from General Relativity we kindly refer to the standard text books as for instance \cite{Misner1973}. For observer definitions in more general spacetime geometries see \cite{2011PhRvD..83d4047R}.} as an example, given $\Sigma_{t_0}$ and frame fields $e_{\alpha}$ that locally at each point $p \in \Sigma_{t_0}$ in consideration form a basis of $T_p\Sigma_{t_0}$ and satisfy $g(e_{\alpha},e_{\beta}) = \eta_{\alpha \beta}$, to complete the three frame fields to a viable observer frame\footnote{Recall that an observer in GR is defined as a curve $\gamma : \mathbb{R} \rightarrow M$ together with curves $e_{\alpha} : \mathbb{R} \rightarrow TM$, s.t. $ e_a(\lambda) :=(\dot{\gamma}(\lambda),e_{\alpha}(\lambda))$ forms a basis of $T_{\gamma(\lambda)}M$ and $g(e_a(\lambda),e_b(\lambda)) = \eta_{ab}$, , for each $\lambda$.} one needs to choose $e_0$ s.t. 
\begin{align}
    g(e_0,e_0) = 1 \ \text{and} \ g(e_0,e_{\alpha}) = 0.
\end{align}
These two conditions are then often referred to as frame conditions. One can then split the tangent space $T_pM$ at each $p \in \Sigma_{t_0}$ into the vertical part spanned by $e_0(p)$,  $\mathcal{V}_p\Sigma_{t_0} = \langle  e_0(p) \rangle$ and tangential part $T_p\Sigma_{t_0}$ . In particular one sees that now by the two frame conditions, the vertical time direction and thereby the vertical sub bundle and the split itself depend on the metric. Along the same lines given any other field theory with a notion for an observer one can then use the observer definition to construct the vertical sub bundle of $TM$ in similar fashion. \\

One way of defining observers from any matter theory that uses the given gravitational field as geometric background can be found in \cite{2018PhRvD..97h4036D} and \cite{2011PhRvD..83d4047R} and also \cite{Rivera}. There the authors construct an observer notion that suits a general gravitational field theory from the \textbf{\textit{principal polynomial}}\footnote{Loosly speaking the principal polynomial encodes the causal structure of the matter EOM. In the following chapter we will encounter this object in more detail.} $\mathcal{P}_{mat}(v_A)$ of the corresponding matter EOM. Note that the principal polynomial in particular depends on the coordinates of the field bundle, i.e. is field dependent. The vertical vector field is then obtained in similar fashion compared with the standard GR case. The principal polynomial allows one to define a map that can be used to normalize the conormal and map the result to a corresponding normal vector.

All these constructions have one thing in common: the vertical sub bundle of the direct sum split of $TM$ is now field dependent. As consequence, if one wants to deduce the hypersurface deformation algebra corresponding to a field dependent direct sum split it no longer suffices to consider the commutators of tangential and vertical vector fields over $\Sigma_{t_0}$. To account for the additional field dependence one now has to work with the induced vector fields on the field bundle\footnote{In the following we assume that the direct sum split only depends on the fields and not on their derivatives. Otherwise we would need to work on the jet bundle $J^qF$ of appropriate order.} $F$. To obtain the corresponding hypersurface deformation algebra one then computes commutators between the lifts of tangential and vertical vector fields according to the Lie algebra morphism (\ref{LieF}).

We again choose coordinates that are adapted to the given slicing. In contrast to the situation before now the vertical vector field $e_0$ that we use for the direct sum split additionally depends on the fibre coordinates of the field bundle $v_A$. The decomposition of vector fields then reads:
\begin{align}
    \xi = \xi_{\perp} + \xi_{\parallel} = N e_0 + N^{\alpha} \frac{\partial }{\partial x^{\alpha}} = N  e_0^a \frac{\partial}{\partial x^a} + N^{\alpha} \frac{\partial }{\partial x^{\alpha}}.
\end{align}
Here we expressed the field dependent vector field $e_0$ in terms of the holonomic basis fields and denoted the component functions by $e_0^a$.
The corresponding split of the induced vector field on the field bundle $\xi_F$ can now be computed to be given by:
\begin{align}
    \begin{aligned}
    \xi_{F,\parallel} &= \mathcal{f} \left ( \xi_{\perp} \right ) = N^{\alpha} \frac{\partial}{\partial x^{\alpha}} + C_{A \nu}^{B m} v_B \partial_{m} N^{\nu} \frac{\partial}{\partial v_A} \\
    \xi_{F, \perp} &= \mathcal{f} \left ( \xi_{\parallel} \right )=  N e_0^a \frac{\partial}{\partial x^a} + C_{A n}^{B m} v_B \partial_{m} (N \cdot e_0^n) \frac{\partial}{\partial v_A},
    \end{aligned}
\end{align}
where $\mathcal{f}$ is the Lie algebra morphism defined in (\ref{LieF}).
Again we define tangential and vertical deformation operators for arbitrary $N^a : M \rightarrow \mathbb{R}$ and $N : M \rightarrow \mathbb{R}$:
\begin{align}
    \begin{aligned}
    \mathcal{D}_F(N^{\alpha})&:= N^{\alpha} \frac{\partial}{\partial x^{\alpha}} + C_{A \beta}^{B m} v_B \partial_{m} N^{\beta} \frac{\partial}{\partial v_A} \\
    \mathcal{H}_F(N) &:=  N e_0^a \frac{\partial}{\partial x^a} + C_{A n}^{B m} v_B \partial_{m} (N \cdot e_0^n) \frac{\partial}{\partial v_A}
    \end{aligned}
\end{align}
and restrict to the case where the component functions $N^{\alpha}$, $N$ and $e^n$ do not depend on the coordinates $x^0$.

When computing commutators of $\mathcal{D}_F$ and $\mathcal{H}_F$ we can use the fact that the map $\mathcal{f}: \Gamma(TM) \rightarrow \Gamma(TF)$ that was defined in (\ref{LieF}) defines a Lie algebra morphism and hence preserves the commutator.
Comparing to the previous case (\ref{Alg}) we get however additional contributions whenever $\frac{\partial}{\partial v_A}$ acts on $e_0^n$. These contributions precisely account for the change of $e_0$ that is induced by a change of the fields in $F$ under the action of vectorfields. The only commutator that does not involve such contributions is the one between two parallel vector fields. Therefore for this commutator we simply get:
\begin{align}\label{FDD}
    \left [ \mathcal{D}_F(N^{\alpha}), \mathcal{D}_F(M^{\alpha})\right ] &= \mathcal{D}_F(\mathcal{L}_{\vec{N}}M^{\beta}).
\end{align}
The two remaining commutators in general contain additional contributions. As these additional contributions obviously depend on the explicit form of the field dependency of $e_0$ we illustrate their appearance for the case of canonical GR. With slight modifications in the computations this will then of course also cover the case where for a given field theory the observer definition and thereby the vertical vector field $e_0$ is construction in analogy to the standard GR case, but with an effective metric that is constructed from the field instead of the fundamental metric field in GR, as it is for instance the case for the observer definition in \cite{2018PhRvD..97h4036D}, \cite{2011PhRvD..83d4047R} and \cite{Rivera}.\\

Following the usual way of obtaining the vertical vector field once the embedded hypersurface $\Sigma_{t_0}$ is specified one starts by taking a conormal vectorfield to $\Sigma_{t_0}$, i.e. a 1-form $\tilde{n} \in \Gamma(\Lambda^1M)$ that restricts to zero on each tangent plane of the hypersurface, $\tilde{n} \vert_{T\Sigma_{t_0}} = 0$. Then one uses the inverse metric to normalize the conormal 1-form, in other words one defines a unit conormal by $n := \frac{1}{\sqrt{ \vert g^{-1}(\tilde{n},\tilde{n}) \vert }} \cdot \tilde{n}$. The vertical vector field is then given by $e_0 := g^{-1}(n, - )$. 

Working in adapted coordinates $\mathrm{d}x^0$ defines such a conormal 1-form. Hence we get the the vertical vector field by $e_0 := \frac{1}{\sqrt{\vert g^{00} \vert }} g^{a0} \frac{\partial}{\partial x^a}$.
In particular as one can readily check with this vertical vector field we have $g(e_0,e_0) = 1$ and $g(e_0,\frac{\partial}{\partial x^{\alpha}}) = 0$. If we chose any additional spatial observer vector fields $e_{\alpha}$ in the tangential sub bundle of $\Sigma_{t_0}$, as the coordinate fields $\frac{\partial}{\partial x^{\alpha}}$ at each point of the embedded hypersurface constitute a basis of the tangential subspace, they are necessarily of the form $e_{\alpha} = e_{\alpha}^{\beta} \frac{\partial}{\partial x^{\beta}}$. Hence $e_0$ completes them to a valid GR observer frame.
The deformation operators can then be obtained as
\begin{align}
    \begin{aligned}
    \mathcal{D}_F(N^{\alpha}) &= N^{\alpha} \frac{\partial}{\partial x^{\alpha}} + 2 g^{\mu (p\vert} \delta^{\vert q)}_{\nu} \partial_{\mu} N^{\nu} \frac{\partial}{\partial g ^{pq}} \\
    \mathcal{H}_F(N) &= N \frac{1}{\sqrt{\vert g^{00} \vert }} g^{a0} \frac{\partial}{\partial x^a} +2 g^{\mu (p\vert} \delta^{\vert q)}_{n} \partial_{\mu} \left (N \frac{1}{\sqrt{\vert g^{00} \vert }} g^{n0} \right )  \frac{\partial}{\partial g ^{pq}},
    \end{aligned}
\end{align}
where we inserted the expression for $C^{Am}_{Bn} \equiv C^{abm}_{cdn} = +2g^{m(a\vert}\delta^{\vert b)}_n$ for the case of the field being given by the inverse metric and further simplified the occuring expressions by aking use of the fact that $N^{\alpha},M$ and $e_0^a$ only depend on the hypersurface coordinates $x^\alpha$.

For this example we do not work in the coordinates on the field bundle that are constructed by means of the intertwiners in order to put the focus on similarities to the standard formulation of canonical GR. We start with the commutator between a  tangential and a vertical vector field.
%
%rename tangential horizontal ?????
%
%
We get 
\begin{multline}\label{1stCom}
    \left[ \mathcal{D}_F(N^{\alpha}) , \mathcal{H}_F(M) \right] = \mathcal{H}_F(\mathcal{L}_{\vec{N}}M) - M \frac{1}{\sqrt{\vert g^{00} \vert }} g^{\mu0} \partial_{\mu} N^{\alpha} \frac{\partial}{\partial x^{\alpha}} \\
    +2M g^{\mu (p\vert} \delta^{\vert q)}_{\nu} \partial_{\mu} N^{\nu} \frac{\partial}{\partial g ^{pq}} \left(\frac{1}{\sqrt{\vert g^{00} \vert }} g^{a0} \right) \frac{\partial}{\partial x^a} + C^{pq} \frac{\partial}{\partial g^{pq}}.
\end{multline}
For the upcoming calculations we want to restrict attention to the two extra terms that are proportional to $\frac{\partial}{\partial x^a}$ and $\frac{\partial}{\partial x^{\alpha}}$ respectively as their treatment nicely illustrates the spirit of the calculations without them getting too involved. In the equation above all additional extra terms that appear, i.e. those that are proportional to $\frac{\partial}{\partial g^{pq}}$ are collectively displayed by $C^{pq}$. We start by computing the derivative $\frac{\partial}{\partial g^{pq}}$ in the second extra term to obtain:
\begin{align}\label{metDer}
    \frac{\partial}{\partial g ^{pq}} \left (\frac{1}{\sqrt{\vert g^{00} \vert }} g^{a0} \right) = -\frac{1}{2} \frac{1}{\vert g^{00} \vert^{\frac{3}{2}}} g^{a0} \delta^0_p \delta^0_q +  \frac{1}{\sqrt{\vert g^{00} \vert }} \delta^a_{(p \vert} \delta^0 _{\vert q)}. 
\end{align}
Up on contraction with $g^{\mu ( p \vert} \delta ^{\vert q)}_{\nu}$ the first term of (\ref{metDer}) vanishes completely due to the appearance of $\delta^0_{\nu}$, the second term yields only one contribution which exactly cancels the first extra term in (\ref{1stCom}). Along similar lines one then proceeds to show that the remaining extra contributions denoted by $C^{pq}$ vanish. In total we find that:
\begin{align}\label{FDH}
    \bigl[ \mathcal{D}_F(N^{\alpha}) , \mathcal{H}_F(M) \bigr] = \mathcal{H}_F(\mathcal{L}_{\vec{N}}M).
\end{align}
We observe that the commutator between a tangential and a vertical vector fields is the same as in the previous case (\ref{Alg}).
Note that this commutation relation is not a general feature of direct sum decomposition with field dependent vertical vector fields but depends on the precise observer definition. 

We proceed with the commutator of two vertical vector fields:
\begin{multline}
    \left[ \mathcal{H}_F(N), \mathcal{H}_F(M) \right] = \left( N\partial_{\mu} M - M \partial_{\mu}N  \right) \cdot \biggl[ \frac{1}{g^{00}} g^{\mu 0} g^{a0} \frac{\partial}{\partial x^a}  \\
    -2 g^{\mu (p\vert} \delta^{\vert q)}_{n} \frac{1}{\sqrt{\vert g^{00} \vert }} g^{n0}  \frac{\partial}{\partial g ^{pq}} \left( \frac{1}{\sqrt{\vert g^{00} \vert }} g^{a0} \right) \frac{\partial}{\partial x^a} \biggr] + \tilde{C}^{pq} \frac{\partial}{\partial g^{pq}}.
\end{multline}
Again we focus on the extra terms proportional to $\frac{\partial}{\partial x^a}$. Using the previously computed result for the derivatives w.r.t. the metric components one finds that the two terms of that kind collaps to 
\begin{align}
    \left[ \mathcal{H}_F(N), \mathcal{H}_F(M) \right] = \left( g^{\mu \alpha} - \frac{1}{g^{00}} g^{0\alpha} g^{0 \mu} \right) \left( N\partial_{\mu} M - M \partial_{\mu}N  \right) \frac{\partial}{\partial x^{\alpha}} + \tilde{C}^{pq} \frac{\partial}{\partial g^{pq}}.
\end{align}
Note that the expression appearing in the first bracket is exactly the inverse of the 3-metric on the embedded hypersurface that is defined in terms of the 4-metric by the component functions $g_3 = g_{\alpha \beta} \mathrm{d}x^{\alpha} \mathrm{d}x^{\beta}$ as it can be easily checked from $(g_3)_{\beta \mu } \cdot ( g^{\mu \alpha} - \frac{1}{g^{00}} g^{0\alpha} g^{0 \mu} ) = \delta_{\beta}^{\alpha}$. Proceeding along the same lines for the remaining contributions $\tilde{C}^{pq}$ we find that
\begin{align}\label{FHH}
    \bigl[ \mathcal{H}_F(N), \mathcal{H}_F(M) \bigr] =  \mathcal{D}_F\bigl( (g_3)^{\mu \alpha}( N\partial_{\mu} M - M \partial_{\mu}N  ) \bigr).
\end{align}
Hence the commutator between two vertical vector fields yields a tangential vector field with components depending on the metric.
In particular we observe that this last commutation relation is now field dependent. This field dependence can be traced back to the definition of the vertical vector field that is used for the direct sum split and involves the field. 
In particular it is not contributed by some instrinsic property of the theory at hand but solely depends on the chosen observer definition and the thus defined vertical vector field.\\

The commutator algebra that is generated by the three commutation relations (\ref{FDD}), (\ref{FDH}) and (\ref{FHH}) is the form of the hypersurface deformation algebra that is usually presented in the literature\footnote{In most cases the algebra is presented with minus signs on the right hand side of each of the three commutation relations. The sign discrepancy results from different ways of defining the Lie bracket structure for vector fields. If one wants to work with the usual definition of the Lie algebra as left invariant vectorfields on a given Lie group then the Lie bracket of two vector fields is actually the negative of their commutator. Details can be found in \cite{1985AnPhy.164..288I}.}, most famously in \cite{HOJMAN197688}. In this context the appearance of the field components in the last commutation relation is usually referred to by stating that the commutator of two vertical vector fields involves structure functions instead of structure constants. Much work regarding this aspect of the hypersurface deformation algebra was contributed in \cite{1985AnPhy.164..288I} and \cite{1985AnPhy.164..316I}. Note that already there the authors remark that the appearance of the metric in the last commutator results from the metric dependent definition of the vertical vector field. 

With the observer definition presented in \cite{2018PhRvD..97h4036D}, \cite{2011PhRvD..83d4047R} and \cite{Rivera} the authors derive a hypersurface deformation algebra similar to the one obtained in the standard case but with the appearance of different field dependent quantities in the commutator of two vertical vector fields. With the direct sum split of this observer definition the first two commutators remain the same whereas the commutator of two vertical vector fields now reads
\begin{align}
    \left[\mathcal{H}(N), \mathcal{H}(M) \right] = \left(\mathrm{deg}(\mathcal{P}_{mat}(v_A)) -1\right ) \cdot  \mathcal{D}_F\left(\mathcal{P}_{mat}(v_A)^{\mu \alpha}( N\partial_{\mu} M - M \partial_{\mu}N  ) \right),
\end{align}
where $\mathcal{P}_{mat}(v_A)^{\mu \alpha}$ is a quantity that is obtained from the \textit{\textbf{principal polynomial}} $\mathcal{P}_{mat}(v_A)$ of the underlying matter theory used as starting point and $\mathrm{deg}(\mathcal{P}_{mat}(v_A))$ denotes its polynomial degree. Also here the appearance of these field dependent quantities in the commutation relation results from the field dependent definition of the vertical vector field in terms of the observer notion that is used for the direct sum split.
\section{Constraint Hamiltonian Dynamics}
In the previous section we have developed the necessary kinematical tools for the canonical treatment of field theories, mainly the notion  of a slicing of the spacetime manifold and the associated split of the tangent bundle. With these tools at hand we will now proceed with our analysis of diffeomorphism invariant field theories by constructing \textbf{\textit{Hamiltonian dynamics}} from a given Lagrangian.

The diffeomorphism invariance of a given field theory will make its appearance in the Hamiltonian formulation by the form of \textit{\textbf{constraints}} on the conjugate momenta that will then force us to treat the system by means of the techniques developed by Dirac to ensure consistency of Constraint Hamiltonian dynamics.

In particular we will find that the Hamiltonian corresponding to a diffeomorphism invariant Lagrangian field theory is in fact fully constrained, i.e. vanishes on the constraint surface. Analyzing the \textit{\textbf{Poisson algebra}} that is generated by these constraints we will finally nicely recover the underlying diffeomorphism invariance of the theory.\\

As before we want to focus on field theories that are described by a first order Lagrangian $\mathcal{L} : J^1F \rightarrow \Lambda^4M$. Given a slicing of the spacetime manifold we again restrict all fields in consideration to the embedded hypersurface $\Sigma_{t_0}$. Given a section of the field bundle $G \in \Gamma(F)$ we write $G_{t_0} := G \vert _{\Sigma_{t_0}}$ for this restriction. As the slicing provides us with a time vector field $\frac{\partial}{\partial t}$ we can now consider the infinitesimal change of such sections in time direction. More precisely we define the \textbf{\textit{time derivative}} of such sections as Lie derivative along $\frac{\partial}{\partial t}$: 
\begin{align}
    \dot{G}_A := \mathcal{L}_{\frac{\partial}{\partial t}}G_A = \lambda \partial_0 G_A + C^{B0}_{A0} \partial_0 \lambda G_B,
\end{align}
where we used the priorly derived coordinate expression in charts adapted to the slicing for the last equality. Similarly we define the time derivative for the restrictions of section to the hypersurface as $\dot{G}_{t_0}:= \dot{G} \vert_{\Sigma_{t_0}}$, i.e. the time derivative of restrictions is the restriction of the corresponding time derivative.

The first step in our development of a Hamiltonian formulation of the given Lagrangian field theory consists of reexpressing the Lagrangian $\mathcal{L}(v_A,v_{Ap})$ not in terms of spacetime 1-jets of a given field but in terms of spatial jets, i.e. 3-dimensional 1-jets over $\Sigma_{t_0}$ of the field and its time derivative. To this end we follow \cite{2004math.ph..11032G} and define the following map:
\begin{align}
    \begin{aligned}
    \beta_{t_0} : (J^1F)_{t_0} \longrightarrow J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0} \\
    \beta_{t_0}((j^1_pG)_{t_0}) \longmapsto (j^1_pG_{t_0}, \dot{G}_{t_0}).
    \end{aligned}
\end{align}
Here a subscripted $t_0$ denotes the restriction of the involved maps, bundles, etc. to $\Sigma_{t_0}$. Hence $(J^1F)_{t_0}$ is the restriction of the previously constructed jet bundle over $F$ to $\Sigma_{t_0}$, with elements being spacetime 1-jets restricted to the hypersurface, i.e. for $p \in \Sigma_{t_0}$ these elements have adapted coordinate expressions: 
\begin{align}
(j^1_pG)_{t_0} = (x^{\mu},G_A,\partial_m G_A),
\end{align}
where we used that on $\Sigma_{t_0}$ in adapted coordinates $x^0=const$. 

The bundle $J^1(F_{t_0})$ is the first order jet bundle that is constructed over  the restricted field bundle $F_{t_0}$. In particular as the restricted field bundle is a bundle with base space $\Sigma_{t_0}$ this is now a bundle over the embedded hypersurface as well. The elements in $J^1(F_{t_0})$ are spatial 1-jets of sections of this bundle, they are obtained by adjoining spatial derivatives along $\Sigma_{t_0}$ to such a section. In adapted coordinates we have: 
\begin{align}
(j^1_pG_{t_0}) = (x^{\mu},G_A,\partial_{\mu} G_A).
\end{align}
Furthermore $(\mathcal{V}F)_{t_0}$ denotes the restriction of the vertical sub bundle $\mathcal{V}F$ of $TF$. The vertical sub bundle is the sub bundle of $TF$, and hence in particular a bundle over $F$, that is obtained from $TF$ as fibre wise kernel of the vector bundle morphism $(\pi_F)_{\ast} : TF \rightarrow
TM$. Details can be found in \cite{1998physics...1019G}. 

Note that only the vertical subbundle of $TF$ can be be obtained naturally from a given bundle $F$. Furthermore $\mathcal{V}F$ can also be given the structure of a bundle over $M$ with projection $\pi_{\mathcal{V}}:=\pi_{TM} \circ (\pi_F)_{\ast}$. We can think of the restriction of the vertical bundle to the embedded hypersurface as encoding the derivative contributions to a spacetime 1-jet $j^1_pG$ that are vertical to the embedded hypersurface.

In particular the map $\beta_{t_0}$ allows us now to decompose a spacetime 1-jet over $\Sigma_{t_0}$ into a spatial 1-jet and a time derivative
\begin{align}
    \beta_{t_0}(x^{\mu},(G_{t_0})_A, \partial_m(G_{t_0})_A) = (x^{\mu},(G_{t_0})_A, \partial_{\mu}(G_{t_0})_A, (\dot{G}_{t_0})_A).
\end{align}
We denote adapted coordinates on $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ by $(x^{\mu}, v_A, v_{A{\mu}}, \dot{v}_A)$. It can now be shown that the map $\beta_{t_0}$ provides a bundle isomorphism (see \cite{2004math.ph..11032G}). Hence we can either work with spacetime 1-jets of sections over the embedded hypersurface or we work with spatial 1-jets of a section and its time derivative, $\beta_{t_0}$ and its inverse provide the identification of the two approaches.

In the following we use the decomposition map $\beta_{t_0}$ to construct an equivalent version of the Lagrangian that is defined in terms of such spatial 1-jets and time derivatives. 
\begin{definition}[time dependend Lagrangian]
Given a Lagrangian $\mathcal{L} : J^1F \rightarrow \Lambda^4M$ and a slicing $\phi : \Sigma \times \mathbb{R} \rightarrow M$ with $\phi_{t_0}(\Sigma) = \Sigma_{t_0}$ we define the \textbf{\textit{time dependent Lagrangian}} at $t_0$ w.r.t. the slicing $\phi$ as:
\begin{align}
\begin{aligned}
    &\mathcal{L}_{t_0} : J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0} \longrightarrow \Lambda^3\Sigma_{t_0}\\
    \mathcal{L}_{t_0}(j^1pG_{t_0}, \dot{G}_{t_0}) &:= i_{\Sigma_{t_0}}^{\ast} \bigl( \mathcal{L}\circ \beta_{t_0}^{-1}(j^1pG_{t_0}, \dot{G}_{t_0})(\frac{\partial}{\partial t},-)\bigr). 
\end{aligned}
\end{align}
\end{definition}
%
%
%where do we need subscripted p's and where not ???
%
%
In other words we obtain the time dependent Lagrangian on $\Sigma_{t_0}$ by using the inverse of the decomposition map $\beta_{t_0}$ to glue the data consisting of a spatial jet and a time derivative together to a spacetime jet. This spacetime jet is then plugged into the given Lagrangian which yields a volume form, i.e. a 4-form over $M$. We let this 4-form act upon the time vector field to obtain a 3-form which we then pull back along the canonical embedding $i_{\Sigma_{t_0}}$ of $\Sigma_{t_0}$ in $M$, to construct a spatial volume form, i.e. a 3-form on the embedded hypersurface $\Sigma_{t_0}$. 

In adapted coordinates, expressing the Lagrangian as $\mathcal{L} = L \mathrm{d}^4x$, with $\frac{\partial}{\partial t} = \lambda \frac{\partial}{\partial x^0}$ we can easily compute the pullback to obtain the corresponding expression for the time dependent Lagrangian on $\Sigma_{t_0}$:
\begin{align}
    \mathcal{L}_{t_0}(x^{\mu}, v_A, v_{A{\mu}}, \dot{v}_A) = L_{t_0}(x^{\mu}, v_A, v_{\mu}, \dot{v}_A)\lambda \mathrm{d}^3x= L(x^0,x^{\mu}, v_A, v_{\mu}, \dot{v}_A)\lambda \mathrm{d}^3x,
\end{align}
where we defined the coordinate expression of the time dependent Lagrangian $L_{t_0}$ and as before $x^0$ denotes the constant value of the zeroth coordinate function on the embedded hypersurface. As always $\mathrm{d}^3x = \mathrm{d}x^1 \wedge \mathrm{d}x^2 \wedge \mathrm{d}x^3$ is the coordinate volume form on $\Sigma_{t_0}$.\\
%
%
%
%
%
%
%is the following really possible ??????
%
%??????????????????????????????????????

Note that by definition the fibres of the vertical bundle $(\mathcal{V}F)_{t_0}$ carry the structure of a vector space. 
Hence one can construct its dual by standard means and we can thus define the bundle\footnote{Note that the notation $(\mathcal{V}F)_{t_0}^{\dagger}$ is used to distinguish this bundle from the standard dual $(\mathcal{V}F)_{t_0}^{\ast}$ and in particular is not meant to provide any association to the usual use of the $\dagger$ symbol in functional analysis. } $(\mathcal{V}F)_{t_0}^{\dagger} := \mathcal{V}_{t_0}^{\ast} \otimes \Lambda^3\Sigma_{t_0}$. Sections of this bundle provide at each $p \in \Sigma_{t_0}$ a $\Lambda^3_pM$-valued linear map on the fibre of $(\mathcal{V}F)_{t_0}$ over $p$.

We are now going to use the given Lagrangian at time $t_0$, $\mathcal{L}_{t_0}$ to construct a bundle isomorphism between $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ and $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}^{\dagger}$ by means of the \textit{\textbf{fibre derivative}}. Details can be found in \cite{abraham2008foundations} for the standard case in particle mechanics and \cite{2000RpMP...45...67G} and \cite{AIF_1973__23_1_203_0} for a more sophisticated treatment.
As the two bundles at use are in particular products of manifolds they both define trivial bundles with basespace $J^1(F_{t_0})$ and projection being the canonical projection onto the second factor that is provided by the product structure. We now define the fibre derivative w.r.t $\mathcal{L}_{t_0} = L_{t_0}\mathrm{d}^3x$ as bundle map:
\begin{align}
\begin{aligned}
    \mathcal{D}\mathcal{L}_{t_0} : &J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0} \longrightarrow J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}^{\dagger}\\
    &(j^1_pG_{t_0},\dot{G}_{t_0}) \longmapsto (j^1_pG_{t_0},\pi_{G_{t_0}}), \\
    & \hspace{1cm} \text{where}  \ \pi_{G_{t_0}} := \frac{\partial L_{t_0}}{\dot{G}_{t_0}}\mathrm{d}^3x.
\end{aligned}
\end{align}
We denote adapted coordinates on  $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}^{\dagger}$ that are obtained from adapted coordinates on $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ by $(x^\mu,v_A,v_{A\mu},\pi^A)$ where $\pi^A = \frac{\partial L_{t_0}}{v_A}$. In the following we call the thus introduced coordinates $\pi^A$ \textit{\textbf{canonical momenta}}.
Using this we obtain the \textit{\textbf{time dependent Hamiltonian}} as always by.
\begin{align}\label{Ham}
\begin{aligned}
&\mathcal{H}_{t_0} : J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}^{\dagger} \longrightarrow \Lambda^3\Sigma_{t_0} \\
    &\mathcal{H}_{t_0}(j^1_pG_{t_0},\pi_{G_{t_0}}) = \pi_{G_{t_0}}(G_{t_0}) - \mathcal{L}_{t_0}(j^1_pG_{t_0},\dot{G}_{t_0}). 
\end{aligned}
\end{align}
Note that just as the the time dependent Lagrangian the time dependent Hamiltonian takes values in $\Lambda^3\Sigma_{t_0}$ and thus can be integrated once it is evaluated on a section of  $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}^{\dagger}$.
In adapted coordinates we obtain the following expression fot the time dependent Hamiltonian:
\begin{align}
    \mathcal{H}_{t_0}(x^\mu, v_A, v_{A\mu},\pi^A) = \pi^A v_A \mathrm{d}^3x - L_{t_0}(x^\mu,v_A,v_{A\mu},\dot{v}_A) \mathrm{d}^3x.
\end{align}

At this point we would like to remark that the autors in \cite{2004math.ph..11032G} follow a slightly different route to nevertheless arrive at a similar expression for the Hamiltonian. They first integrate the Lagrangian evaluated for a given section of $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ over $\Sigma_{t_0}$ to define a functional on the space of such sections. Then the fibre derivative is taken with respect to this functional to obtain the corresponding integrated version of the Hamiltonian as Legendre transform. 
Hence compared with the approach here they start working on the usual canonical phase space\footnote{One usually defines the canonical phase space at a given time $t_0$ as the space of all local sections of the canonical configuration bundle.} and therefore dealing with functionals earlier. 

Although this is the standard approach
we decided to take an alternative route. The reason for this decision lies in our goal of deducing implications of the diffeomorphism invariance of a given Lagrangian on the corresponding Hamiltonian formulation. To be more precise we are interested in consequences of the PDE system (\ref{DiffeoEqn}) that such a Lagrangian then necessarily has to solve, on the constructed Hamiltonian. In particular we would like to make use of these PDEs for the Lagrangian when computing the time evolution a given Hamiltonian provides by means of a yet to define poisson structure. Just as it was the case for the Lagrangian picture, where we were able to avoid working on infinite dimensional spaces of  sections we would then like to achieve something similar, i.e. avoid working on the canonical phase space and hence with functionals.

Furthermore it is worth mentioning that here in our approach the Legendre transform of the time dependent Lagrangian is only taken w.r.t. the time derivative coordinates $\dot{v}_A$, in contrast to the multimomentum approach where one defines conjugate momenta for all derivative coordinates of the field as it is for instance discussed in \cite{1998physics...1019G}. \\

To proceed further note that $F_{t_0}$ just as $(\mathcal{V}F)_{t_0}^{\dagger}$ define bundles over $\Sigma_{t_0}$. Hence we can construct their jet bundles of some given finite order $q$, $J^q(F_{t_0})$ and $J^q((\mathcal{V}F)_{t_0}^{\dagger})$. 
We define for all $q>0$
\begin{align} 
\mathcal{K}^q_{t_0} := J^q(F_{t_0})\times J^q((\mathcal{V}F)_{t_0}^{\dagger}),
\end{align}
and set $\mathcal{K}^0_{t_0} := F_{t_0}\times (\mathcal{V}F)_{t_0}^{\dagger}$
In the following we will call $\mathcal{K}^q_{t_0}$ the \textit{\textbf{canonical configuration bundle}} at time parameter $t_0$. 

\begin{remark}
For the following considerations the appropriate setting would be the infinite jet bundle $\mathcal{K}^{\infty}_{t_0} := J^{\infty}(F_{t_0})\times j^{\infty}((\mathcal{V}F)_{t_0}^{\dagger})$ construction which can be obtained as limit of finite jet bundles $\mathcal{K}^q_{t_0}F$ (see for instance \cite{saunders_1989}). The reason for this is that certain operations such as taking \textit{\textbf{total}} and \textit{\textbf{variational derivatives}} increase the differential order of functions that are defined on $\mathcal{K}^{q}_{t_0}$.

This does however not pose a problem for us as any function $f_q$ on $\mathcal{K}^q_{t_0}$ can be equivalently considered as a function that is defined on some higher order jet bundle $\mathcal{K}^r_{t_0}$ with $r\geq q$ by using the canonical jet bundle projections $\pi_{r,q}$. In other words given such a function $f_q$ we might define an essentially equivalent function on $\mathcal{K}^r_{t_0}$ by $f_r:=f_q \circ \pi_{r,q}$. If $r$ is chosen high enough we are then guaranteed that taking derivatives of $f_r$ does not leave the r-th order jet bundle. Hence we will simply assume that the order $q$ of the canonical configuration bundle $\mathcal{K}^q_{t_0}$ is chosen large enough s.t. all functions in use and all their total and variational derivatives, etc. can by this identification be considered as being defined on $\mathcal{K}^q_{t_0}$.  
\end{remark}

Our  next step lies in the construction of \textit{\textbf{Poisson bracket}} to encode dynamical information in the Hamiltonian formulation. 
We follow \cite{1997hep.th....9164B} to some extend. Consider the space of sections  $\Gamma(\mathcal{K}^0_{t_0})$ that is traditionally referred to as \textit{\textbf{canonical phase space}}\footnote{One often restricts to sections that have compact support over $\Sigma_{t_0}$} at time $t_0$. 

Just as the Lagrangian might by evaluated on the prolongation $j^2\phi$ of sections $\phi \in \Gamma(F)$ and then be integrated to define a functional on the space of sections we obtain a functional on $\Gamma(\mathcal{K}^0_{t_0})$ by specifying a bundle map on the canonical configuration bundle\footnote{As mentioned in the introduction we try to stick to the convention of denoting density valued maps by calligraphic letters $\mathcal{L}, \mathcal{f},..$ and the corresponding coordinate expressions in terms of the coordinate volume form by the respective letters in standard font $L,f,...$.} $\mathcal{f}: \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0}$. Given a section of $\Gamma(\mathcal{K}^0_{t_0})$ we compose it with this bundle map and then integrate the resulting 3-form over $\Sigma_{t_0}$. We call phase space functionals that are obtained in this way \textit{\textbf{local functionals}}. 
\begin{definition}[local functional]
Let $\mathcal{f}: \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0} $ be a bundle map covering $id_{\Sigma_{t_0}}$, in local coordinates \footnote{For simplicity of the notation in the following we will not denote the arguments of such coordinate expressions explicitly.} $\mathcal{f} = f(x^{\mu},v_A,v_{A\mu},v_{AI}, ... v_{AI_k},\pi^A,\pi^{A \mu}, \pi^{AI} ... \pi^{AI_k})\mathrm{d}^3x$. We define the induced phase space functional as
\begin{align}
\begin{aligned}
    \mathcal{S}_{\mathcal{f}} : &\Gamma(\mathcal{K}^0_{t_0}) \longrightarrow \mathbb{R}\\
    &\phi \longmapsto \mathcal{S}_{\mathcal{f}}[\phi] := \int_{\Sigma_{t_0}} \mathrm{d}^3x f \circ j^q(\phi),
\end{aligned}
\end{align}
where $j^q\phi$ denotes the prolongation of sections of $\mathcal{K}^0_{t_0}$ to sections of $\mathcal{K}^q_{t_0}$. Such phase space functionals are called local functionals.
\end{definition}
In the following we will only work with local functionals. 
Given this definition we can now define a poisson structure on the canonical phase space 
\begin{definition}[poisson bracket]
Let $\mathcal{f},\mathcal{g} : \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0} $ with coordinate expressions $\mathcal{f} = f\mathrm{d}^3x$ and $\mathcal{g} = g\mathrm{d}^3x$. Denote the corresponding local functionals as $\mathcal{S}_{\mathcal{f}}, \mathcal{S}_{\mathcal{g}}$. We define their poisson bracket as the phase space functional given by:
\begin{align}
\left \{ \mathcal{S}_{\mathcal{f}}, \mathcal{S}_{\mathcal{g}} \right \}[\phi] := \int _{\Sigma_{t_0}} \mathrm{d}^3x \left ( \frac{\delta f}{\delta v_A} \frac{\delta g}{\delta \pi^A} - \frac{\delta g}{\delta v_A} \frac{\delta f}{\delta \pi^A}     \right ) \circ j^q(\phi)  .
\end{align}
\end{definition}
Here $\frac{\delta}{\delta v_A}$ and $\frac{\delta}{\delta \pi^A}$ denote the variational derivatives as they are defined in (\ref{varDer}) in terms of the total derivative (for the definition see (\ref{totDer})) on $\mathcal{K}^q_{t_0}$.
%check this definition !!!!!
%
%
%
\begin{align}
    \begin{aligned}
    \frac{\delta f}{\delta v_A} &= \frac{\partial f}{\partial v_A} - D_{\mu}(\frac{\partial f}{\partial V_{A\mu}}) + D_{\mu}D_{\nu} (\frac{\partial f}{\partial v_{A\mu\nu}}) - ... \\
    \frac{\delta f}{\delta \pi^A} &= \frac{\partial f}{\partial \pi^A} - D_{\mu}(\frac{\partial f}{\partial \pi^{A}_{\mu}}) + D_{\mu}D_{\nu} (\frac{\partial f}{\partial \pi^{A}_{\mu\nu}}) - ... \\
    D_\mu f &= \partial _\mu f + \frac{\partial f}{\partial v_A} v_{A\mu} + \frac{\partial f}{\partial \pi^A } \pi ^{A}_{ \mu} + \frac{\partial f}{\partial v_{A\nu}} v_{A\mu \nu} + \frac{\partial f}{\partial \pi^{A}_ {\nu}}\pi^{A}_{ \nu \mu} ... \ \ .
    \end{aligned}
\end{align}

Given this definition we see that the poisson bracket of two local phase space functionals actually only depends on the \textit{\textbf{variational derivatives}} of the corresponding functions. This suggests that we might equivalently define a \textit{\textbf{bracket structure}} directly on the canonical configuration bundle $\mathcal{K}^q_{t_0}$.
\begin{definition}
Let $\mathcal{f},\mathcal{g} : \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0} $ with the usual coordinate expressions in terms of $f$ and $g$. We define their bracket as 
\begin{align}
    \left \{ f \mathrm{d}^3x,g\mathrm{d}^3x\right \} := \left ( \frac{\delta f}{\delta v_A} \frac{\delta g}{\delta \pi^A} - \frac{\delta g}{\delta v_A} \frac{\delta f}{\delta \pi^A} \right ) \mathrm{d}x^3  .
\end{align}
\end{definition}
\begin{remark}
When it does not cause confusion we will also write $\left \{ f ,g\right \}$ for this expression and also might drop the volume factor of the coordinate expressions $\mathrm{d}^3x$ in concrete computations to lighten the notation. 
\end{remark}
The idea to avoid working with functionals and instead directly work over the space of local functions was first developed by Gel'fand, Dickey and Dorfman. For a review see \cite{doi:10.1142/5108}.
Details regarding a construction along the lines that we follow here can be found in \cite{1997hep.th....9164B} and \cite{Barnich1998}. 

Note that this bracket structure is now in particular completely specified in terms of functions on $\mathcal{K}^q_{t_0}$ and nevertheless fully encodes the standard Poisson bracket on local phase space functionals. Two distinct bundle maps $\mathcal{f},\mathcal{g} : \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0}$ might still define the same local functional, namely simply if they only differ by a total divergence of a function $\mathcal{h}: \mathcal{K}^q_{t_0} \rightarrow \Lambda^2\Sigma_{t_0}$, i.e. if it holds that 
\begin{align}
\mathcal{f} = \mathcal{g} +\mathrm{d}x^{\alpha}(D_{\alpha} \mathcal{h}).
\end{align}
We can equivalently express the 2-form valued bundle map $\mathcal{h}$ in coordinates as $\mathcal{h} = h^{\mu} \epsilon_{\mu \nu \alpha} \mathrm{d}x^{\nu} \otimes \mathrm{d}x^{\alpha}$. The total divergence is then expressed as $\mathrm{d}x^{\alpha}(D_{\alpha} \mathcal{h}) = D_{\alpha} h^{\alpha}$. \\

In the bracket structure that we defined on $\mathcal{K}^q_{t_0}$ this is reflected by the fact that the variational derivative annihilates total divergences, in other words we have $\frac{\delta}{\delta v_A}D_{\alpha} h^{\alpha} = 0$ and $\frac{\delta}{\delta \pi^A}D_{\alpha} h^{\alpha}=0$. Hence we are free to add arbitrary total divergences to the given functions \footnote{More rigorously one can also work with equivalence classes and the corresponding quotient, by defining two functions to be equivalent if they define the same functional in the above sense. this is for instance done in \cite{1997hep.th....9164B} and \cite{Barnich1998}.} once we compute the bracket of two functions on $\mathcal{K}^q_{t_0}$. In the standard functional derivative formulation of the Poisson bracket on the canonical phase space this corresponds to integrating by parts. 

With this definition of a bracket on $\mathcal{K}^q_{t_0}$ one obviously recovers the standard properties of Poisson brackets in field theory.
%add properties of poisson bracket ??
One in particular fiends that the bracket satisfies the Leipniz rule and hence defines a \textit{\textbf{derivation}} of functions $\mathcal{f},\mathcal{g},\mathcal{h} : \mathcal{K}^q_{t_0} \rightarrow \Lambda^3\Sigma_{t_0}$ 
\begin{align}
\left \{\mathcal{f}, \mathcal{g}\cdot \mathcal{h} \right \} = \mathcal{g} \cdot \left \{ \mathcal{f}, \mathcal{h} \right \} + \left \{ \mathcal{f} , \mathcal{g}\right \} \cdot \mathcal{h},
\end{align}
where the product of $\Lambda^3\Sigma_{t_0}$-valued maps is defined by $f \mathrm{d}^3x \cdot g\mathrm{d}^3x = (fg)\mathrm{d}^3x$ (for details see \cite{1997hep.th....9164B} and \cite{Barnich1998}). Furthermore one readily computes 
\begin{align}
\left \{ v_A, \pi^B\right \} = \delta_A^B.
\end{align}

Our main goal in the following section will be to translate the PDE system (\ref{DiffeoEqn}) that is obtained from the diffeomorphism equivariance of the Lagrangian into a language that allows us to use the resulting equations in the formulation of Hamiltonian dynamics. To this end we chose adapted coordinates on $J^1F$ that additionally satisfy $x^0 = t$ and hence we find $\frac{\partial}{\partial t} = \frac{\partial}{\partial x^0}$ and $\dot{v}_A = v_{A0}$. 

From the expression (\ref{LieJ1}) of the Lie algebra morphism $j^1(\mathcal{f})$ that lifts vectorfields $\xi \in \Gamma(TM)$ to vector fields on $J^1F$ we then find that using the decomposition map $\beta_{t_0}$ the lift of vectorfields to $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ reads:
\begin{multline}\label{LieJ1Dec}
    \xi \longmapsto \xi_{J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}} = \xi^m \frac{\partial}{\partial x^m} + C_{An}^{Bm} v_B \partial_{m} \xi ^n \frac{\partial}{\partial v_A}
    + C_{An}^{Bm} \partial_{m} \xi^n v_{B\nu} \frac{\partial}{\partial v_{A\nu}}\\
    + C_{An}^{Bm} \partial_{m} \xi^n \dot{v}_{B} \frac{\partial}{\partial \dot{v}_A} - v_{A\nu} \partial_{\mu} \xi^{\nu} \frac{\partial}{\partial v_{A\mu}} 
     - v_{A\nu} \partial_{0} \xi^{\nu} \frac{\partial}{\partial \dot{v}_{A}} 
    - \dot{v}_{A} \partial_{\mu} \xi^{0} \frac{\partial}{\partial v_{A\mu}}
     - \dot{v}_{A} \partial_{0} \xi^{0} \frac{\partial}{\partial \dot{v}_{A}}\\
    + C_{An}^{B\mu} v_B \partial_{\mu} \partial_{\nu} \xi^n \frac{\partial}{\partial v_{A\nu}}
    + C_{An}^{B0} v_B \partial_{0} \partial_{\nu} \xi^n \frac{\partial}{\partial v_{A\nu}}
    + C_{An}^{B\mu} v_B \partial_{\mu} \partial_{0} \xi^n \frac{\partial}{\partial \dot{v}_{A}}
    + C_{An}^{B0} v_B \partial_{0} \partial_{0} \xi^n \frac{\partial}{\partial \dot{v}_{A}}.
\end{multline}
%use x^0 or t ???
If a given Lagrangian on $J^1F$ is now diffeomorphism invariant from this action of vectorfields on the space of functions on $J^1(F_{t_0}) \times (\mathcal{V}F)_{t_0}$ we obtain the following system of PDEs for the corresponding time dependent Lagrangian with coordinate expression $\mathcal{L}_{t_0} = L_{t_0}\mathrm{d}^3x$:
\begin{align}\label{diffeoHam}
    \begin{aligned}
    0 = &\partial_m L_{t_0} \\
    0 = &L_{t_0}^{:A} C_{An}^{Bm} v_B + L_{t_0}^{:A\nu} C_{An}^{B m} v_{B\nu} + \pi^A C_{An}^{B m}\dot{v}_B - L_{t_0}^{:A \mu} v_{A\nu} \delta^{\nu}_n \delta^m_{\mu}
    - \pi^A v_{A\nu} \delta^{\nu}_n \delta^m_{0} \\
    &- L_{t_0}^{:A \mu} \dot{v}_A \delta^0_n \delta^m_{\mu}
    - \pi^A \dot{v}_A \delta^0_n \delta^m_0 + L_{t_0} \delta_n^m \\
    0 = &L_{t_0}^{:A (\nu \vert } C_{An}^{B \vert \mu)} v_B \delta_{\nu}^{p} \delta_{\mu}^{q}
    +\pi^A C_{An}^{B  \mu} v_B \delta_{0}^{p} \delta_{\mu}^{q}
    +L_{t_0}^{:A \nu } C_{An}^{B 0} v_B \delta_{\nu}^{p} \delta_{0}^{q}
    +\pi^A C_{An}^{B  0} v_B \delta_{0}^{p} \delta_{0}^{q},
    \end{aligned}
\end{align}
where we used that in the chosen coordinates $\lambda = 1$ together with the definition of the canonical momenta $\pi^A = \frac{\partial L_{t_0}}{\partial \dot{v}_A}$. 
From this PDE system we deduce the first consequence of the underlying diffeomorphism invariance on the hamiltonian formulation. If we evaluate the last equation in (\ref{diffeoHam}) for $p=0=q$ we get 
\begin{align}\label{ConstrPi}
    0=\pi^A C_{An}^{B0}v_B.
\end{align}
There exist 4 independent linear combinations of the canonical momenta that vanish. This poses a problem for the Hamiltonian formulation as the fibre derivative that allowed us to obtain canonical momenta from the given Lagrangian and hence define the Hamiltonian is then no longer injective. To put it simple the number of independent fibre coordinates on the field bundle $F$ is given by its fibre dimension. On the other hand due to the 4 vanishing linear combinations of canonical momenta the image of the fibre derivative has dimension 4 less.\\

This is something that we could have already seen from the PDE system that encodes diffeomorphism invariance on the level of the EOM (\ref{EOM}). For theories that are describe by a Lagrangian on $J^1F$ evaluating the last equation for $m=p=q=0$ we get 
\begin{align}\label{Constr1}
    0 = \frac{\partial^2 L}{\partial \dot{v}_A \partial \dot{v}_C } C_{An}^{B0} v_B.
\end{align}
Hence we see that the Jacobian of the transformation $\dot{v}_A \rightarrow \pi^B$ is non invertible but has a non vanishing kernel that contains $C_{An}^{B0}v_B$. In particular this shows that the transformation that maps the time derivatives $\dot{v}_A$ to canonical momenta defined by $L$ is locally non invertible. \\

If one now takes a closer look at the equations of motion that are obtained from a diffeomorphism invariant first order Lagrangian we get
\begin{align}
    0 = E^A = \frac{\delta L}{\delta v_A} = L^{:A} - L^{:Am:B} v_{Bm} - L^{:Am:Bp}v_{Bpm}.
\end{align}
Only the last term in this expression contains second derivatives.
Contracting the EOM with $C_{An}^{C0}v_C$ we get:
\begin{multline}
    E^A C_{An}^{C0}v_C = L^{:A0:B0}v_{B00}C_{An}^{C0}v_C + 2L^{:A\mu : B 0} v_{B\mu 0}C_{An}^{C0}v_C + L^{:A\mu : B \nu} v_{B\mu \nu}C_{An}^{C0}v_C\\
    + \text{lower derivative order}.
\end{multline}
Note that only the first term contains second time derivatives. Precisely this term however vanishes due to the last equation in (\ref{EOM}) evaluated for the case of $L$ being a first order Lagrangian. Hence contracting the EOM with $C_{An}^{C0}v_C$ yields for independent linear combinations that are of first derivative order in the time-derivatives.

In the case of second derivative order PDE systems that describe the time evolution of some field\footnote{More precisely we consider such PDE systems that are hyperbolic in the sense that the cauchy problem is well posed for appropriate cauchy surfaces and initial conditions. We will provide a more detailed formulation regarding this in the next chapter.} over $M$ the values of the field together with its first derivatives must be specified on suitable hypersurface of $M$ as \textbf{\textbf{initial values}} in order to uniquely determine solutions away from the hypersurface. 


Any individual equation appearing in the system that is of lower than second derivative order in the time derivatives then does not contribute dynamical information to the evolution process described by the system but  rather poses a restriction on the possible choices of the initial values on the hypersurface . In other words the previously obtained 4 linear combinations that were only of first time derivative order are no evolution equations but  constrain the possible initial data one may specify. These PDEs are then called \textbf{\textit{constraints}}.
Similarly one also calls the conditions on the canonical momenta in the Hamiltonian formulation (\ref{ConstrPi}) constraints.\\

The treatment of Hamiltonian systems with constraints requires specific care as one needs to ensure that once initial data in terms of values for the field and the canonical momenta $(v_A,\pi^B)$ at given time is chosen, the time evolution generated by the Hamiltonian is such that also for later times the thus obtained values of the field and momenta satisfy the constraints. One often states that \textbf{\textit{consistency}} of the Hamiltonian formulation requires that the time evolution does not leave the constraint surface, i.e. the surface that is describe as vanishing set of the constraints.

Much of the treatment of such constraint Hamiltonian systems was developed by Dirac \cite{dirac_1950} and cast into the well know \textit{\textbf{Dirac-Bergmann algorithm}} (\cite{PhysRev.83.1018}, \cite{doi:10.1063/1.523597}). An introduction that mainly deals with constraint Hamiltonian systems in the context of GR can be found in \cite{bojowald_2010}. The necessary computations are included in more details in \cite{thiemann_2007}. \\

Given the previously constructed Hamiltonian (\ref{Ham}) and the Poisson structure on $\mathcal{K}^q_{t_0}$ we now follow along the lines of the Dirac-Bergmann algorithm to render the Hamiltonian in consistent form. We start with the constraints 
\begin{align}
C_n := \pi^A C_{An}^{B0}v_B
\end{align}
that according to the terminology of Dirac are referred to as \textbf{\textit{primary constraints}}. The system is then described by the constraint Hamiltonian which takes the form 
\begin{align}
\mathcal{H}_1 := \pi^A v_A \mathrm{d}^3x -L_{t_0}\mathrm{d}^3x + \lambda^n C_n\mathrm{d}^3x.
\end{align}
The 4 functions $\lambda^n = \lambda^n(x^{\alpha})$ are up to now arbitrary functions that take the role of Lagrange multipliers. Such functions that do not depend on the phase space coordinates $(v_A, \pi^B)$ obey a trivial time evolution w.r.t. any Hamiltonian $\dot{\lambda}^n = \left \{H, \lambda^n \right \} = 0$. The reason for adding them to the previously obtained Hamiltonian is that one can show the unconstrained Hamilton equations of motion obtained in the standard way from $\mathcal{H}_1$ to be equivalent to the constrained Hamilton equations of motion computed from $\mathcal{H}_{t_0}$ (see for instance \cite{bojowald_2010}).

We proceed by computing the time evolution of the primary constraints $\left \{ H_{t_0}, C_n \right \}$. Note that the bracket between two primary constraints can be computed to be 
\begin{align}
    \left \{C_n, C_m \right \} = \frac{\delta C_n}{\delta v_A}\frac{\delta C_m}{\delta \pi^A} - m \leftrightarrow n = \pi^B C_{Bn}^{A0}C_{Am}^{C0} v_C - m \leftrightarrow n .
\end{align}
We now take the second equation in (\ref{diffeoHam}) evaluate it for $m=0$ and act with $\frac{\partial}{\partial \dot{v}_B}C_{Bm}^{D0}v_D$ upon it to obtain: 
\begin{align}\label{CC1}
    0 = L_{t_0}^{:A:B0}C_{An}^{C0}v_CC_{Bm}^{D0}v_D + L_{t_0}^{:A0}C_{An}^{B0}C_{Bm}^{D0}v_D,
\end{align}
where any additional terms vanish as they are prolongations of (\ref{ConstrPi}) once we reinsert the definition of the momenta in terms of $L_{t_0}$. We now take equation (\ref{ConstrPi}) reexpress $\pi^A = L_{t_0}^{:A0}$, act with $\frac{\partial}{\partial v_A}C_{Am}^{v_D}$ and interchange the free indices $m$ and $n$ to get:
\begin{align}\label{CC2}
    0 = L_{t_0}^{:A:B0}C_{An}^{C0}v_CC_{Bm}^{D0}v_D + L_{t_0}^{:A0}C_{Am}^{B0}C_{Bn}^{D0}v_D.
\end{align}
Subtracting (\ref{CC1}) and (\ref{CC2}) yields now $0=\pi^B C_{Bn}^{A0}C_{Am}^{C0} v_C - m \leftrightarrow n$ and we find
\begin{align}
    \left \{C_n, C_m \right \} \approx 0.
\end{align}
Here and in the following $\approx$ denotes \textit{\textbf{weak equality}}, i.e. equality on the constraint surface, the vanishing set of the constraints. Hence $\left \{C_n, C_m \right \} \approx 0$ states that the bracket of two constraints vanishes not identically for all possible canonical coordinates $(v_A,\pi^A)$ but only if one uses the constraints (\ref{ConstrPi}) as we did in the previous derivation. We move on by computing the remaining part of the time evolution of the primary constraints
\begin{align}
    \left \{H_{t_0}, C_m \right \} = -\frac{\delta L_{t_0}}{\delta v_A}C_{Am}^{B0}v_B - \pi^A C_{Am}^{B0}\dot{v}_B = (D_{\mu}(L_{t_0}^{:A\mu}) - L_{t_0}^{:A}) C_{Am}^{B0}v_B - \pi^A C_{Am}^{B0}\dot{v}_B.
\end{align}
We now use that the bracket is only defined up to a total divergence and subtract $D_{\mu}(L_{t_0}^{:A\mu} C_{Am}^{B0}v_B)$ from this result. Doing this we obtain
\begin{align}
    \left \{H_{t_0}, C_m \right \} = -L_{t_0}^{:A} C_{Am}^{B0} v_B - L_{t_0}^{:A\mu} C_{Am}^{B0}v_{B \mu} - \pi^A C_{Am}^{B0} \dot{v}_B.
\end{align}
Reexpressing this result with the use of the second equation in (\ref{diffeoHam}) we find 
\begin{align}
    \left \{H_{t_0}, C_m \right \} = -\pi^A v_{A\mu} \delta^{\mu}_m - \pi^A \dot{v}_A \delta^0_m + L_{t_0}\delta^0_m.
\end{align}
Seperating this equation for the cases $m=0$ and $m = \mu$ we find in particular
\begin{align}
\begin{aligned}
    \left \{C_0 , H_{t_0}\right \} = H_{t_0} =: \mathbf{H} \\
    \left \{C_{\mu} , H_{t_0}\right \} = \pi^A v_{A\mu} =: \mathbf{D}_{\mu}.
\end{aligned}
\end{align}
Continuing the Dirac procedure we have to add also these in Diracs terminology \textit{\textbf{secondary constraints}} with multiplier functions $N=N(x^{\alpha})$ and $N^{\mu}= N^{\mu}(x^{\alpha})$ to the Hamiltonian. We get
\begin{align}
H_{2} = N \cdot \mathbf{H} + N^{\mu} \cdot \mathbf{D}_{\mu} + \lambda^0 C_0 + \lambda^{\mu}C_{\mu}.
\end{align}
We call the thus computed time evolution of the primary constraints the \textit{\textbf{Hamiltonian constraint}} $\mathbf{H}$ and the \textit{\textbf{diffeomorphism constraint}} $\mathbf{D}_{\mu}$.
We find that the new Hamiltonian $H_{2}$ is given by a linear combination of the 4 primary and 4 secondary constraints and hence vanishes on the constraint surface. This observation is summarized in the following theorem
\begin{theorem}
The Hamiltonian $H$ corresponding to any given diffeomorphism invariant Lagrangian field theory on $J^1F$ is a linear combination of constraints, i.e. \textbf{\textit{fully constraint}} and hence vanishes weakly
\begin{align}
   H \approx 0.
\end{align}
\end{theorem}
\begin{proof}
Follows immediately from the above considerations.
\end{proof}
\begin{remark}
Although this consequence of diffeomorphism invariance for the Hamiltonian picture seems to be widely known and accepted it is surprisingly hard to find concrete proofs of it in the literature. Furthermore in most cases the authors assume without deeper investigation that certain results and techniques from the case of classical Hamiltonian dynamics can be generalized to the case of a Hamiltonian treatment of field theories without running into further trouble. As we do not want to follow this practice we provided the precise proof with its full derivation in the above.
\end{remark}
%Citation from Witte ???
Moving on with the Dirac-Bergmann algorithm we now have to compute the time evolution of the secondary constraints and require it to vanish weakly. We have already shown that the bracket between $H_{t_0}$ and the primary constraints $C_n$ yields exactly the secondary constraints and hence $\left \{\mathbf{H}, C_n \right \} \approx 0$  . One also readily finds that the bracket between the diffeomorphism constraint $\mathbf{D}_{\mu}$ and the primary constraints $C_n$ yields the total derivative of the primary constraints and hence vanishes weakly:
\begin{align}
    \left \{\mathbf{D}_{\mu}, C_n \right \} = D_{\mu} (\pi^A C_{An}^{B0}v_B) \approx 0.
\end{align}
Given this the only contributions to the time evolution of the secondary constraints that do not immediately vanish weakly come from brackets between $\mathbf{D}_{\mu}$ and $\mathbf{H}$. 

We are now going to calculate the three possible brackets between the secondary constraints times multiplier. In order to not only compute their time evolution but also deduce the evolution they generate themselves we are not going to use any consequence of the constraints, i.e. we do not immediately drop terms that vanish on the constraint surface but keep them until the end of the calculation. This approach will turn out to be practical when we are going to interpret the result that we will now compute. We start with the bracket between two diffeomorphism constraints and find:
\begin{align}
    \left \{ N^{\mu}\mathbf{D}_{\mu}, M^{\nu} \mathbf{D}_\nu \right \} =
    -M^{\nu}v_{A\nu}D_{\mu}(\pi^AN^{\mu}) + M \leftrightarrow N  
\end{align}
We now add the two total divergences $D_{\mu}(M^{\nu}v_{A\nu} \pi^A N^{\mu})$ and the same expression with $M \leftrightarrow N$ to obtain
\begin{align}
    \left \{ N^{\mu}\mathbf{D}_{\mu}, M^{\nu} \mathbf{D}_\nu \right \} = (N^\mu \partial_\mu M^\nu - M^\mu \partial_\mu N^\nu) \mathbf{D}_\nu,
\end{align}
Where we used that the total derivative acts on the multipliers by $D_\mu N^{\nu} = \partial_\mu N^\nu$ and the terms proportional to $D_\mu v_{A\nu} = v_{A\mu \nu}$ drop out as then $v_{A\mu \nu}$ which is symmetric under the exchange of $\mu$ and $\nu$ is contracted against $N^\mu M^\nu - M^\mu N^\nu$ which obviously is anti-symmetric. \\

We continue with the bracket between diffeomorphism constraint and Hamiltonian constraint. The computation can be simplified significantly by using the linearity to split the bracket w.r.t. the two contributions from $\mathbf{H}$: 
\begin{multline}
    \left \{ N^{\mu}\mathbf{D}_{\mu}, M \mathbf{H} \right \} = \left \{ N^\mu \mathbf{D}_\mu , M \pi^A \dot{v}_A   \right \} - \left \{ N^\mu \mathbf{D}_\mu , M L_{t_0}  \right \} = \\
    - D_\mu (N^\mu \pi^A) M \dot{v}_A - N^\mu v_{A\mu} (M L^{:A} - D_\nu (M L^{:A \nu}))  .
\end{multline}
After adding the total divergence $D_{\mu}(N^\mu \pi^A M \dot{v}_A)$ the first term can readily computed to equal $N^\mu \partial_\mu M \pi^A \dot{v}_A$. Similarly one finds adding a total divergence to the second term yields an expression containing the total derivative of $L$. Adding one further total divergence this expression equals $-N^\mu \partial _\mu M L$. Hence in total we find that the only non vanishing contribution is given by:
\begin{align}
    \left \{ N^{\mu}\mathbf{D}_{\mu}, M \mathbf{H} \right \} = N^\mu \partial_\mu M \mathbf{H}.
\end{align}
We finally compute the bracket between two Hamiltonian constraints. The computation can be simplified to a large extend by making the Leipniz rule satisfied by the bracket. Doing so one trivially finds 
\begin{align}
    \left \{N \mathbf{H}, M \mathbf{H} \right \} = N M \left \{ \mathbf{H}, \mathbf{H}\right \} = 0.
\end{align}
Summing up we have found that the secondary constraints obey the following relations of brackets between them
\begin{align}
    \begin{aligned}
    \left \{N^\mu \mathbf{D}_\mu, M^\nu \mathbf{D}_\nu \right \} &= (N^\mu \partial_\mu M^\nu - M^\mu \partial _\mu N^\nu)\mathbf{D}_{\nu}\\
    \left \{ N^\mu \mathbf{D}_\mu , M \mathbf{H}\right \} &= (N^\mu \partial_\mu M) \mathbf{H}\\
    \left \{N \mathbf{H , M \mathbf{H}} \right \} &= 0.
    \end{aligned}
\end{align}
In particular we find that all these bracket relations are weakly zero and hence we do not need to add any further constraints to the Hamiltonian $H_2$. $H_2$ is consistent and the Dirac-Bergmann algorithm thus terminates. 

Besides of that we observe that the evolution generated by the secondary constraints satisfies the same algebra as the deformation of Hypersurface quantities displayed in (\ref{Alg}). This is something that we could have expected as in the context of Diracs treatment of constraint Hamiltonians the secondary constraints are said to generate gauge transformations which obviously in the case of diffeomorphism invariant field theory precisely correspond to diffeomorphisms. 

When we deviated from the covariant picture by means of slicing spacetime into spatial 3-manifolds the underlying action of the diffeomorphism was translated into the hypersurface deformation algebra (\ref{Alg}).
Hence if the associated Hamiltonian formulation reflects the diffeomorphism invariance of the Lagrangian field theory in the context of constraint hamiltonian dynamics we would expect the secondary constraints to generate diffeomorphisms and thereby their algebra to resemble the hypersurface deformation algebra. this is exactly what happened here.\\
%further comments ??

Note in particular that the previous considerations about possible changes in the algebra relations if one uses a differently constructed vertical vectorfield to obtain the direct sum decomposition of the tangent bundle, for instance by a field dependent observer construction also would affect the constraint algebra computed above. Such a different vertical vector field would for instance influence the decomposition of the spacetime 4-jet of fields by means of changing the definition of $\dot{v}_A$. In the end this would probably yield a different split of the 4 secondary constraints into diffeomorphism constraint, which from the comparison to the deformation algebra can be thought of as generating spatial diffeomorphisms and hamiltonian constraint which we might interprete in a similar way as the generator of diffeomorphism in the chosen time direction. 

Nevertheless even when choosing a different split by the above reasoning we still expect the correspondence between constraint algebra and hypersurface deformation algebra to be valid. Taking again GR as an example there the constraint algebra in the canonical formulation (see for instance \cite{thiemann_2007} for a rigorous derivation) features precisely the same metric dependent contribution in the bracket of two Hamiltonian constraints that we already observed in the commutator of two vertical vector fields. \\

Before we proceed with any further developements we quickly summarize the results of this section in particular with their meaning towards constractive gravity in mind. 

Foremost we have developed a rigorous framework for Lagrangian field theories in terms of the jet bundle formalism.  Giving this framework we provided a precise definition of diffeomorphism invariance of a Lagrangian field theory, which at least infinitesimally turned out out be equivallent to the requirement that the Lagrangian solves a certain first order linear PDE system.  This in particular constitutes a huge achievement from the point of view of constructive gravity as the requirement of finding diffeomorphism invariant dynamics for a given gravitational field theory is now translated into a merely mathematical problem, namely obtaining solutions to the corresponding PDE system. Furthermore solving PDEs is a widely occuring probleme and henceforth the underlying theory is quite extensively developed\footnote{In particular we will see in the following sections how we can always solve such equations at least perturbatively.}.

In the second part we investigated implications of the required diffeomorphism invariance on the associated Hamiltonian formulation. Above all we have proven that the Hamiltonian of any diffeomorphism invariant field theory is fully constraint and hence vanishes weakly. Additionally we revealed the underlying diffeomorphism group in form of the hypersurface deformation algebra in the Poisson algebra satisfied by the secondary constraints of the Hamiltonian. Although this treatment of field theories in the Hamiltonian formulation will play a minor role the the future elaborations yet to come in this thesis it nevertheless is essential for the conceptual understanding of constructive gravity. \\ 

The precise meaning of this last statement can be understood best by taking a closer look at the three different ways the dynamics underlying GR, i.e. the Einstein Field Equations or equivalently the Einstein-Hilbert-Lagrangian or any other equivalent formulation, were reconstructed from first principles that we already mentioned in the introduction. There we have Lovelock who constructed GR from the direct requirement of diffeomorphism invariant dynamics (see \cite{Lovelock1969}, \cite{doi:10.1063/1.1665613} and \cite{doi:10.1063/1.1666069}). 
We have Hojman, Kuchař and Teitelboim who derived the GR dynamics from the requirement that the canonical dynamics represent the hypersurface deformation algebra. And last but not least we have contributions mainly due to Deser (\cite{1970GReGr...1....9D}) nicely reviewed in \cite{2008IJMPD..17..367P} that recover Einsteinian dynamics starting from a linearized version of metric gravity by the requirement that the self-coupling of the gravitational field be consistent in the sense that the total energy momentum is conserved.

With the formerly developed deeper understanding at hand all these methods can be understood as incorporating the principle of diffeomorphism invariance on different levels. Lovelock obviously directly required diffeomorphism invariant dynamics. Hojman, Kuchař and Teitelboim used the hypersurface deformation algebra as guiding principle and required to obtain a representation thereof in the canonical formulation of dynamics. As we have seen in the above this is however a necessary consequence fulfilled by the Hamiltonian dynamics of any diffeomorphism invariant field theory. Finally Deser's self-coupling approach uses energy momentum conversation as driving force. The energy momentum tensor obtained from diffeomorphism invariance via the usual Noether construction presented in \cite{Gotay1992StressEnergyMomentumTA}, that furthermore in the case of GR boils down to the known formula, is always conserved. Hence also here the central requirement can be understood as a consequence of diffeomorphism invariance. \\

Finally we wish to comment on more recent work \cite{2018PhRvD..97h4036D}, \cite{2012PhRvD..85j4042G} and also \cite{2017arXiv170803870S}, where the authors use a similar approach as the one used by Hojman, Kuchař and Teitelboim, however comprehensively generalised to not only recover Einsteinian dynamics but provide a framework that essentially allows one to compute dynamical equations to any given geometry at wish. Also here the fundamental requirement is the representation of the hypersurface deformation algebra in terms of the constraint algebra of the canonical formulation of dynamics. There the observer used there depends on the causal structure of given matter dynamics that use the gravitational field as background geometry, more precisely the principal polynomial of the matter EOM. This quantity also appears in one commutation relations of the hypersurface deformation algebra. Requiring the constraint algebra to form a representation of this the authors conclude that the guiding principle of their framework is the \textit{\textbf{causal compatibility}} between matter gravitational dynamics.

Although the ideas underlying the causal compatibility between matter and gravitational dynamics presented in the referenced work is unarguably of fundamental importance\footnote{We will in fact return to precisely these requirements in the following section.} the interpretation that this constitutes the main ingredient of their framework solely from the appearance of the principal polynomial --- already here the authors of \cite{2018PhRvD..97h4036D} remark that this appearance can in fact be traced back to the used observer frame --- in one of the commutation relations of the hypersurface defomation algebra seems a bit ad hoc.
With our developements in mind we would argue that also in their framework the one and only guiding principal is the requirement of the to-be-constructed dynamics to be invariant under spacetime diffeomorphisms.
\chapter{PDE Theoretic Approach to Constructive Gravity}
\dictum{
We gather main results and techniques from the formal approach to PDE theory with the focus lying in particular on the construction of formal power series solutions to a given PDE. These techniques allow us to formulate a second requirement that is posed on the gravitational dynamics and thus provides the yet missing link between matter and gravitational dynamics, namely their causal compatibility. Furthermore we develop a concise framework for the perturbative treatment of the problem posed by constructive gravity.
}
\section{Formal Theory, Symbols and Involution}
Given the achievement of translating the first requirement we wish to pose on any gravitational dynamics --- the invariance under spacetime diffeomorphisms --- into an equivalent linear, first order PDE system that we presented in the previous chapter any further considerations the we provide in this chapter aim to answer one of the following questions:
\begin{itemize}
    \item Given a concrete matter theory, once we solve the stated PDE system to obtain a gravitational Lagrangian describing a theory of gravity that is sourced by this matter, how can we make sure the two theories are in fact compatible in the sense that the given EOM can actually be solved together?
    \item How can we actually construct solutions to the PDE system ?
\end{itemize}
In order to answer either of the above questions we have to make use of some additional tools that can be obtained from the \textit{\textbf{formal theory}} of partial differential equations.\\

The formal theory of partial differential equations is phrased in terms of differential geometry. A PDE is defined as a submanifold of a certain jet bundle that is constructed over the bundle with base space coordinates being provided by the \textit{\textbf{independent variables}} and fibre coordinates given by the \textit{\textbf{dependent variables}} of the given problem. The advantage of such a description of partial differential equations does not only lie in the fact that one works now with coordinate independent geometric objects, the jet bundle approach also enables one to treat the derivatives of a given function as algebraically independent new variables. Thus many problems that arise in the context of PDEs can be stated and solved in terms of basic \textit{\textbf{linear algebra}}. This in particular yields the additional convenience that many of those arising problems can be solved using \textbf{\textit{computer algebra}}.

An Introduction to the formal theory of partial differential equations can be found in \cite{saunders_1989}. A comprehensive treatment is provided in \cite{seiler2009involution} were also deeper results regarding the algebraic and homological aspects of the formal PDE theory are included. We will especially follow along the lines of \cite{seiler1994analysis} as there the relevant notions are concisely described with just the right level of rigour that is necessary for our subsequent developments. We start by stating the needed definitions most of which can be found in \cite{seiler1994analysis}.
\begin{definition}[PDE]
Let $(F,\pi_F,M)$ be a bundle and $J^qF$ the q-th order jet bundle over $F$. A q-th order PDE $R_q$ on $F$ is a  submanifold of $J^qF$. A solution to given PDE is a section $G \in \Gamma(F)$ s.t. the jet prolongation $j^q(G)$ lies entirely in $R_q$.  
\end{definition}
Sometimes one additionally requires $R_q$ to be fibred. Thereby one ensures that the PDE poses no restrictions on the independent variables, i.e. the base space coordinates on $M$ (this version is for instance used in \cite{seiler1994analysis}).
For many of the following computations it is importatn to know the dimension of the jet bundle of order $q$ over a bundle $F$ with basespace dimension $m$ and fibre dimension $n$:
\begin{align}
    \mathrm{dim}J^qF = n\binom{m+q}{q}
\end{align}
Form this definition of a PDE one can obtain a PDE in the traditional sense from the above definition by specifying an aditional vector bundle $(E,\pi_E,M)$ over the same base space together with a bundle morphism
\begin{align}
    \begin{aligned}
    \Phi : J^qF &\longrightarrow E\\
    (x^a, v_A, v_{Ap},...,v_{AI_q}) &\longmapsto \Phi^{\tilde{B}}(x^a, v_A, v_{Ap},...v_{AI_q}),
    \end{aligned}
\end{align}
such that the PDE is given as kernel of $\Phi$. 

Given such a representation of a PDE can apply the usual jet bundle operations to the individual component functions $\Phi^{\tilde{B}}$, i.e. we can \textit{\textbf{prolong}} them to higher derivative orders by taking the previously defined \textit{\textbf{total derivative}} of the representation $\Phi^{\tilde{B}}$ along any base space direction and we can use the jet bundle projections to \textit{\textbf{project}} them to any lower derivative order at wish. We denote the PDE that is described by the combined set of the given equations $\Phi^{\tilde{B}}=0$ and all possible prolongations $D_i\Phi^{\tilde{B}}=0$ by $R_{q+1} \subset J^{q+1}F$. Similarly we denote higher prolongations of a PDE by $R_{q+r}$. We call $R_{q+r}$ the \textit{\textbf{prolongation}} of $R_q$ to $(q+r)$-th order or short the r-th prolongation of $R_q$. 

Considering this it is particularly interesting to first prolong a given PDE and then use the jet projections to define $R_q^{(1)} := \pi_{(q+1),q}\left ( R_{q+1} \right ) \subset J^qF $. As the following example taken from \cite{seiler1994analysis} illustrates this in general does not recover the equation we started with but only a subset $R_q^{(1)} \subset R_q$, i.e. prolonging and projecting might reveal additional equations.
\begin{example}
Consider the PDE on the first order jet bundle over the trivial bundle $\mathbb{R}^3 \times \mathbb{R}$ with jet bundle  coordinates $(x,y,z,u,u_x,u_y,u_z)$ defined by 
\begin{align}
    R_1 : \begin{cases} u_z + y \cdot u_x = 0 \\
                        u_y = 0.
            \end{cases}
\end{align}
prolonging the first equation w.r.t. $y$, i.e. applying the total derivative 
\begin{align}
D_y = \partial_y + u_{xy} \cdot \partial_{u_x} + u_{yy} \cdot \partial_{u_y} + u_{yz} \cdot \partial_{u_z}
\end{align}
to it we obtain the equation $u_{yz} + y \cdot u_{xy} + u_x =0$. Prolonging the second equation w.r.t. $x$ and $z$ however shows that all second order derivatives in the newly obtained equation vanish and we are left with $u_x = 0$. Inserting this equation into the first equation we furthermore find $u_z = 0$. Hence we find that the prolonged and projected system is given by 
\begin{align}\label{prolo}
    R_1^{(1)} : \begin{cases} u_x = 0 \\
                        u_y = 0\\
                        u_z = 0 .
            \end{cases}
\end{align}
Note that the involved prolongations were mandatory to get this result, the additional equations in (\ref{prolo})  can not be with purely algebraic manipulations. 
\end{example}
Such additional independent equations that can only be obtained by prolonging to a higher order and then projecting the PDE again to the previous order are called \textit{\textbf{integrability conditions}}. 
Usually they might be found by constructing certain linear combination of prolonged equations and thereby eliminating appropriate terms that contain derivatives of leading order such that one ends up with an equation of sub maximal derivative order. In our example we prolonged the first equation w.r.t. $y$ and then subtracted the prolongation of the second equation w.r.t $x$ and $z$ to discharge the second derivatives $u_{xy}$ and $u_{yz}$ respectively. \\

If a PDE $R_q$ already contains all its integrability conditions, i.e. if for all $n\geq 0$ it holds that $R_{q+n}^{(1)} = R_{q+n}$ we call it \textit{\textbf{formally integrable}}.
Formal integrability is of particular importance when one wants to construct \textbf{\textit{power series solutions}} to a given PDE. In orther words more prominent in physics literature, one wants to treat the given problem \textit{\textbf{perturbatively}}. The occurrance of integrability conditions during prolongations of the PDE then hinders one to construct such power series solution order by order. In other words to obtain a power series solution up to some finite order $r$ one not only has to include all the information of the given PDE but also all possible integrability conditions of order up to $r$. One can however in general not simply predict during what prolongations these might be obtained. 

More precisely such a finite power series solution to a given PDE $R_q$ with $q\leq r$, described by the equations $\Phi^{\tilde{A}} = 0$ can be obtained as follows: We take a point $p_0 \in M$, in adapted coordinates on $F$, $x^m(p_0) =: x_0^m$ and construct a general section $G\in\Gamma(F)$ as arbitrary, finite power series around $x_0$:
\begin{align}
\begin{aligned}
    G_A(x^m) :=  \sum_{k=0}^{r} a_{AI_k}I^{I_k}_{i_1...i_k}(x^{i_1}-x_0^{i_k}) \cdot ... \cdot (x^{i_k}- x_0^{i_k}), 
\end{aligned}
\end{align}
%Give a better definition of this expression ??
where $a_{AI_k}$ are constants. Note in particular that the $q$-th jet prolongation of $G$ evaluated at $p_0$ then yields up to combinatorical\footnote{This is a result from our factorless definition of the finite pwoer series. The factorless definition is used as it is in closer relation to the later treatment of perturbative Lagrangians.} factors the following coordinate expression:
\begin{align}
    j^k(G)(p_0) \equiv \left ( x_0^m, a_A, a_{Am}, a_{AI}, ... a_{AI_q} \right ).
\end{align}
Hence plugging in the finite power series into the PDE $\Phi^{\tilde{A}} =0$ and evaluating at $x^m=x^m_0$ yields a purely algebraic equation system for the expansion coefficients up to order $g$ that we can solve. Prolonging the equation to order $q+r$ and then plugging in the power series ansatz we then obtain algebraic equations for the higher expansion coefficients up to order $q+r$ that feature the priorly solved coefficients $a_{AI_0},...,a_{AI_q}$ as inhomogenities. We then can solve these equations for the new expansion coefficients $a_{AI_{q+1}}...a_{AI_{q+r}}$. The solutions in general will be given by expressions that contain the lower order coefficients:
\begin{align}
\begin{aligned}
&R_q : \Phi^{\tilde{A}}(x_0^m,a_A,...,a_{AI_q}) = 0 \\
&R_{q+r} : D_{J_r}\Phi^{\tilde{A}}(x_0^m,a_A,...,a_{AI_{q+r}}) = 0,
\end{aligned}
\end{align}
where as before $D_{J_r} = J^{i_1...i_r}_{J_r} D_{i_1} ... D_{i_r}$.
It is important to observe that if during the prolongation integrability conditions appear we get additional equations of lower order that only can be solved in terms of the lower order coefficients. This process severely obstructs any perturbative treatment of equations that might produce such integrability conditions. Going to a certain fixed order one can then never be entirely sure if the expansion obtained coefficients are now really fixed or further change due to integrability conditions that arise during any higher prolongation.  Thus without making sure the PDE at hand is formal integrable any perturbative solution might still be too general as not all of the information provided by the PDE is used to obtain it, there might be further information hidden in integrability conditions that are yet to reveal. Although the perturbative treatment of PDEs compromises a huge field in theoretical physics, techniques that allow for the proof of formal integrability of a given PDE that we will subsequently present seem to be to a large extent unknown in this area of research. \\

We have seen that the occurrance of integrability conditions thoroughly disturbs the order by order construction of power series solutions, and hence provides the notion of a formally integrable PDE with its significance. Without further developements checking formal integrability of a given PDE is however extensively involved, as in principle one would need to check the infinite number of conditions $R_{q+r}^{(1)} = R_{q+r}$ corresponding to an infinte number of prolongations and projections of the PDE. Thus the subsequent developments concentrate on the question how we might predict in advance whether or not integrability conditions occur during the prolongation of a given PDE, without actually carrying out the prolongation.

To that end we take a closer look at the order by order construction of power series solutions.
In each order $q+r$ only a certain number of the newly obtained highest order coefficients $a_{AI_{q+r}}$ is in fact determined by the prolonged PDE $R_{q+r}$, some of them might be chosen freely.
For the special case of \textit{\textbf{quasi linear}}\footnote{Note that although if $R_q$ is not quasi linear all the prolongations will be quasi linear as the total derivative always yields quasilinear equations. For the same reason PDEs that are obatined as EOMs to a given Lagrangian will always be quasilinear.} PDEs, i.e. PDEs that are linear in the highest order derivatives that occur, the expansion coefficients that can be chosen freely are simply given as the kernel of a matrix that is obtained from the highest derivative part of the PDE. This matrix is called the \textit{\textbf{symbol}} of the PDE.
\begin{definition}[symbol]
Given a PDE $R_q$ with representation $\Phi^{\tilde{A}}=0$, its symbol is the matrix that describes the following linear equation system 
\begin{align}
    M_q : \left ( \frac{\partial \Phi^{\tilde{A}}}{v_{AI_q}} \right ) a_{AI_q} = 0.
\end{align}
Similarly we can directly obtain the symbol of the prolonged equations $R_{q+r}$ as  
\begin{align}
    M_{q+r} : \left ( \frac{\partial D_{J_r}\Phi^{\tilde{A}
    }}{v_{AI_{q+r}}} \right ) a_{AI_{q+r}} = 0. 
\end{align}
\end{definition}
Will will synonymously call the matrix and the cooresponding equation system the symbol of the PDE, the precise meaning can be inferred from the context. When constructing order by order power series solutions precisely those $a_{AI_q}$ that lie in the kernel of the symbol $M_q$ can be specified arbitrarily. In particular their number is equal to the dimension of the kernel of $M_q$. The total number of expansion coefficients in the given order $q$ can be computed to be given by $n\binom{m+q-1}{m-1}$.
Hence the rank of the symbol $M_q$ and all its prolongations $M_{q+r}$ govern all the information regarding the size of the solution space of the given PDE. Furthermore note that the for integrability conditions to occur one must be able by means of some linear combinations to get rid of the highest derivative order contributing to a certain equation. Such linear combinations obviously only exists if the symbol has sub maximal rank.  In fact integrability conditions can only occur if the symbol suffers from rank defects. This can be seen in more detail by prolonging a given PDE and then taken its linearization, i.e. the Jacobi matrix of its representation:
\begin{align}
\begin{bmatrix}
        \frac{\partial D_i\Phi^{\tilde{A}}}{\partial v_{AI_{q+1}}} & \vline & \frac{\partial D_i \Phi^{\tilde{A}}}{\partial v_{AI_k}} \ \ k \leq q \\
        \cmidrule(lr){1-3}
        0 & \vline & \frac{\partial \Phi^{\tilde{A}}}{\partial v_{AI_k}} \ \ k \leq q 
\end{bmatrix}.
\end{align}
%reformat this, use tikz ??
The lower right block is simply the Jacobi matrix of the unprolonged PDE. The upper left block is exactly the prolonged symbol matrix $M_{q+1}$. Only if $M_{q+1}$ has sub maximal rank there exist certain linear operations that produce zero rows in $M_{q+1}$ and hence yield equations of sub maximal derivative order. Given such there exist essentially three possibilities. If the corresponding linear operation also produce a zero row in the upper right block we are simply left with an overall zero row which thus can be removed. If the obtained row in the upper right block is not equal to zero there might nevertheless exist a linear combination of equations in the original system $R_q$ that is equivalent to this row. Also then we do not obtain an integrability condition. Only if the row that is produced in the upper right block yields a new equation that is independent of the original system we have found an integrability condition.  
Hence for an integrability condition to occur it is necessary to have sub maximal rank in the prolonged symbol.

This discovery can be summed up by the following formula that allows one to compute the manifold dimension of the prolonged and projected PDE $R_q^{(1)}$ in terms of the manifold dimension of the prolonged PDE $R_{q+1}$ and the dimension of the solution space of the prolonged symbol $\mathrm{dim(}M_{q+1})$, ie. the dimension of its kernel that is proven in \cite{seiler1994analysis}:
\begin{align}
    \mathrm{dim}(R_{q}^{(1)}) = \mathrm{dim}(R_{q+1}) - \mathrm{dim}(M_{q+1}).
\end{align}

Given this observation the question is no if can avoid the possible occurance of integrability conditions entirely by performing only such prolongations that are certain to not produce rank defects in the prolonged symbol. These obviously only constitute a subset of all possible prolongations and hence in general do not yield all independent equations of the prolonged PDE. In special cases they however nevertheless contain the full information of $R_{q+1}$.

The following considerations that we use to make this previous statement precise only involve linear algebra. Therefore we are free to apply linear operations to the symbol $M_q$ to equivalently work with its . We call the thus obtained row reduced symbol matrix the solved form of $M_q$. Furthermore we follow \cite{seiler1994analysis} in introducing the \textit{\textbf{class}} of a \textit{\textbf{derivative index}} $I_k$. We label the coordinates of $M$ from $1$ to $m$. The class of a derivative index is the according to this labeling smallest $i$ s.t. there exist $j_2,...,j_k\geq i$ with $I^{I_k}_{ij_2...j_k} \neq 0$, i.e. the "smallest" spacetime derivative label that occurs in the higher derivative index $I_k$.

We proceed by sorting the columns of the solved symbol $M_q$ according to classes without demanding a particular ordering of the elements inside a given class. We call a row of $M_q$ with pivot belonging to class $k$ a row of class $k$ and we denote the number of rows in $M_q$ that are of class $k$ by $\beta_q(k)$. Note that there is no deeper meaning to the class of a derivative index. We could in particular apply a simple change of coordinates on $J^qF$ to completely mix up the class sortation. The reason why we nevertheless sort the columns of $M_q$ according to classes is that we will use this labeling to introduce a systematic way of computing all possible prolongations that are guaranteed to not produce rank defects in the symbol and therefore are also certainly free of integrability conditions.

This can be achieved proceeding as follows:
Given an equation that corresponds to a row of class $k$ we only prolong such it w.r.t. $D_j$ for  $j \leq k$. The corresponding independent variables $x^j$ are then called the \textbf{\textit{multiplicative variables}} of this equation. If we proceed like this for the whole PDE, i.e. prolonging each equation only with respect to its multiplicative variables, we are guaranteed to not produce integrability conditions. All prolongations that are obtained in such a way will have distinct pivot elements in $M_{q+1}$ and hence are independent. Therefore we get no rank defects in the prolonged symbol and hence no integrability conditions.\\ 

For the above reason it would be particular nice to be able to prolong the equations of a given PDE $R_q$ only w.r.t. to their multiplicative variables and nevertheless know that we one obtains all independent equations of $R_{q+1}$ by doing so, as then we would know that the PDE never generates integrability conditions. This idea leads to the notation of an \textit{\textbf{involutive symbol}}. 
\begin{definition}[involutive symbol]
The symbol $M_q$ of a PDE $R_q$ is called involutive if 
\begin{align}\label{sumBeta}
    \sum_{k=1}^m k\beta_q(k) = \mathrm{rank}(M_{q+1}).
\end{align}
\end{definition}
\begin{remark}
Whereas the rank of the prolonged symbol obviously does not change under a change of coordinates,
as we have outlined above the class of a derivative index and hence also the sum of the beta numbers that is involved in this definition are coordinate dependent notions. One can however show that the sum is the same in a certain class of coordinates, so called \textit{\textbf{$\boldsymbol{\delta}$-regular}} coordinates. These are characterised by the requirement that the sum admits its maximal value.
We thus have to restrict to $\delta$-regular coordinates for the above definition to be well defined.
Hence for concrete computations in principle one would have to check that the chosen coordinates are $\delta$-regular.  We will however not be too much concerned by that restriction as in the future developments we will only have to deal with linear PDEs and for such we will trivially obtain the maximum value of the sum from the dimension of the underlying space of independent variables, without ever having to introduce classes of derivative indices.
\end{remark}
Note that the sum of these beta numbers is a lower bound for the rank of the prolonged symbol $M_{q+1}$. $\sum_{k=1}^m \beta(k)$ is precisely the number of independent equations of order $q+1$ that can be obtained by prolongations with respect to multiplicative variables only.  
Hence for a PDE with an involutive symbol we can obtain all independent equations of order $q+1$ by prolongations with respect to multiplicative variables only. Any prolongation w.r.t. a non multiplicative variable then is necessary of sub maximal order $\geq q$ and hence either is already included in $R_q$ or produces integrability conditions. There are further interesting properties of involutive symbols most of which can be found in \cite{seiler2009involution} and \cite{seiler2009involution}. It is also worth noting that the idea of analyzing the generation of integrability conditions along the lines presented above can be traced back to early works of Janet \cite{janet1920systemes}, \cite{MSM_1927__21__1_0} and Riquier \cite{bateman_1910} in the context of Janet-Riquier theory.

Our main use of the notion of an involutive symbol will be a reformulation of the requirement that a given PDE shall not generate integrability conditions in any prolongation order in terms of a condition that unlike formal integrability can be checked in a finite amount of steps. to that end we a \textit{\textbf{PDE}} to be \textit{\textbf{involutive}} if it is formally integrable and has an involutive symbol.
\begin{definition}[involutive PDE] \label{invol}
We call a PDE $R_q$ involutive if it is formally integrable and its symbol $M_q$ is involutive.
\end{definition}
In order to show that unlike formal integrability, involution can actually be checked in finite manner we would like to state the following result that is proven in \cite{seiler1994analysis}.
\begin{theorem}\label{invoCons}
Let $R_q$ be a PDE with involutive symbol $M_q$ then it holds that:
\begin{itemize}
    \item $M_{q+1}$ is involutive too.
    \item $(R_{q}^{(1)})_{+1} = R_{q+1}^{(1)}$ .
\end{itemize}
\end{theorem}
\begin{proof}
Can be found in \cite{seiler1994analysis}.
\end{proof}
It is important to understand the second implication of the involution of $M_q$. On the left hand side of the equality we take the prolonged and projected equation $R_q^{(1)}$ and prolong it to order $q+1$. In particular any integrability conditions that were obtained during the projection are hence prolonged. On the right hand side we take the prolonged equation $R_{q+1}$ prolong it to order $q+2$ and then project it down to order $q+1$. Therefore the equality states that for the case of PDEs with involutive symbol all integrability conditions that might be found during projection from the second prolongation with order $q+2$ to order $q+1$ are actually prolongations of integrability conditions that were obtained one order before, during the prolongation to order $q+1$.
Using these two implications of the involution of $M_q$ we can proof the following statement
\begin{theorem}
A PDE $R_q$ is involutive according to definition (\ref{invol}) if its symbol $M_q$ is involutive and performing one prolongation and projection reveals no additional integrability conditions: $R_q^{(1)} = R_q$ .
\end{theorem}
\begin{proof}
The following proof can also be found in (\cite{seiler1994analysis}). As it is quite short we nevertheless also provide it here explicitly.
Looking at the definition of an involutive PDE we have to proof that the assumption $R_q^{(1)} = R_q$ already guarantees formal intgerabillity, i.e. it holds for all $r \in \mathbb{N}$ that $R_{q+r}^(1) = R_{q+r}$. We proof the statement by induction and start with $r=0$. Here the requirement reads $R_q^{(1)}=R_q$ which is precisely the assumption of the theorem. Assuming it holds that $R_{q+r}^{(1)}=R_{q+r}$, as $M_q$ is involutive, theorem (\ref{invoCons}) states that $M_{q+r}$ is involutive too and hence using again the above theorem we find that $R_{q+r+1}^{(1)}= (R_{q+r}^{(1)})_{+1}$. Using the induction hypotheses $(i.h.)$ we thus have in total
\begin{align}
   R_{q+r+1}^{(1)} \overset{(\ref{invoCons})}{=} (R_{q+r}^{(1)})_{+1} \overset{(i.h.)}{=} (R_{q+r})_{+1} = R_{q+r+1} 
\end{align}
which completes the proof. 
\end{proof}
Hence to proof involution of a given PDE, which in  particular comprises formal integrability, we only have to proof the involution of the symbol ---which only encompasses standard linear algebra--- and then check whether one further prolongation and projection reveals integrability conditions. Therefore in concrete applications we proceed as follows: We first compute the symbol $M_q$ and its prolongation to check whether $M_q$ meets the requirement 
\begin{align}
        \sum_{k=1}^m k\beta_q(k) = \mathrm{rank}(M_{q+1}).
\end{align}
If it does we proceed be computing the prolonged PDE $R_{q+1}$ to investigate whether or not one further prolongation reveals additional integrability conditions. This can be achieved by comparing dimensions
\begin{align}
    \mathrm{dim}(R_{q+1}) - \mathrm{rank}(M_{q+1}) = \mathrm{dim}(R_q^{(1)}) \stackrel{?}{=} \mathrm{dim}(R_q).
\end{align}
If this requirement is also met the PDE is involutive and hence in particular formally integrable. We are then in a position where we can apply perturbative techniques like the construction of power series solutions to solve the given PDE order by order without running into trouble due to the generation of integrability conditions.

If either one of the above conditions is not met one needs to work somewhat harder. It can actually be proven that any given PDE can be completed to an equivalent involutive PDE that has the same solution space by a finite number of prolongations. This is famously known as \textbf{\textit{Cartan-Kuranishi Theorem}}. A proof of it can be found in \cite{sweeney1968}. Further details are included in \cite{seiler2009involution} and \cite{seiler1994analysis}.

In this thesis we will actually never have to complete a PDE to involution. This is a huge advantage as it will turn out that the PDE (\ref{DiffeoEqn}) encoded the diffeomorphism invariance of a given Lagrangian is rather big and therefore any concrete computation involving it poses a real technical problem not to mention the technicalities arising if we actually had to work with its prolongation. \\

Before we proceed with a deeper investigation of the PDE (\ref{DiffeoEqn}) from the point of view of formal PDE theory we would like to return to the priorly mentioned fact that the extensive use of linear algebra in formal PDE theory allows one to solve many of the associated problems by means of computer algebra systems. In fact there already exist several implementation that were developed for precisely that task. One such can be found the last chapters of \cite{seiler1994analysis} although it seems like there is not much recent contribution to this project. Another such tool consists of the Maple package Janet (see \cite{Janet2} and \cite{Janet}) that implements similar techniques steming mainly from Janet-Riquier theory in the computer algebra system Maple. 

Although for our case the treatment of the PDEs that will arise once we consider concrete examples of our developed framework in chapter 4 by means of computer algebra will be inevitable we wont use either of the implementations presented above. The reason for that being the fact that the shear dimension of the systems that we will encounter forces us to use computer algebra that is more spezialized towards the systems at with regard to memory usage. To that end the computer algebra that we used for these examples was newly developed to a large extend. Details can be found in chapter \ref{computerAlg}.    


\section{Causal Compatibility of Matter and Gravitational Dynamics}
Besides of its role in the definition of an involutive PDE the symbol turns out to be also closely related to the causal structure of a given PDE. To that end we define its principal part, the so called \textbf{\textit{principal symbol}}.
\begin{definition}[principal symbol] \label{PSym}
Let $\Gamma(\Lambda^1M) \ni k = k_{a} \mathrm{d}x^a$ be a one form. The principal symbol to a given PDE $R_q$ described as kernel of the map $\Phi^{\tilde{A}}$ is the matrix
\begin{align}
    T^{\tilde{A} B}(k) = \left ( \frac{\partial \Phi^{\tilde{A}}}{\partial v_{BI_q}} \right ) J_{I_q}^{i_1...i_q} k_{i_1} \cdot ... \cdot k_{i_q}.
\end{align}
\end{definition}
\begin{remark}
Note that it is essential to complete a given PDE to its equivalent involutive form before one can obtain its principal symbol in  a meaningful way. The reason for that lies in the fact that lower derivative order terms that therefore a priory are not present in the principal symbol might nevertheless generate integrability conditions that enter in the highest derivative order. Obviously the completion to involution depends on the given PDE, hence in the following treatment we will assume that all involved PDEs are already involutive.
\end{remark}
The entries of the principal symbol are homogeneous polynomials in the components of the one form $k_a$.
The connection of the principal symbol to the causal structure of a given PDE can be nicely illustrated by considering \textit{\textbf{wave like}} solutions to the PDE in the \textit{\textbf{infinite frequency}} limit. 
More precisely we insert the following ansatz into the PDE:
\begin{align}\label{waveAns}
    G_A(x^m) = a_A(x^m) \cdot \mathrm{Re}\left \{ e^{\frac{iS(x^m)}{\lambda}} \right \},
\end{align}
Plugging this ansatz into the PDE, described by $\Phi^{\tilde{A}}=0$ and taking the infinite frequency limit $\lambda \rightarrow 0$ one obtains in leading order, i.e. in $\lambda^{-q}$ the following equation
\begin{align}
    \left ( \frac{\partial \Phi^{\tilde{A}}}{\partial v_{BI_q}} \right ) J_{I_q}^{i_1...i_q} k_{i_1} \cdot ... \cdot k_{i_q} = 0,
\end{align}
where now $k_a = - \partial_aS(x^m)$. Hence if the ansatz shall provide a solution to the PDE in the desired limit it in particular has to solve $T^{\tilde{A} B}(k) = 0$. The infinite frequency limit $\lambda \rightarrow 0 $ is sometimes also called \textit{\textbf{geometric optical limit}}. In terms of standard electrodynamics on a possibly curved background it treats light propagation by means of approximating light waves as geometrical rays. Also in the context of any other PDE at hand this limit can be thought of as describing the ray approximation to the propagation of wave like solutions. Further information can be found in \cite{seiler1994analysis}, \cite{2012arXiv1211.1914K}, \cite{2011PhRvD..83d4047R} and \cite{2018PhRvD..97h4036D}.

To sum up we have seen that in order for the ansatz (\ref{waveAns}) to yield a nontrivial solution with $k_a \neq 0$ to the given PDE $k_a$ has to satisfy the condition $T^{\tilde{A} B}(k) = 0$. To that end the principal symbol must not be injective. The non injectivity of $T^{\tilde{A} B}(k)$ can be expressed by the vanishing of certain polynomials that can be obtained  as sub determinants of the principal symbol. To clarify that last statement further we will in the following restrict to the case where the representation of the PDE is obtained from Euler-Lagrange equations of a Lagrangian. Then we have in particular the situation where the principal symbol matrix is a square matrix. We write 
\begin{align}
T^{A B}(k) =  \left ( \frac{\partial E^{A}}{\partial v_{BI_q}} \right ) J_{I_q}^{i_1...i_q} k_{i_1} \cdot ... \cdot k_{i_q}.
\end{align}
In the simplest case the non injectivity can now be described as the vanishing of the determinant $\mathrm{det}(T^A_B(k))$. There is however a slight obstruction to this. If the theory at hand satisfies gauge symmetries then $T^A_B(k)$ is necessarily non injective irrespective of the particular $k_a$ (see for instance \cite{2018PhRvD..97h4036D}).

Returning to the relevant example of $E^A$ describing the second derivative order EOM of a diffeomorphism invariant theory of gravity we find that the principal symbol takes the following form:
\begin{align}
    T^{A B} (k) = \left (\frac{\partial E^A}{\partial v_{BI}} \right )J_I^{pq} k_p k_q = E^{A: BI} J_I^{pq} k_p k_q.
\end{align}
Taking now a closer look at the PDE (\ref{DiffeoEqn}) that such EOM then necessarily  satisfy we find from the last such equation:
\begin{align}\label{symbolDef}
\begin{aligned}
    T^{A B} (k) C_{An}^{Cm}v_Ck_m = 0 \\
    T^{B A} (k) C_{An}^{Cm}v_Ck_m = 0 .
\end{aligned}
\end{align}
Hence there exits 4 independent vectors $C_{An}^{Cm}v_Ck_m$ that are in the kernel of the principal symbol and $T^{AB}(k)$ is not injective but as a 4 dimensional rank defekt\footnote{Note that this is closely related to the previous derivation of the 4 constraints of such EOM.}. To compensate for that we have to require $T^{AB}(k)$ to not only be injective but to have at least a 5 dimensional kernel in order to ensure to have at least a one dimensional subspace in this kernel of $T^{AB}(k)$ that is not generate simply due to the gauge symmetry of the theory at hand. More formally we require that the adjunct matrix of order $4$ vanishes. Here the adjunct matrix of order $4$ is the $\binom{n}{4} \times \binom{n}{4}$ square matrix, with $n$ as before being the fibre dimension of the relevant field bundle, that has entries given by the order $4$ sub determinants of $T^{AB}(k)$:
\begin{align}\label{MinorDef}
    Q_{(A_1...A_4) (B_1...B_4)}(k) := \frac{\partial^4 (\mathrm{det}(T^{AB}(k)))}{\partial T^{A_1 B_1}(k) ... \partial T^{A_4 B_4}(k)}.
\end{align}
In other words we index the entries of the adjunct matrix $Q_{(A_1...A_4) (B_1...B_4)}(k)$ by a symmetric 4 tuple of rows $(A_1...A_4)$ and a symmetric 4 tuple of columns $(B_1...B_4)$ that encode the rows and columns that must be removed from the matrix $T^{AB}$ to obtain a certain $(n-4) \times (n-4)$ sub matrix. The value of $Q_{(A_1...A_4) (B_1...B_4)}(k)$ is then given by the determinant of this sub matrix. 
%check row columns problem !
%
%
%
% !!!!!!!!!!!
One can show (\cite{2018PhRvD..97h4036D} and \cite{2009JPhA...42U5402I}) that due to the existance of the 4 independent null vectors in (\ref{symbolDef}) also this requirement of a vanishing adjunct matrix\footnote{Obviously one can obtain similar results for other gauge symmetries \cite{2018PhRvD..97h4036D}. The only difference then lies in the particular expression for the null vectors and the dimension of the rank defekt in the symbol.} boils down to the vanishing of a certain homogeneous polynomial in the components $k_a$. More rigorously one obtains the following general expression for the entries of the adjunct matrix of order 4 that holds for any diffeomorphism invariant field theory at hand:
\begin{align}\label{diffeoMinor}
    Q_{(A_1...A_4) (B_1...B_4)}(k) = \epsilon^{0123} \epsilon^{0123} \chi_{A_10}(k) \cdot ... \cdot \chi_{A_43}(k) \cdot \chi_{B_10}(k) \cdot ... \cdot \chi_{B_43}(k) \cdot \mathcal{P}(k),
\end{align}
%check this, maybe actually epsilon is evaluated ???
% 
%
%
%
where $\chi_{Am}(k) = C^{Bm}_{An} v_B k_m$ are the 4 independent vectors that constitute the kernel of the principal symbol. Hence the non trivial zeros of the adjunct matrix are completely determined by the zeros of $\mathcal{P}(k)$. We call this homogeneous degree $2n-16$ polynomial in the components $k_a$ the \textit{\textbf{principal polynomial}}  $\mathcal{P}(k)$ of the EOM.
This explicit form of the entries of the adjunct matrix of order 4 is also vital for concretely computing the principal polynomial. We simply have to take a $(n-4) \times (n-4)$ sub matrix of the principal symbol with non vanishing determinant by removing 4 rows $(A_1...A_4)$ and 4 columns $(B_1...B_4)$. From the choice of removed rows and columns we can compute the occuring prefactor expression 
\begin{align}\label{prefacF}
f_{(A_1...A_4)(B_1...B_4)}(k) := \epsilon^{0123} \epsilon^{0123} \chi_{A_10}(k) \cdot ... \cdot \chi_{A_43}(k) \cdot \chi_{B_10}(k) \cdot ... \cdot \chi_{B_43}(k).
\end{align}
Then we compute the determinant of the chosen sub matrix, i.e. the explicit value $Q_{(A_1...A_4)(B_1...B_4)}(k)$. The principal polynomial can now be obtained by dividing the determinant by the computed prefactor:
\begin{align}
    \mathcal{P}(k) = \frac{Q_{(A_1...A_4)(B_1...B_4)}(k)}{f_{(A_1...A_4)(B_1...B_4)}(k)}.
\end{align}
In particular the only technical complexity in this computation is provided by calculating one order 4 subdeterminant of the principal symbol. 
Note that if the theory at hand incorporates different gauge symmetries than the discussed diffeomorphism invariance one can obtain similar formulae with the concrete order of the appropriate sub matrices and the expression for the prefactor obviously depending on the explicit form of the gauge symmetry.
\begin{definition}[principal polynomial of EOM]
Given a field bundle $(F,\pi_F,M)$ with $n$-dimensional fibres and a Lagrangian on $F$ that generates EOM with derivative order $q$. Let  $T^{AB}(k)$ be the principal symbol of the EOM. Assume further that the $s$ vectors $\chi_{A1}(k),...,\chi_{As}(k)$ form a basis of $\mathrm{ker}(T^{AB}(k))$. The principal polynomial $\mathcal{P}(k)$
is the homogeneous degree $qn - (q+2)s$ polynomial in the components $k_a$ that satisfies the following relation with the components of the adjunct matrix $Q_{(A_1...A_s)(B_1...B_s)}(k)$ of $T^{AB}(k)$:
\begin{align}
   Q_{(A_1...A_s)(B_1...B_s)}(k) = \epsilon^{i_1...i_s} \epsilon^{j_1...j_s} \chi_{A_1i_1}(k)\cdot ... \cdot \chi_{A_si_s}(k) \cdot \chi_{B_1j_1}(k) \cdot ... \cdot \chi_{B_sj_s}(k) \cdot \mathcal{P}(k).
\end{align}
\end{definition}
Hence we have seen that for a wave ansatz of the form (\ref{waveAns}) to provide a non trivial solution to the given PDE, the corresponding gradients $k_a = - \partial_aS(x^m)$ must be zeros of the principal polynomial, i.e. yield $\mathcal{P}(k) = 0$.
Such one forms are called \textit{\textbf{characteristic}} one forms. We can now restrict the principal polynomial to a given point $p \in M$ to obtain 
\begin{align}
    \begin{aligned}
    \mathcal{P}_p : &T^{\ast}_pM \longrightarrow \mathbb{R}\\
    &k \longmapsto \mathcal{P}(k).
    \end{aligned}
\end{align}
The set of one forms that are characteristic in $p \in M$ is then precisely given as \textbf{\textit{vanishing set}} of the polynomial function $\mathcal{P}_p$ and will be denoted by $V_p(\mathcal{P}_p)$. Sometimes it is also called characteristic variety as it can be shown that this indeed defines a variety in the strict algebraic sense. Returning to our wave ansatz we now see that precisely those $k \in V_p(\mathcal{P}_p)$ are admissible wave covectors for a non trivial solution of the given PDE. Hence the principal polynomial encodes the entire information regarding the propagation of such wave like solution in the geometric optical limit.\\ 

We will also use the principal polynomial to decide whether or not a given PDE is \textit{\textbf{predictive}} in the sense that the PDE allows us to specify initial data on a suitable hyper surface of $M$ and the use the PDE to uniquely predict the values of the involved fields away from the initial data hypersurface. We follow along the lines of the second chapter of \cite{Rivera}.
\begin{definition}[Cauchy problem]
We call the Cauchy problem to a given PDE well posed if specifying suitable initial data on a suitable initial data hypersurface there exits a unique solution of the PDE satisfying the initial data and depending continuously on the chosen data.  
\end{definition}
As explained in \cite{Rivera} this should really be a fundamental requirement fulfilled by any meaningful physical field theory. Roughly speaking it simply states that the EOM of the given theory can be used to evolve collected, i.e. measured data in order to predict future values of the appropriate fields.
Before we can finally explore the implications of a well posed initial value problem on the corresponding principal polynomial we quickly recall the following definition which also can be found in \cite{Rivera}:
\begin{definition}[hyperbolic polynomial]
A homogeneous polynomial $\mathcal{P}_p : T_p^{\ast}M \rightarrow \mathbb{R}$ is called hyperbolic w.r.t. $h\in T_p^{\ast}M$ with $\mathcal{P}(h) \neq 0$ if for all $q\in T_p^{\ast}M$ the equation $\mathcal{P}(q + \lambda h)=0$ only has real solutions $\lambda \in \mathbb{R}$.
\end{definition}
\begin{remark}
If a given polynomial $\mathcal{P}_p$ is hyperbolic and hence we find such $h\in T^{\ast}_pM$ one actually readily obtaines further $\tilde{h}\in T^{\ast}_pM$ such that $\mathcal{P}_p$ is hyperbolic w.r.t. those $\tilde{h}$ as well. In fact one can show (see for instance \cite{Rivera} and \cite{10.2307/24900665}) that $\mathcal{P}_p$ is then also hyperbolic w.r.t any covector in 
\begin{align}
    C_p(\mathcal{P}_p,h) := \{ \tilde{h} \in T_p^{\ast}M \ \vert \ \mathcal{P}_p(\tilde{h}- \lambda h) = 0 \implies \lambda > 0\}.
\end{align}
We call $C_p(\mathcal{P}_p,h)$ the \textbf{\textit{hyperbolicity cone}} containing $h$. One can also show that $C_p(\mathcal{P}_p,h)$ indeed constitutes an open and convex cone \cite{10.2307/24900665}.
\end{remark}
The notion of hyperbolic polynomials and the corresponding hyperbolicity cones is best illustrated by a picture. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Poly.pdf}
    \caption{Hyperbolicity cone $C_p$ and vanishing set $V_p$ of a second and a fourth degree polynomial taken from \cite{Rivera}.}
    \label{hyperbol}
\end{figure}
One can now see the underlying geometric interpretation hyperbolic covectors for a given polynomial. If $\mathcal{P}_p$ is hyperbolic w.r.t. $h$ than any affine line in direction $h$ intersects $\mathrm{deg}(\mathcal{P}_p)$ times with the vanishing set $V_p(\mathcal{P}_p)$.
Finally we can state the connection between hyperbolic polynomials and the well posedness of the Cauchy problem.
\begin{theorem}
If a given PDE is well posed in a region of $M$ than the principal polynomial restricts to a hyperbolic polynomial $\mathcal{P}_p$ at every $p$ in that region. Furthermore at $p \in M$ exactly those hypersurface containing $p$ and having a conormal vector that is hyperbolic w.r.t. $\mathcal{P}_p$ are suitable \textit{\textbf{initial data hypersurfaces}}, i.e. yield a well posed Cauchy problem.
\end{theorem}
\begin{proof}
The proof can be found in \cite{Hormander1977} and \cite{Ivrii_1974}.
\end{proof}
Hence we can sum up the situation as follows: If we have given a predictive PDE in the sense that there exist hypersurfaces such that the corresponding Cauchy problem is well posed, then we can calculate the principal polynomial of this PDE. This principal polynomial is then necessarily a hyperbolic polynomial when restricted to the appropriate points $p \in M$. As shown above the zero variety of the principal polynomial describes the propagation of waves in the infinite frequency limit. Furthermore from the principal polynomial we can compute the hyperbolicity cones. These then precisely encode the admissible initial data hypersurface of the given PDE.\\ 

The above consideration immediately opens up a problem that we are going to outline in the following. in the first section we developed the necessary techniques that allowed us to phrase the requirement of diffeomorphism invariance in terms of a linear, first order PDE. given a concrete space time geometry in terms of a field bundle $F_{grav}$ computing solutions to the corresponding equivariance equation (\ref{DiffeoEqn}) would yield a diffeomorphism invariant theory of gravity in terms of a Lagrangian for the gravitational field and the corresponding gravitational equations of motion.  

On the other hand in order to really put that theory into use and calculate predictions we additionally would need a theory of the matter that couples to that particular gravitational field. In the context of constructive gravity usually one even assumes that the matter theory is given first according to some phenomenological observation and one then tries to construct a compatible theory of gravity. In either case we get additional to the gravitational EOM a set of matter EOM.

More precisely the situation now looks as folllows.
The gravitational Lagrangian is given by a bundle map 
\begin{align}
    \mathcal{L}_{grav} : J^2F_{grav} \longrightarrow \Lambda^4M
\end{align}
where $F_{grav}$ is the gravitational field bundle. Additionally we now also have a matter field described by a Lagrangian that depends on the values of the matter field, its first derivatives\footnote{We might also here consider the case where the Lagrangian also depends on second derivatives of the matter field and is thus required to be degenerate s.t. the matter EOM are again of second derivative order but for simplicity we restrict to first order matter Lagrangians.} but also exploits the spacetime geometry provided by the gravitational field. Thus the matter Lagrangian is bundle map
\begin{align}
    \mathcal{L}_{mat} : F_{grav} \times J^1F_{mat} \longrightarrow \Lambda^4M,
\end{align}
where $F_{mat}$ is the matter field bundle. In the following we will denote adapted coordinates on $J^1F_{mat}$ by $(x^m,\phi^{\tilde{A}},\phi^{\tilde{A}}_m)$ Hence the total Lagrangian is given by:
\begin{align}
\begin{aligned}
    \mathcal{L}_{tot} : &J^2F_{grav} \times J^1F_{mat} \longrightarrow \Lambda^4M \\
    &\mathcal{L}_{tot} = \mathcal{L}_{grav} + \mathcal{L}_{mat}.
\end{aligned}
\end{align}
We now get two sets of EOM one from taking the variational derivative of $\mathcal{L}_{tot}$ w.r.t the gravitational field bundle coordinates, i.e. the gravitational EOM and one from taking the variational deriavtive w.r.t. the matter field bundle coordinates $\phi^{\tilde{A}}$, the matter EOM.
The gravitational EOM now feature an inhomogeneous terms that describes how the gravitational field is sourced by the matter.
\begin{align}
    0 = \frac{\delta \mathcal{L}_{tot}}{\delta v_A} = \frac{\delta \mathcal{L}_{grav}}{\delta v_A} + \frac{\delta \mathcal{L}_{mat}}{\delta v_A}.
\end{align}
The matter EOM is given by 
\begin{align}
    0 = \frac{\delta \mathcal{L}_{mat}}{\delta \phi^{\tilde{A}}}.
\end{align}
We can then compute the two principal polynomials that correspond to the two EOMs, $\mathcal{P}_{grav}$ and $\mathcal{P}_{mat}$. Note in particular that these will depend both on the values of the gravitational field. We denote the corresponding vanishing sets by $V_{p,grav}$ and $V_{p,mat}$. As we require both theories to be predictive, i.e. have well posed Cauchy problems we can also compute the two hyperbolicity cones $C_{p,grav}$ and $C_{p,mat}$ that at each spacetime point $p\in M$ encode the possible choices of initial data hypersurfaces for the two theories. 

Doing this at given $p \in M$ we would now end up with one of the following three situations of which we will show in the following only one can serve the purpose of describing a meaningful physical theory:
%3 possible situations
%add figures here ???
\begin{itemize}
    \item $C_{p,grav} \neq C_{p,mat}$ and $V_{p,grav} \neq V_{p,mat}$
    
This situation obviously incorporates the special case where the two hyperbolicity cones are disjoint. This case can be immediately ruled out as then there would not exist a single initial data hypersurface that is common to both theories and hence could serve as a starting point for solving the coupled matter gravity system.   

Also if the two hyperbolicity cones are not disjoint but nevertheless do not coincide we immediately get problems. Then we would either find a suitable matter initial data hypersurface that is no admissible initial data hypersurface for the gravitational EOM, or vice versa we would find a gravitational initial data hypersurface that could not serve for specifying the initial data for the matter EOM. As it can be seen for instance in \cite{Rivera} and \cite{2011PhRvD..83d4047R} the hyperbolicity cone of a given EOM is in close relation with feasible observer definitions for the underlying theory. Any physical meaningful observer must be able to collect data in his spatial surrounding and use the EOM provided by the theory to evolve this initial data to future values and thereby make physical predictions. To allow for this process the spatial surrounding of any possible observer must serve as initial data hypersurface. Hence if there exist initial data hypersurfaces that are exclusive to either the matter or the gravitational EOM we would end up with certain observers that are limited to one of the two theories. Such observers could in particular meaningfully measure data and predict processes in the corresponding free theory, being either the matter theory or gravity, but become meaningless once the coupled case is concerned.

In the following we want to restrict to situations that allow for a unified observer definition, i.e. the observers of gravity and matter theory obey the same laws and in particular posses the same properties no matter if the two theories are coupled or one considers the case of a free treatment of either one independently. Hence we discharge the case where $C_{p,grav} \neq C_{p,mat}$.

\item $C_{p,grav} = C_{p,mat}$ and $V_{p,grav} \neq V_{p,mat}$ 

For this case all initial data hypersurfaces are common to both theories. Nevertheless the vanishing sets of the two principal polynomials differ. We have seen that these vanishing sets govern the propagation behaviour of wave like solutions in the infinite frequency limit. It might hence be possible that if $V_{p,grav} \neq V_{p,mat}$ the wave propagation of matter waves and gravitational waves shows quite different properties. The structure provided by the vanishing sets of the two principal polynomials in particular governs the information regarding future domains that such a propagating wave might causally influence. Doing so it particular encodes the speed of such waves. 

With the recent detection of gravitational waves \cite{2017ApJ...848L..12A}, \cite{2017PhRvL.119n1101A} and \cite{2016PhRvL.116f1102A} also further insight regarding their propagating speed is gained rapidly. 
To provide an example using the observed time difference between the gravitational wave event GW170817 and the gamma ray burst GRB 170817A emitted by a Binary Neutron Star Merger the propagation speed of gravitational waves has already be constrained to deviate not more than $-3\cdot 10^{{-}15}c$ and $+7\cdot 10^{{-}16}c$ from the speed of light $c$ (see \cite{2017ApJ...848L..13A}). Hence it seems reasonable to additionally require that $V_{p,grav} = V_{p,mat}$ to incorporate the thus already observed similarities in the propagation of gravitational and matter waves into our framework. We are hence left with the final option.
\item $C_{p,grav} = C_{p,mat}$ and $V_{p,grav} = V_{p,mat}$ 

This is the only option left and henceforth precisely what we will require in the following. Not only all possible initial value hypersurfaces and therefore all possible observers of the matter and gravity EOM then coincide, this option also requires that the causal structure in the form of wave propagation in the infinte frequency limit is the same for the two theories.  
\end{itemize}
The previous investigation yields the second and last requirement that we wish to pose on the theory of gravity that we want to construct. We require that given a matter theory that employs the theory of gravity as geometric background, the two theories are \textit{\textbf{compatible}} in their \textit{\textbf{causal structure}} in the sense that the two principal polynomials at each point yield the same vanishing set, i.e. it holds for all $p \in M$ that: 
\begin{align}
    V_{p,grav} = V_{p,mat}.
\end{align}
Note that then $C_{p,grav} = C_{p,mat}$ immediately follows. Further note that as  here we are working with vanishing sets of individual polynomials the above condition simply requires the two polynomials to be compromised of the same irreducible polynomial factors. In most relevant cases the factorization of the polynomials can easily be obtained. In other words here we do not need to work with the polynomial ideals that vanish on the respective sets and hence do not need further techniques from algebraic geometry such as Gröbner basis that allow for efficiently deciding whether or not a given polynomial lies inside such an ideal and hence vanishes on the corresponding set.  
%remove last statement

Summing up the previous achievements it is now conceptually completely clear how we must proceed to find the most general gravitational lagrangian that is compatible with any given matter theory. We first solve the equivariance equations (\ref{DiffeoEqn}) to implement the required diffeomorphism invariance of the theory of gravity paying close attention to make sure that the resulting EOM are of no higher than second derivative order. Then we compute the gravitational principal polynomial and compare it to the one obtained from the matter theory. We require that at each spacetime point the two polynomials define the same vanishing set and by this get further conditions the gravitational lagrangian has to solve, this time encoding the causal compatibility between matter theory and gravity.\\

We end this section by reformulating the previous results in terms of a definite construction recipe that is displayed as Algorithm (\ref{Algo1}).
\begin{algorithm}[ht]
\SetAlgoLined
\KwData{Matter theory $ \mathcal{L}_{mat} : F_{grav} \times J^1F_{mat} \rightarrow \Lambda^4M$
}
\KwResult{Most general diffeomorphism invariant, causal compatible $\mathcal{L}_{grav} : J^2F_{grav} \rightarrow \Lambda^4M$ }
Compute the constant tensors $C^{Bm}_{An}$ \\
Set up the equivariance equations (\ref{DiffeoEqn}) \\
Solve the equivariance equations (\ref{DiffeoEqn}) to obtain the most general $L_{grav}(x^m,v_A,v_{Am},v_{AI})$\\
Compute the EOM $\frac{\delta L_{grav}}{\delta v_A}$\\
Consider the most general subtheory that has 2nd order EOM\\
Calculate the principal polynomials $\mathcal{P}_{grav}$ and $\mathcal{P}_{mat}$\\
Determine the arbitrary quantities in $L_{grav}$ s.t. for all $p \in M$ : $V_{p,grav} = V_{p,mat}$
 \caption{Construction of Gravitational Lagrangian}\label{Algo1}
\end{algorithm}
\section{Perturbative Approach to Constructive Gravity}
In the previous section we have completed the required diffeomorphism invariance of the gravitational theory with the second requirement $V_{p,grav}=V_{p,mat}$ ensuring the compatibillity with a given matter theory. In principal it is therefore entirely clear how one can solve the problem posed by constructive gravity. Practically obtaining solutions to the diffeomorphism equivariance equations (\ref{DiffeoEqn}) is however a different question. Although the relevant PDE is linear in the unknown Lagrangian and furthermore only of first derivative order --- methods for solving  such PDEs have actually already been known for a long time \cite{Han2015} --- already treating the standard case of finding the most general such Lagrangian that can be constructed from a metric tensorfield and its first and second derivatives is surprisingly hard. The reason is the shear size of the resulting pde system. For the stated example we will later see that the PDE system is compromised of a total of 136 partial differential equations and the Lagrangian is a function of 150 independent variables.  

Regarding this we are essentially left with one of two options that avoid the mammoth task of solving the equivariance equations in fully general form and nevertheless furnish us with access to two exeptionally relevant realms of gravitational physics. We can either apply \textbf{\textit{symmetry}} methods to the equivariance equations and thereby for a given gravitational field obtain solutions, i.e. theories of gravity that describe the relevant phenomenology under these symmetry assumptions. We could for instance solve the equations for spatial homogenity and isotropy to obtain a description of cosmology in the generalised spacetime geometry. Alternatively --- and this is the path that we will take in the following --- we can perturbatively solve the equivarinace equations by means of a \textit{\textbf{power series solution}} to some finite order. We in particular chose this path with the treatment of gravitational waves in mind.  The recent developments in the dectection of gravitational waves makes them an excellent tool to test alternatives to GR \cite{2010PhRvD..81f4008Y}, \cite{2011PhRvD..83j4022B}, \cite{2017PhRvD..95j4027Z}, \cite{2013LRR....16....9Y}. \\

We have already gathered the necessary techniques that allow us to construct perturbative solutions to the equivariance equations in a rigorous fashion in the previous section. primarly we have seen that such an approach only yields meaningful results if the relevant PDE is \textit{\textbf{involutive}}. We are now going to apply these technniques to the equivariance equations (\ref{DiffeoEqn}). To that end it is important to take a closer look at the jet bundle that underlies the  geometric treatment of the PDE provided by (\ref{DiffeoEqn}). Recall that the lagrangian was given by a bundle map 
\begin{align}
\mathcal{L} : J^2F \longrightarrow \Gamma^4M.
\end{align}
Abstractly the PDE (\ref{DiffeoEqn}) is then given as a submanifold of the first order jet bundle $J^1(J^2F \times \Lambda^4M)$ over the trivial bundle $J^2F \times \Lambda^4M$. 
We can denote adapted coordinates on $J^1(J^2F \times \Lambda^4M)$ by $(x^m,v_A,v_{Am},v_{AI},l,l^{m},l^{A},l^{Am},l^{AI})$ such that we could obtain the formal representation of the PDE (\ref{DiffeoEqn}) by replacing the bundle map $L$ the derivatives $L^{:m},L^{:A},...$ with fibre coordinates $l$ and derivative coordinates $l^m,l^A,...$. Hence in terms of formal PDE theory the equivariance equation reads 
\begin{align}\label{DiffeoEqnFormal}
\begin{aligned}
    0 &= l^{m} \\
    0 &= l^{A} C_{An}^{Bm} v_B + l^{Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + l^{AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ} + l \delta^m_n \\
    0 &= l^{A(p\vert}C_{An}^{B \vert m)} v_B + l^{ AI} \bigl[ C_{An}^{B(m\vert} 2 J_I^{\vert p) q} - \delta^B_A J_I ^{pm} \delta_n^q \bigr] v_{Bq} \\
    0 &= l^{AI} C_{An}^{B(m\vert} v_B J_I^{\vert p q )}.
    \end{aligned}
\end{align}

We begin the perturbative treatment of constructive gravity with one of the main results that in the end justifies the perturbative approach:
\begin{theorem}
PDE (\ref{DiffeoEqnFormal}) is involutive.
\end{theorem}
\begin{proof}
We start by proofing that the symbol of the equation (\ref{DiffeoEqnFormal}) is involutive. Due to the fact that the PDE is of first derivative order it symbol simply consists of its homogeneous part, namely the PDE with the second equation being replaced by the homogeneous counterpart:
\begin{align}
    0 &= l^{A} C_{An}^{Bm} v_B + l^{Ap} \bigl[ C_{An}^{Bm} \delta_p^q - \delta_A^B \delta_m^n \bigr] v_{Bq} + l^{AI} \bigl[ C_{An}^{Bm} \delta_I^J - 2 \delta_A^B J_I^{pm} I^J_{pn}  \bigr] v_{BJ}.
\end{align}
This homogeneous system describes functions that are invariant under the action of any lifted vector field (\ref{LieJ2}). 

The first step in proving involution of the symbol is computing the sum of beta numbers. We have to pay attention as we have to compute this sum in coordinates that maximize it, i.e. are $\delta$-regular. However due to the fact that we are dealing with first order equations the classes of the derivative indices all posses exactly one member and range from 1 to $\mathcal{k}$ where 
\begin{align}
    \mathcal{k} := \mathrm{dim}(J^2F) = 4+n+4n+10n.
\end{align}
In the following we are going to assume that $\mathrm{dim}(J^2F)$ is bigger than the number of independent equations in (\ref{DiffeoEqnFormal}) which is given by 140 as otherwise in general there will not exist solutions in the first place. 
The maximum value for the sum of betas is obtained when we transform the coordinates on $J^2F$ s.t. the first equation is solved w.r.t. the derivative coordinate with class $\mathcal{k}-140+1$ and the $i$-th equation is solved w.r.t. the derivative coordinate of class $\mathcal{k}-140+i$ such that the last equation is solved w.r.t. the derivative coordinate of maximum class $\mathcal{k}$. If we display the symbol $M_1$ as matrix and label its columns by class starting from 1 and its rows from 1 to 140 this transformation this coordinate transformation on $J^2F$ corresponds to bringing the matrix to the following upper triangular form
\begin{align}\label{symbolMat}
\begin{bmatrix} 
      0 & \hdots & 0 & 1 & a_{1,1} & \hdots & \hdots & a_{1,139} \\
      \vdots &  & \vdots & 0 & \ddots & \ddots & & \vdots  \\
      \vdots & & \vdots & \vdots & \ddots  & 1 & \ddots & \vdots  \\
      \vdots & & \vdots & \vdots & & \ddots & \ddots & a_{139,139} \\
      0 & \hdots & 0 & 0 & \hdots  & \hdots & 0 & 1
\end{bmatrix}
\end{align}
for some $a_{i,j}$ that are functions on $J^2F$ . The sum of betas can then be computed to be given by:
\begin{align}
    \sum_{i=1}^{\mathcal{k}} i \beta_1(i) = \sum_{i = 1}^{140} \mathcal{k} -140+i.
\end{align}
We now have to show that this equals the rank of the prolonged symbol $M_2$. By previous arguments this is the case if we obtain all equations of order 2 by prolongation w.r.t. multiplicative variables only. We again consider the symbol solved to the form given by matrix (\ref{symbolMat}). 
To simplify the notation in the following we denote coordinates on $J^2F$ that bring the symbol to this form by $y_a$ for $a = 1,...,\mathcal{k}$. Note that such coordinates always exist. We take an arbitrary row $\sigma$ from the matrix (\ref{symbolMat}) for $\sigma=1,...,140$ and then consider the corresponding equation $E_{\sigma}$. The multiplicative variables of this $\sigma$-th equation are $y_1,...,y_{\mathcal{k}(\sigma)}$ where we introduced 
\begin{align}
\mathcal{k}(\sigma):= \mathcal{k} -140 + \sigma.
\end{align}
We now show that prolonging this equation w.r.t. an arbitrary non multiplicative variable $y_b$ for $b>\mathcal{k}(\sigma)$ yields no additional independent contribution to the prolonged symbol. This is achieved by showing that we can add prolongations w.r.t. multiplicative variables to such a non multiplicative prolongation $D_bE_{\sigma}$ and thereby obtain an equation of first derivative order which is thus not present in $M_2$. Note that in the matrix (\ref{symbolMat}) multiplicative variables of a given row are exactly those corresponding to the columns that are left to the pivots $1$ (including these) and contain zeros and non multiplicative variable correspond to the columns right to the pivots and hence contain the $a_{i,j}$. Prolonging $E_{\sigma}$ w.r.t. such a $y_b$, i.e. computing $D_bE_{\sigma}$ we get second order derivatives of the form $y_{bc}$ for $c > \mathcal{k}(\sigma) $. For these derivatives there are now essentially two possibilities, either $b<c$ then we can get rid of this contribution by considering the equation with leading derivative, i.e. lowest ranked derivative $y_c$. This is precisely equation $E_{\mathcal{k}^{-1}(c)}$. We prolong this equation w.r.t. $y_b$. As $b<c$ this prolongation is now multiplicative. We then subtracting this multiplicative prolongation from $D_bE_{\sigma}$ to remove this second derivative order contribution $y_{bc}$.

Consider now the case where $c<b$. We then simply apply the procedure the other way round. We take equation $E_{\mathcal{k}^{-1}(b)}$, apply the multiplicative prolongation w.r.t. $y_c$ and subtract the result $D_cE_{\mathcal{k}^{-1}(b)}$ from the original prolongation. Doing this we can remove all second order derivatives from the non multiplicative prolongation $D_bE_{\sigma}$ by subtracting  multiplicative prolongations $D_bE_{\mathcal{k}^{-1}(c)}$ or $D_cE_{\mathcal{k}^{-1}(b)}$. Hence any non multiplicative prolongation does not contribute an independent new second derivative order equation to the prolonged symbol $M_2$ and the symbol. The symbol is therefore involutive. 

In order to proof that not only the symbol but also the PDE (\ref{DiffeoEqnFormal}) itself involutive we have to show that the PDE generates no integrability conditions after one prolongation. We have already discussed that prolonging each equation only w.r.t. its multiplicative variables never produces integrability conditions. Furthermore we have just shown that any prolongation w.r.t. non multiplicative variables can be reduced to first derivative order by adding further multiplicative prolongations in the way it is outlined above. Hence the question is if the first derivative order contributions that we get from such a procedure are already included in $R_1$ or contribute additional independent equations. 

We first consider the homogeneous case. We take again the m-th equation and work as before in the coordinates $y^a$ on $J^2F$ that solve the symbol $M_1$ to (\ref{symbolMat}). Note that this homogeneous linear first order PDE can then be concisely written as
\begin{align}
    E_{\sigma} = l_{\mathcal{k}(\sigma)} + \sum_{i = 1}^{140-\sigma} a_{\sigma,i} l_{\mathcal{k}(\sigma)+i} .
\end{align}
We can equivalently write this in the form of vector fields on $J^2F$. We define for $\sigma = 1..140$ the vector fields $\zeta_{\sigma} \in \Gamma(J^2F) $ corresponding to $E_{\sigma}$:
\begin{align}
    \zeta_{\sigma} := \frac{\partial}{\partial y_{\mathcal{k}(\sigma})} + \sum_{i = 1}^{140-\sigma} a_{\sigma,i} \frac{\partial}{\partial y_{\mathcal{k}(\sigma)+i}}.
\end{align}
The solutions to the given homogeneous PDE are then precisely those functions on $J^2F$ that are invariant under these vector fields, i.e. that satisfy $\zeta_{\sigma} f = 0$ for all values of $\sigma$.
Note that we obtained the homogeneos PDE from precisely such an invariance requirement. The only difference to the previous case is that we now work coordinates that render the symbol particularly simple. 
In terms of vector fields this corresponds to the fact that the $\zeta_{\sigma}$ commute (see also \cite{seiler1994analysis}).
%136 vs 140 !!!
%
%
%

Carefully analysing the above procedure we now find that the remaining first order equation that is produced when eliminating all second order contributions of a non multiplicative prolongation $D_bE_{\sigma}$ by multiplicative prolongations is simply given by the commutator equation
\begin{align}
    \left [\zeta_{\sigma}, \zeta_{\mathcal{k}^{-1}(b)} \right] = 0.
\end{align}
Hence for the system to not produce integrability conditions this commutator must now be given by a linear combination of unprolonged equations. For the particular case of the $\zeta_{\Sigma}$ this is trivially true as the commutator of two such fields vanishes. This can however already be seen from the original system (\ref{DiffeoEqnFormal}).
Recalling that we obtained homogeneous counterpart to (\ref{DiffeoEqnFormal}) by requiring invariance under the prolonged vector fields $\xi_{J^2F}$ (\ref{LieJ2}) that were in particular constructed via a Lie algebra morphism. It is therefore clear that also these vector fields close under the operation of taking commutators. Hence no integrability conditions are generated and the homogeneous system is involutive.

This is no special property of the homogeneous PDE in consideration but is actually general to all linear homogeneous first order PDEs. Such PDEs that can be described entirely by vector fields are called \textit{\textbf{complete}} if the vector fields that generate the given system form a Lie algebra w.r.t. the commutator. If the vector fields commute the system is furthermore called a \textit{\textbf{Jacobian}} system. One can show along the same lines that we followed her that in fact any complete system is involutive. For details regarding the treatment of complete systems and in particular for a proof of this last statement see \cite{seiler1994analysis} and also for further information \cite{Clebsch1866}, \cite{caratheodory1956variationsrechnung} and \cite{lie1970theorie}.\\

What remains show is that the presence of the inhomogenity in $(\ref{DiffeoEqnFormal})$ does not change this observation, i.e. does not generate integrability conditions. We again work in the coordinates $y_a$. The inhomogenity is only present in the equations $5-20$ and only when the indices in (\ref{DiffeoEqnFormal}) are such that $m = n$. We label the equations s.t. it contributes to equation $5,9,13$ and $17$. In each case the contribution is simply given by an extra term of $+l$ and the respective equations $\Tilde{E}_{\sigma}$ of the inhomogeneous PDE read now 
\begin{align}
     \tilde{E}_{\sigma} = \begin{cases}
     E_{\sigma} + l \ \text{for} \ \sigma \in \{ 5,9,13,17 \}\\
     E_{\sigma} \ \text{else}.
     \end{cases}
\end{align}
One can now show along the same lines the we followed in the homogeneous case that the inhomogeneous system is involutive if for any two such equations $\tilde{E}_{\sigma}$ and $\tilde{E}_{\tau}$ the so called \textit{\textbf{Jacobi brackets}} or sometimes also called Mayer bracket 
\begin{align}
  \llbracket \tilde{E}_{\sigma}, \tilde{E}_{\tau} \rrbracket = \sum _{i = 1}^{\mathcal{k}} (\frac{\partial \tilde{E}_{\sigma}}{\partial l_i}) D_{i}\tilde{E}_{\tau} -  (\frac{\partial \tilde{E}_{\tau}}{\partial l_i}) D_{i}\tilde{E}_{\sigma} 
\end{align}
vanish on $R_1$ (see  \cite{seiler1994analysis} and also also Example 2.3.12 in \cite{seiler2009involution}), i.e. can be obtained by linear combinations of unprolonged equations. Comparing this to the previous case where the possible integrability conditions where built from the commutator we now get extra terms whenever either $\sigma$, $\tau$ or both are contained in $\{ 5,9,13,17 \}$. Consider first the case where $\sigma \in \{5,9,13,17\}$ but not $\tau$, then we have $\tilde{E}_{\sigma} = E_{\sigma} + l$ and $\tilde{E}_{\tau} = E_{\tau}$ and hence using the linearity of the Jacobi bracket
\begin{align}
    \llbracket \tilde{E}_{\sigma}, \tilde{E}_{\tau} \rrbracket = \llbracket E_{\sigma}, E_{\tau} \rrbracket - \sum_{i=1}^{\mathcal{k}} \frac{\partial E_{\tau}}{\partial l_i} l_i.
\end{align}
We can now use that the first contribution vanishes, $\llbracket E_{\sigma}, E_{\tau} \rrbracket = 0$ as this again essentially nothing different that the commutator of the two vector fields $[\zeta_{\sigma},\zeta_{\tau}]$ expressed in terms of the quantities $E_{\tau}$ and $E_{\sigma}$. We are thus left with the second term $- \sum_{i=1}^{\mathcal{k}} \frac{\partial E_{\tau}}{\partial l_i} l_i$, but as the equation $E_{\tau}$ is in particular linear in the $l_i$ this again yields $E_{\tau}$ and hence vanishes on $R_1$. Along the same lines one can proceed with the case of $E_{\tau}$ also containing extra contributions from $+l$. In total we thus find that also the inhomogeneous PDE is involutive which proofs the statement.
\end{proof}
%check this proof !!!

We can now apply the preveous developed techniques to obtain a power series solution to the PDE (\ref{DiffeoEqnFormal}). This then yields a finite order power series expansion of the requested gravitational Lagrangian of which we can of course compute the Euler Lagrange equations in the appropriate perturbative order. In other words we thus get a perturbative theory of gravity. We consider the result that the PDE (\ref{DiffeoEqnFormal}) is involutive to be essential for this approach as only now as we have proven that we can be sure the contribution to the perturbative Lagrangian that we compute up to some finite order will not change in any higher order of the power series procedure. Differently stated only with the PDE being involutive we can be certain to  really extract all the for the particular order relevant information from it and not end up with a solution that is too general. \\

Note that from the point of view of formal theory involution of a PDE is not only essential for the construction of power series solution. Previously we have already stated that one of the key ideas behind a PDE with involutive symbol lies in the fact that one is then able to predict the rank of the prolonged symbol by means of (\ref{sumBeta}) without actually having to compute the relevant prolongations. As for an involutive symbol according to (\ref{invoCons}) also all prolonged symbols are involutive one can thus obtain formulae that also allow for the prediction of the ranks of higher order prolongations of the symbol. Recall that the rank of the symbol $M_q$ essentially determines the number of expansion constants of order q the one might specify arbitrarily when constructing a power series solution to the PDE  and thereby the various ranks of the symbol govern the entire information regarding the solution space of the given PDE.
Thus the involution of the PDE (\ref{DiffeoEqnFormal}) allows us to formulate the following statement about its general solution for arbitrary field bundles $F$.
\begin{theorem}\label{GeneralSol}
Given any field bundle $F$ with fibre dimension $n$. The general solution to the invariance equation, i.e. the homogeneous version of (\ref{DiffeoEqnFormal}) admits the form:
\begin{align}
    \mathcal{F} \left (\Psi_1,...,\Psi_r \right ),
\end{align}
where $r:=\mathcal{k}-140$, $\Psi_1,...\Psi_r$ are $r$ funtionally independent solutions of the homogeneous PDE and $\mathcal{F}$ is an arbitrary function of these independent solutions. The general solution to the equivariance equation, i.e. the inhomogeneous PDE (\ref{DiffeoEqnFormal}) is given by:
\begin{align}
    \omega \cdot \mathcal{F} \left (\Psi_1,...,\Psi_r \right ),
\end{align}
where $\omega$ is any explicit solution of the PDE. 
\end{theorem}
\begin{proof}
The proof of the first statement is given by Proposition 7.1 in \cite{seiler1994analysis}. Further information can in particular be found in chapter 3 of \cite{seiler2009involution} and \cite{articleCH}. 

The second statement then simply follows from the previously stated fact that given an arbitrary solution $\mathcal{F}$ of the homogeneous PDE, multiplying by any solution $\omega$ of the inhomogeneous system yields again a solution of the inhomogeneous system and vice versa the quotient of any two solutions of the inhomogeneous PDE defines again a solution of the homogeneous system. The first part of this statement is simply a result of the linearity of the given PDE. For simplicity we consider the case of a single equation 
\begin{align}
    0=D_i l + l 
\end{align}
and the corresponding homogeneous version. The generalization to finitely many equations is then realy straight forward. 
We thus have $0 = D_i \mathcal{F}$ and also $0 = D_i \omega + \omega$.
We first show that $\omega \cdot \mathcal{F}$ then also solves the inhomogeneous system. Inserting $\omega \cdot \mathcal{F}$ into the inhomogeneous PDE we get 
\begin{align}
    0 = \left ( D_i \mathcal{F} \right ) \cdot \omega + \mathcal{F} \cdot \left ( D_i \omega \right) + \mathcal{F} \cdot \omega. 
\end{align}
As $\mathcal{F}$ is supposed to provide a solution of the homogeneous equation the first term vanishes. As $\omega$ solves the inhomogeneous equation the second term yields $\mathcal{F} \cdot \left ( - \omega \right )$,
and thus cancels the third term. We thus have shown that $\omega \cdot \mathcal{F}$ solves the inhomogeneous system. To complete the proof we still have to show that any solution of the inhomogeneious PDE is neccessary of this form. We proof this by first taking an arbitrary solution of the inhomogeneous system $\rho$. We now take any other solution $\omega$ and insert the quotient $\frac{\rho}{\omega}$ into the homogeneous PDE
\begin{align}
    0 = D_i \left (\frac{\rho}{\omega} \right) = - (\frac{\rho}{\omega}) - \frac{\rho}{\omega^2} \cdot (-\omega) ,
\end{align}
where we used $D_i \rho = -\rho$ and the same for $\omega$ as they both are supposed to solve the inhomogeneous system. Hence the quotient solves the homogeneous system. Obviously we can write the solution $\rho$ as 
\begin{align}
    \rho = \omega \cdot  \frac{\rho}{\omega}.
\end{align}
But as the first factor is taken to be a solution of the inhomogeneous system and we have now shown that the second factor solves the homogeneous PDE this is precisle the form that was claimed. 
Hence in total we have thus shown that any solution of the inhomogeneous PDE is of the form $\omega \cdot \mathcal{F}$.
The straight forward generalization of this to the case finitely many PDEs then completes the proof.
\end{proof}
It is important to observe that the previous theorem not only tells us the generall form any solution to the equivariance or invariance equations respectively will admit but also provides us with information regarding the number of functionally independent solutions to these systems. Concretely for any field bundle there will be 
\begin{align}
    \mathcal{k} - 140 = \mathrm{dim}(J^2F)
\end{align}
functionally independent solution. This is obviously something that one might intuitively guess as it is simply the difference between the number of independent variables in $J^2F$ and the number of equations in the PDE. However only now that we know the equations are involutive we can be sure that there is no further information hidden in them yielding possibly additional restrictions on the unknowns and thereby reducing the number of functionally independent solutions. 
Also note that these solutions now in general will correspond to Lagrangians that might generate 4-th derivative order EOM. 
In the context of GR these functionally independent solutions of the homogeneous equations are called \textit{\textbf{curvature invariants}}. In most cases they are constructed form the Riemann curvature tensor. Observe however that also her the Riemann curvature is strictly speaking not necessary to obey such a notion of curvature invariants. 
In GR it is a well known result that there exist 14 functionally independent curvature invariant, yet already for this rather simply case their concrete expressions is surprisingly difficult. Obtaining possible generating sets that allow one to express arbitrary other invariants in terms of them is still a topic of research. The curvature invariants and also possibly higher invariants involving higher derivatives of the metric tensor are then mostly used to classify spacetimes. Details can be found in
\cite{2009CQGra..26b5013C}, \cite{Zakhary1997}, \cite{2002IJMPD..11..827C} and also \cite{doi:10.1063/1.531425}.
Note that we can easily reproduce the known result of 14 curvature invariants for GR in our framework. The fibre dimension of the bundle of symmetric $(0,2)$ tensors $F_{GR}$ is obviously 10 yielding for this case 
\begin{align}
    \mathrm{dim}(J^2F_{GR}) = 4 + 10 + 40 + 100 = 154,
\end{align}
which according to the above considerations yields $154-140=14$ functionally invariant solutions. Beyond the standard case of GR we are know in a position where we can easily determine the number of curvature invariants for any other spacetime geometry. We will consider a particular example in the next chapter.\\ 

Before we concretely work out a general framework for obtaining perturbative solutions to the equivariance equations we concern ourselves for a moment with the question what points $p_0 \in J^2F$ might serve the purpose as expansion points for such a power series. From the shear mathematical point of view there is really no restriction on the possible expansion points. Yet from a physical point of view the specific expansion point will in the end determine the interpretation and foremost the range of validity of the thereby constructed perturbative theory of gravity. As we want to work towards a treatment of gravitational waves we are going to restrict ourselves to certain expansion points that describe a \textit{\textbf{flat}} variant of the theory of gravity at hand. Solutions of the obtained perturbative equations can then be thought of as corrections to that flat background theory with the special case of gravitational waves being wave like solutions that propagate on the given background. More precisely with a slight adaption of the usual meaning in Riemanian geometry\footnote{In Riemannian geoemtry one call a Riemannian manifold flat if the associated Riemannian curvature tensor vanishes everywhere. The existance of coordinates for which the derivatives of the metric are zero is then a consequence (see \cite{petersen2006riemannian}).} we call a section $G \in \Gamma(J^2F)$ \textit{\textbf{flat}} if there exist adapted coordinates on $J^2F$ s.t. the coordinate expression of $G$ satisfies $\partial_mG_{A}=0$.
Hence we chose an expansion point with adapted coordinates $(x_0^m,N_A,0,0)$. note that such a point is essentially already determined by a point in $F$. 

We are in particular interested in the case where the expansion point is not only flat but furthermore supports our intuition that the geometry of spacetime is in quite good approximation provided by the flat Minkowski metric $\eta_{ab} = \mathrm{diag}(-1,+1,+1,+1)$. 
%check signature !!!
%
%
In other words we want to interpret solutions of the to-be-constructed perturbative theory of gravity as corrections to a flat Minkowski background. Note we are not restricting our treatment to metric theories of gravity but deliberately want to allow for more general tensor fields as spacetime geometry.
The interpretation of doing perturbation theory around Minkowski spacetime will nevertheless become meaningful once the additional matter theory $\mathcal{S}_{mat}[\phi^{\tilde{B}},G_A)$ is provided. We then require from our flat expansion point that the matter theory obtained from this flat expansion background $\mathcal{S}_{mat}[\phi^{\tilde{B}},N_A)$ is equivalent to the counterpart that we get when placing the matter field $\phi^{\tilde{B}}$ on a Minkowski background. 
In most cases this is achieved by constructing $N_A = N_A(\eta_{ab})$ from the Minkowski metric. We call such expansion points in the following \textit{\textbf{$\boldsymbol{\eta}$-induced}}. Note that this clearly restricts the set of field bundles $F$ we can possibly treat to those that are compromised of tensors with an even total rank, as we simply cannot write down expressions with odd rank that are solely constructed from $\eta_{ab}$. This will not affect any further developements as in all examples we will treat, the gravitational field is described by an even rank tensor field. There are even certain arguments mainly stemming from QFT that the gravitational field must be of even rank in order to describe an attractive force (\cite{vecchiato2017variational}). \\

We start our development of a framework that will allow us to construct perturbative expansions of diffeomorphism invariant Lagrangians for arbitrary fields by writing down the general finite power series expansion of an arbitrary such Lagrangian up to some order $r > 0$ around a flat expansion point $p_0 \in J^2F$ with adapted coordinates $p_0 \equiv (x_0^m,N_A, 0, 0)$ . Note again that such a Lagrangian is already a bundle map on the second order jet bundle $J^2F$, the equivariance PDE (\ref{DiffeoEqnFormal}) is then constructed as submanifold of $J^1(J^2F \times \Lambda^4M)$. It is important to not confuse the two different ways the jetbundle construction is involved therein. In order to concisely display such a power series Lagrangian we need to agree on some further notation. We denote the adapted coordinates on $J^2F$ collectively by
\begin{align}
    (v_{AI_k}) := (x^m,v_A,v_{Ap},v_{AI}).
\end{align}
In order to distinguish the appearing derivative indices $I_k,J_k,...$ that label higher spacetime derivatives from those that label higher derivatives w.r.t. the coordinates of $J^2F$ we denote the latter ones by caligraphic letters
%add this to introduction !!!
$\mathcal{I}_k, \mathcal{J}_k$, ... . Finally we introduce the coordinate expression of the deviation from the expansion point 
\begin{align}
    (H_{AI_k}) := (v_{AI_k}) - (N_{AI_k}) = (x^m-x_0^m,v_A-N_A,v_{Ap},v_{AI}).
\end{align}

Before we write down the concrete expression for the expansion of the Lagrangian note that the first equation in (\ref{DiffeoEqnFormal}) simply states that the Lagrangian must not explicitly depend on $x^m$, hence we can discharge any explicit $x^m$ dependency from the very beginning and therefore also exclude such from the power series expansion. In total with the introduced notation a general power series expansion reads as follows. 
\begin{align} \label{generalPowerSL}
    \begin{aligned}
    &L_{per} \cdot  \mathrm{d}^4x = \mathcal{L}_{per} : J^2F \longrightarrow \Lambda^4M \\
    &L_{per} = \sum_{n=0}^r \sum_{k_1,...,k_n = 0}^2 a^{\mathcal{I}_k} \cdot J_{\mathcal{I}_k}^{A_1I_{k_1}...A_nI_{k_n}} H_{A_1I_{k_1}} \cdot ... \cdot H_{A_nI_{k_n}}
    \end{aligned}
\end{align}
%rewrite this ???
%
%
As before the $a^{\mathcal{I}_k} =: a^{A_1I_{k_1}...A_nI_{k_n}}$ are constants. 
In order to clarify this last expression further we now explicitly provide the first three orders of this expansion. For that particular case we drop any numerical factors due to the intertwiners as this simply corresponds to redefining the constants. Furthermore as before we are going to split the sums over the ${A_iI_{k_i}}$ into the different contributions $(v_A,v_{Ap},v_{AI})$:
\begin{multline}\label{LPert}
    L_{per} = a_0 + a^A H_A + a^{Ap} H_{Ap} + a^{AI}H_{AI} + a^{AB} H_{A}H_{B} + a^{ABp}H_A H_{Bp} + a^{ABI} H_{A} H_{BI}\\
    +a^{ApBI}H_{Ap} H_{BI} + a^{AIBJ} H_{AI}H_{BJ} + a^{ABC} H_a H_B H_C 
    + a^{ABCp} H_A H_B H_{Cp} \\
    +a^{ABCI} H_A H_B H_{CI} + a^{ABpCq} H_{A}H_{BP}H_{Cq} + a^{ABpCI} H_A H_{Bp} H_{CI}\\
    + a^{ABICJ} H_A H_{BI}H_{CJ} 
    + a^{ApBqCm} H_{Ap} H_{Bp} H_{Cm}+ a^{ApBq CI} H_{Ap} H_{BP} H_{CI}\\
    + a^{Ap BI CJ} H_{Ap} H_{BI} H_{CJ} + a^{AIBJCK} H_{AI} H_{BJ} H_{CK} + \mathcal{O}(4),
\end{multline}
where $\mathcal{O}(4)$ denotes terms that are of order 4 or higher in any $(H_{AI_k})$. We now proceed exactly as outlined before in calculating the perturbative solution to solution to (\ref{DiffeoEqn}) from the above power series ansatz of the Lagrangian. We start by plugging in the expansion (\ref{LPert}) into the PDE (\ref{DiffeoEqnFormal}) and evaluate at the expansion point $p_0$. Note that evaluating at $p_0$ in coordinates corresponds to evaluating at $(H_{AI_k})=0$. This then yields equations for the expansion constants $a_0, a^A, a^{Ap}$ and $a^{AI}$. To obtain equations for the remaining higher order expansion coefficients we simply prolong the PDE again insert the series expansion and evaluate at $p_0$. As we have shown that PDE (\ref{DiffeoEqnFormal}) is involutive there will not occur integrability conditions.\\

There is however one obstruction to this procedure that will become noticeable by providing additional lower order equations that are revealed only after prolongations, yet they have nothing to do with integrability conditions. 
The reason for this obstruction is that the chosen $\eta$-induced expansion point in general features higher symmetry than an arbitrary point in $J^2F$. More precisely due to the fact that we required $N_A(\eta_{ab})$ to be constructed from the Minkowski metric by means of products, sums and possibly contractions, it is \textit{\textbf{Lorentz invariant}}. To further investigate the consequences thereof we consider first an arbitrary flat expansion point $\tilde{p}_0 \in J^2F$ with coordinates $(\tilde{x}_0^m,M_A,0,0)$. Evaluating the second equation in (\ref{DiffeoEqnFormal}) for this point yields:
\begin{align}
    0 = l^A \vert _{\tilde{p}_0} C^{BM}_{An}M_B.
\end{align}
When evaluated at a general point these are 16 independent equations. 
%but why ? from the properties of the lie algebra morphism ?
Conversely evaluating the same equation at the $\eta$-induced expansion point we will find that 
\begin{align}\label{RankDef}
    0 = l^A \vert_{p_0} C^{BM}_{An}N_B
\end{align}
provides only 10 independent equations. The reason for this rank defekt and foremost its connection to the observed Lorentz invariance can be observed by considering the expression $K_{m[rs]}^n :=\eta_{m[r}\delta_{s]}^n$. 
Taking a closer look at these six expressions that can be obtained from the six possible values of the anti symmetric index pair  $\{K_{m[rs]}^n \ \vert \ r < s \} $ we find that they all are by construction anti symmetric w.r.t. the Minkowski metric, i.e. they satisfy 
\begin{align}
K_{m[rs]}^n\eta_{n p} - m \leftrightarrow p = 0.
\end{align}
Furthermore they are all linearly independent when identified as real $4 \times 4$ matrices. Considering the dimension of the vector space of $4 \times 4$ matrices that are anti symmetric w.r.t. the matrix expression of $\eta_{ab}$ we find that  the expressions $K_{m[rs]}^n$ even constitute a basis of the vector space of $\eta$-antisymmetric matrices. 

%rewrite all this ??
%wrong logic ??
%
%
One can show that the Lie algebra of the Lorentz group $SO(1,3)$ is isomorphic to the Lie algebra that is obtained by equipping this vector space of $\eta$-antisymmetric matrices with the standard matrix commutator. Hence these six matrices actually yield a basis of the Lie algebra $\mathrm{Lie}(SO(1,3))$. One often calls such matrices the \textit{\textbf{generators}} of the corresponding group as finite group elements can be obtained by exponentiating linear combinations of them. Much information regarding the Lorentz group can be found in \cite{doi:10.1142/p199} and \cite{naimark2014linear}.

It is now clear that it henceforth holds that
\begin{align}
    0 = C^{Bm}_{An}N_B K_{m[rs]}^n,
\end{align}
as this is nothing but the infinitesimal change of $N_A$ under Lorentz transformations, but as $N_A$ is sole constructed from $\eta_{ab}$ and hence Lorentz invariant this infinitesimal change obviously vanishes. As there exist six independent generators of the Lorentz group we can thus obtain six independent vanishing linear combinations of the 16 equations in (\ref{RankDef})
\begin{align}
    0 = a^A C^{BM}_{An}N_B K_{m[rs]}^n,
\end{align}
leaving us with 10 independent equations. At first sight it seems like due the the higher symmetry of the expansion points the equations are now weaker. This is however not true as the Lorentz invariance of the expansion point $N_A$ also influences the power series solution in higher order. To illustrate this effect we consider the prolongation of the second equation in (\ref{DiffeoEqnFormal}) w.r.t. $v_B$, i.e. we apply the total derivative $D_B$ to this equation. We get the second derivative order equation
\begin{multline}
    0 = l^AC_{An}^{Bm} + l^{AB}C_{An}^{Cm}v_C + l^{BAp} \bigl[ C_{An}^{Cm} \delta_p^q - \delta_A^C \delta_m^n \bigr] v_{Cq}\\
    + l^{BAI} \bigl[ C_{An}^{Cm} \delta_I^J - 2 \delta_A^C J_I^{pm} I^J_{pn}  \bigr] v_{CJ} + l^{B} \delta^m_n.
\end{multline}
When evaluating at $p_0$ only terms that have no $v_{Ap}$ or $v_{AI}$ contribute and we are left with 
\begin{align}\label{prolongE}
    0 = a^A C_{An}^{Bm} + a^{AB} C_{An}^{Cm} N_C +  a^B \delta^m_n.
\end{align}
If we had chosen a different prolongation, i.e. $D_{Bp}$ or $D_{BI}$ we would have got a similar expression. All these prolongations of the second equation of (\ref{DiffeoEqnFormal}) have in common that the only second derivative order contribution is proportional to $C^{Cm}_{An} N_C$ and hence all allow for the following construction. We simply take the whole prolonged equations (\ref{prolongE}) and contract it with the Lorentz generators $K_{m[rs]}^n$. Note that such a contraction is equivalent to a certain linear combination of equations of the prolonged equations. Then obviously the second derivative order contribution vanishes, as already $C_{An}^{Bm} N_B K_{m[rs]}^n = 0$.  Therefore the resulting equation is now again of first derivative order . Furthermore one readily finds that the contribution from $a^B \delta^m_n K_{m[rs]}^n$ vanishes. Hence we are left with 
\begin{align}\label{ansatz1}
    0 = a^A C^{Bm}_{An}  K_{m[rs]}^n.
\end{align}
Note that we can obtain similar equations from arbitrary other prolongations of the second equation in (\ref{DiffeoEqnFormal}), in particular also for higher order prolongations, we will always obtain equations similar to (\ref{ansatz1}). Ultimately comparing the newly obtained first derivative order equation with the condition for $N_A$ to be Lorentz invariant also the meaning of the above equation (\ref{ansatz1}) becomes clear: the equation simply states that the expansion constants $a^A$ must be the components of a Lorentz invariant tensor. With these new first order equations at hand we therefore see that the Lorentz invariance of the expansion point $N_A$ in fact does not yield weaker but stronger equations than a general expansion point, with the only obstruction lying in the fact the additional first order equations could only be obtained after a prolongation and subsequent evaluation. Note that we could carry through a similar construction for any expansion point $M_A$ that is invariant under any arbitrary subgroup of the infinitesimal $GL(4)$ transformations that are locally induced by the lifted vector fields $\xi_F$. \\

Before we proceed with a further investigation of these additional first order equations we want to reinforce the remark that these in fact have nothing in common with the integrability conditions discussed in the previous sections. Whereas integrability conditions really yield new independent partial differential equations, that in particular can be prolonged again and thereby can themselves contribute to further integrability conditions in the situation above we only obtain lower order equations once we evaluate the whole equation at a certain point. Such already evaluated equations can obviously not be prolonged. The additional equations are hence not a feature of the PDE itself but of the expansion point. Similar effects can actually already occur during the treatment of a single ordinary differential equations, in short ODE, where the construction of true integrability conditions is clearly not possible. We illustrate the above arguments by considering the following single ODE
\begin{align}
    0 = x \cdot f^{\prime}(x) - 2x^2.
\end{align}
The general solution is given by $f(x) = x^2 +c$ where $c=const$. We construct a finite power series solution up to some $k \geq 0$ around $x_0 = 0$. Therefore we insert the following ansatz into the ODE:
\begin{align}
    f_{per} = \sum_{i=0}^k a_i x^i.
\end{align}
We evaluate the equation at $x_0=0$ to find in the first order the trivial equation $0=0$. Note that the the ODE is of first derivative order. Hence in general we would expect to obtain equations for $a_1$ from inserting the power series ansatz. We proceed by prolonging the ODE to obtain the second derivative order equation
\begin{align}
    0 = f^{\prime}(x) + x \cdot f^{\prime \prime}(x) - 4 x.
\end{align}
Inserting the series ansatz and evaluating at $x_0=0$ now yields the equation $a_1=0$. Although the prolonged ODE is of second derivative order, after evaluating it at the expansion point we now obtained an equation for the first order expansion coefficient. Proceeding with the next orders we will find $a_2 = 1$ and $a_i = 0$ for $i < 2$. Hence the power series solution reads $f_{per} = a_0 + x^2$, which obviously is correct solution. Nevertheless during each  step of the construction of this power series solution we had to prolong the ODE one order further than it is usually required for the given order of the expansion constants. This was necessary due to the fact that although the ODE was of first derivative order, at the expansion point $x_0=0$ the only term that contains first order derivatives vanishes. If we had constructed a power sereis solution around any other point $x_0 \neq 0$ this would not have happened. Note that this vanishing of the highest derivative order at the expansion point is in one to one correspondence with the rank defects that were featured by (\ref{DiffeoEqnFormal}) when evaluated at the expansion point provided by $N_A$. \\

Returning now to our original problem the construction of power series solutions to (\ref{DiffeoEqnFormal}) we thus can proceed as usual, by successively prolonging the PDE and inserting the series expansion, we only have to keep in mind that in order to obtain the general solution up to a given order we actually have to prolong to one order higher in order to take the additional lower order equations stemming from the rank defect at $N_A$ into account.  At first sight this might sound like a huge disadvantage as with each prolongation order the PDE gets increasingly magnified in its size. We are however in the fortunate situation that we can predict the precise form of the additionally occurring lower order equations in arbitrary order. All these equations simply encode the Lorentz invariance of the various expansion constants. In particular note that by taking appropriate prolongations of the second equation in (\ref{DiffeoEqnFormal}) we can really construct such lower order equations that govern the Lorentz invariance for all expansion constants in the general power series expansion of the Lagrangian. 
Considering this we can severely reduce the dimensions of any linear equation systems that may arise during the construction of such power series solutions by not taking the expansion constants $a^{\mathcal{I}_k} \equiv a_0, a^A, a^{Ap},...$ as arbitrary constants and then solve the appropriate newly obtained lower order equations to ensure that they describe the components of Lorentz invariant tensors, but by including from the very beginning only such expansion coefficients in the power series that are Lorentz invariant.\\

As we are dealing with linear equations it suffices to construct a basis of the appropriate space of Lorentz invariant constant tensors of the corresponding valence and symmetry. This can be achieved by making use of some well known results from classical invarinat theory, more precisely the so called \textit{\textbf{first fundamental theorem}} (see \cite{Aslaksen1995InvariantTO} and also \cite{PROCESI1976306}). Roughly speaking and restricting attention to the special orthogonal groups $SO(n)$ it states all objects that are invariant under the $(m,n)$ tensor representation, i.e. the tensor product of $m$ copies of the fundamental $SO(n)$ representation and $n$ copies of its dual, for $m \neq n$ can be obtained by forming expressions that solely involve the $SO(n)$ invariant metric\footnote{Which is given by $\eta_{ab}$ for the special case of the Lorentz group $SO(1,3)$.}, the $n$-dimensional Levi-Civita symbol $\epsilon_{a1...an}$ and the corresponding contravariant objects. Applying this to the special case of the Lorentz group $SO(1,3)$ at hand we find that for a given kind of expansion coefficient, for instance $a^{AB}$ we can obtain a basis of Lorentz invariant expressions by first writing down the most general expression for $a^{AB}$ that can be constructed from the invariant metric $\eta_{ab}$, its inverse $\eta^{ab}$, $\epsilon_{abcd}$ and $\epsilon^{abcd}$ that is consistent with the symmetries and the index structure of $a^{AB}$. To that end it is best to transfrom $a^{AB}$ back to the form involving spacetime indices for instance $a^{abcdefgh} := a^{AB}J_A^{abcd}J_B^{efgh}$, if the gravitational field is described by a rank $(0,4)$ tensor field. It is then straight forward to reduce the most general expression obtained in this fashion to a basis by removing linear dependencies.

Details regarding how one needs to proceed step by step to achieve the above are provided when we discuss particular examples. As this endeavour is straight forward but extensively laborious it is best done relying on computer algebra. For specifically that purpose we developed a highly performant computer program. 
%cite own computer program here
Further information regarding the underlying mathematics but also the concrete implementation and a short how-to-use guide can be found in chapter \ref{computerAlg}. \\

Finally we would like to emphasise the practical advantage of this approach. When computing power series solutions to (\ref{DiffeoEqnFormal}) the arising linear equations rapidly increas in size. Assuming $F$ has fibre dimension $21$ to provide an example that we will in fact treat in the next chapter the PDE compromises of $136$ equations for a function of $315$ independent variables (when the explicit $x^m$ dependency is discharged from the very beginning and hence the first equation in (\ref{DiffeoEqnFormal}) removed.).  When inserting the power series ansatz into the prolonged PDE obviously some of the obtained linear equations for the expansion constants decouple into sub systems that can be solved independently. Nevertheless due to the shear dimensionality of the expansion constants even solving the obtained subsystems poses a real problem. In our example for instance we would encounter the expression $a^{AIBJCK}$ which no includes $210\cdot 211\cdot212/6=1565620$ constants. Hence even if the obtained linear system decouples in such a way that allows us to treat all expressions that solely involve $a^{AIBJCK}$ independently from the rest we still have to solve a linear system with roughly $1.5$ Mio constants. On the other hand the space of Lorentz invariant expressions that we can obtain for $a^{AIBJCK}$ only has a dimension of several hundred. Hence working with the Lorentz invariant expression from the very beginning reduces the size of the problem from roughly $1.5$ Mio involved constants to several hundreds. 

Furthermore note that we cannot construct expression with an odd total number of spacetime indices from $\eta_{ab}$ and $\epsilon_{abcd}$. As we assumed the gravitational field to be of even rank we can further reduce the power series expansion by discharging all terms that contain an odd number of spacetime derivative indices. In addition to that from the expansion (\ref{LPert}) one can readily compute the contribution the individual terms yield in the EOM.  As we required these to be of second derivative order such the the associated Hamiltonian formulation is free of instabilities we can drop additional terms in the power series. After all we find that the following general expansion remains: 
\begin{multline}\label{LperRed}
     L_{per} = a_0 + a^A H_A + a^{AI}H_{AI} + a^{AB} H_{A}H_{B} + a^{ApBq} H_{Ap}H_{Bq} + a^{ABI} H_{A} H_{BI} \\
    + a^{ABC} H_a H_B H_C + a^{ABpCq} H_{A}H_{Bp}H_{Cq} +
    + a^{ABCI} H_A H_B H_{CI} 
    + \mathcal{O}(4).
\end{multline}
Inserting this in the last 3 equations of the PDE (\ref{DiffeoEqnFormal}) and evaluating at the flat expansion point yields:
\begin{align}\label{order1}
    \begin{aligned}
    &0 = a^A C_{An}^{Bm}N_B + a_0 \delta^m_n\\
    &0 = a^{AI}C_{An}^{B(m\vert }N_B J^{\vert pq)}_I.
    \end{aligned}
\end{align}
doing the same for the prolonged PDE we find 
\begin{align}\label{order2}
    \begin{aligned}
    &0 = a^A C_{An}^{Bm} + 2 a^{AB}C_{An}^{Cm}N_C + a^B\delta^m_n\\
    &0 = a^{AI}\left [C_{An}^{Bm}\delta^I _J- 2 \delta^A_B J_I^{pm}I^J_{pn} \right ] + a^{ABJ}C_{An}^{Cm}N_C + a^{BJ} \delta^m_n \\
    &0 = 2a^{A(p\vert Bq}C_{An}^{C\vert m)}N_C + a^{AI} \left [C_{An}^{B(m\vert} 2 J_{I}^{\vert p)q} - \delta_A^BJ_I^{pm}\delta^q_n \right ]\\
    &0 = a^{BAI}C_{An}^{C(m\vert}N_CJ_I^{\vert pq)} + a^{AI}C_{An}^{B(m \vert} J_I^{\vert pq)}.
    \end{aligned}
\end{align}
Finally prolonging the PDE to third derivative order inserting the power series ansatz and evaluating at $p_0$ yields
\begin{align}\label{order3}
\begin{aligned}
&0 = 2 a^{AC}C_{An}^{Bm} + 2a^{AB}C_{An}^{Cm} + 6 a^{ABC}C_{An}^{Dm} N_D + 2a^{BC} \delta^m_n\\
&0 = 2 a^{BqCr} \left [ C_{An}^{Bm} \delta ^q_p - \delta^B_A \delta^m_n \right ] +2 a^{A Bq Cr} C_{An}^{Dm} N_D + 2 a^{BqCr} \delta^m_n\\
&0 = a^{CAI} \left [C_{An}^{Bm}\delta^I _J- 2 \delta^A_B J_I^{pm}I^J_{pn} \right ] + 2 a^{ACBJ} C_{An}^{Dm} N_D + a^{CBJ} \delta ^m _n \\
&0 = 2 a^{C A(p \vert B q} C_{An}^{D \vert m )} N_D + a^{CAI} \left [C_{An}^{B(m\vert} 2 J_{I}^{\vert p)q} - \delta_A^BJ_I^{pm}\delta^q_n \right ]\\
&0 = 2 a^{BCAI}C_{An}^{D(m\vert}N_DJ_I^{\vert pq)} + a^{CAI}C_{An}^{B(m \vert} J_I^{\vert pq)}.
\end{aligned}
\end{align}
%go one order further ??
Together with the requirement that the expansion constants are Lorentz invariant these equations really contain all information that we can extract from the PDE (\ref{DiffeoEqnFormal}) for the construction of the power sereis Lagrangian. In particular as we have shown that the PDE is involutive we are now sure that we do not miss hidden information.  By means of this perturbative power series solution the requirement of constructing diffeomorphism invariant Lagrangians for any given field theory hence boils down to the much simpler quest of solving the above system of linear equations.
Note that we could no easily derive similar expression for the higher order contributions to such a power series expansion. Is the resulting linear systems then however become extensively complicated and in most cases practically unsolvable we only provide the linear systems contributing to the first three orders of the power series Lagrangian here. 

Besides these equations take precisely this form no matter what specific field is in consideration. The only quantities in the above linear equations (\ref{order1}), (\ref{order2}) and (\ref{order3}) that explicitly depend on the chosen field bundle are the constant tensors $C_{An}^{Bm}$ that are easily obtained from the infinitesimal diffeomorphism action on the field bundle $F$.  For that reason we have also cast the treatment of such equations into a computer program. Once we have specified the particular field bundle that we wish to work on and the expansion point we therefore only need to set up the computer with the expression for $C_{An}^{Bm}$ and the appropriate ranges for the fibre indices of the field bundle $A$ as initial input, the construction of the appropriate Lorentz invariant expansion coefficients and the subsequent solution of the above linear equations is then treated fully automatically. Details can again be found in chapter \ref{computerAlg}. \\

We end this chapter by examining how we can incorporate the requirement of causal compatibility of matter and gravitational equations in the perturbative approach.
We assume again that we are handed a matter theory 
\begin{align}
    \mathcal{L}_{mat} : F_{grav} \times J^1F_{mat} \longrightarrow \Lambda^4M,
\end{align}
that exploits the gravitational field as geometric background. Along the previously prescribed lines we can now construct a perturbative expansion of the requested gravitational Lagrangian
\begin{align}
    \mathcal{L}_{grav,per} : J^2F_{grav} \longrightarrow \Lambda^4M,
\end{align}
and implement the required diffeomorphism invariance by solving the linear systems for the expansion coefficients that are obtained from inserting the power series around $p_0 \in J^2F_{grav}$ into (\ref{DiffeoEqnFormal}). 
Following along the lines of the construction recipe (\ref{Algo1}) we would now have to compute the matter and gravitational principal polynomials.
These obviously must no also be computed perturbatively. 
To lighten the notation we will drop the explicit $k$ dependency in the following and simply write $\mathcal{P}_{mat}$ instead of $\mathcal{P}_{mat}(k)$ and analogously for the gravitational principal polynomial. It is straight forward how we can expand $\mathcal{P}_{mat}$ in coordinates around the chosen expansion point $p_0$ up to some arbitrary order, as we already know the full expression $\mathcal{P}_{mat}$. We get the following expansion of $\mathcal{P}_{mat}$ up to and including the quadratic order in $H_A$:
\begin{align}
    \mathcal{P}_{mat}(v_A) = (P_{mat})_{0} + (P_{mat})^A_1 H_A+ (P_{mat})^{AB}_2 H_A H_B +\mathcal{O}(3),
\end{align}
where $(P_{mat})_0 = \mathcal{P}_{mat}(N_A)$ and $(P_{mat})_1^A = \partial^A \mathcal{P}_{mat} \vert _{N_A}$. Of course one readily computes similar expression for expansion up to higher order. 
Finding the expansion of the gravitational principal Polynomial requires somewhat more work as we do not have access to the full expression for $\mathcal{P}_{grav}$. All we can provide is the perturbative expansion of the Lagrangian. From this we can compute perturbative EOM and also a perturbative expression for the principal symbol of the EOM.
Note that when expanding the Lagrangian up to order $k$ from applying the variational derivative $\frac{\partial\mathcal L_{grav,per}}{\partial v_A}$ we will get the corresponding EOM up to order $k-1$. They are then necessarily quasi linear and by requirement of second derivative order. According to definition (\ref{PSym}) we can obtain the principal symbol by deriving the EOM w.r.t. the highest occurring derivative  --- as the EOM are linear in the highest derivative this simply admits to taking the coefficients in front of $v_{AI}$ --- and then contracting with $J_I^{pq} k_p, k_q$, for $k_p$ being the components of some one form on the base manifold $M$. 
The explicit formula for the EOM corresponding to the expansion (\ref{LperRed}) can be computed as:
\begin{align}\label{EOMPert}
    \begin{aligned}
    E_{per}^A = \frac{\partial L_{per}}{\partial v_A} &= a^A + 2 a^{AB}H_B + 3a^{ABC}H_B H_C \\
    &\hphantom{=}+ \left [a^{ABI} + a^{BAI} - 2 a^{ApBq}I_{pq}^I  \right ] H_{BI} \\
    &\hphantom{=}+ \left[a^{ABpCq} -2a^{BApCq} +2a^{BCAI} J_I^{pq} \right]H_{Bp}H_{Cq} \\
    &\hphantom{=}+ \left [2a^{ACBI} -2a^{CApBq}I_{pq}^I + 2a^{BCAI} \right ]H_C H_{BI} \\
    &\hphantom{=}+ \mathcal{O}(3),
    \end{aligned}
\end{align}
where we used the symmetries of the expansion coefficients that are enforced by the way they are contracted in (\ref{LperRed}).
In particular observe that the thus obtained principal symbol is now given up to order $k-2$. Therefore we can only compute the principal polynomial up to order $k-2$. In the following we denote the thereby obtained expansion of the principal symbol by 
\begin{align}
    T(v_A) = T_0 + T_1^CH_C + T_2^{CD}H_CH_D + \mathcal{O}(3).
\end{align}
Reading of the values in constant and linear order from the expansion of the EOM we find:
\begin{align}
    \begin{aligned}
    T_0^{AB} &= \left [a^{ABI} + a^{BAI} - 2 a^{ApBq}I_{pq}^I  \right ]\\
    \\
    (T_0^{AB})^C &= \left [2a^{ACBI} -2a^{CApBq}I_{pq}^I + 2a^{BCAI} \right ]. 
    \end{aligned}
\end{align}
Note that the principal symbol and thus also the principal polynomial could in general also depend on the coordinates $v_{Ap}$ as the EOM would then be still of second derivative order. We could then immediately exclude such contributions containing derivative coordinates $v_{Ap}$ from our discussion as the matter polynomial cannot contain such and in the end we want to require that the two polynomial describe at each $p \in M$ the same vanishing set. Hence from now only we assume that the gravitational principal symbol $T(v_A)$ only depends on $v_A$. If this is not the case for a concrete gravitational Lagrangian the above obviously yields a further condition, namely restricting to the sub theory that does satisfy this.  \\

Recall that before in the exact setting we obtained the principal polynomial from the principal symbol by computing any non vanishing order 4 sub determinant that was obtained by removing rows $(A_1...A_4)$ and columns $(B_1...B_4)$ from the symbol, and then dividing by the expression by the appropriate prefactor (\ref{prefacF})
Note that also the expression $\chi_{An}(v_A) = C_{An}^{Bm}v_Bk_m$ is linear in $v_A$ and hence also the prefactor contributes in different orders. We expand $\chi_{An}$ around $p_0$ to obtain
\begin{align}
\chi_{An}(v_A) =  C^{Bm}_{An} N_B k_m + C^{Bm}_{An} H_B k_m =: (\chi_0)_{An} + (\chi_1)^B_{An}H_B
\end{align}
Inserting this into (\ref{prefacF}) we denote the thereby induced expansion by
\begin{multline}\label{prefacExp}
    f_{(A_1...A_4)(B_1...B_4)}(v_A) = (f_0)_{(A_1...A_4)(B_1...B_4)} + (f_1)^C_{(A_1...A_4)(B_1...B_4)}H_C\\ + (f_2)^{CD}_{(A_1...A_4)(B_1...B_4)}H_CH_D
    + \mathcal{O}(3).
\end{multline}
Recall now that the determinant of a $(n-4) \times (n-4)$ sub matrix of the expansion of the principal symbol with entries being m-th order expressions in $H_A$ will be of order $m\cdot(n-4)$. This would in general pose no problem as we could simply compute the closed form expression of the determinant of this sub matrix end then again expand the result up to the required order in $H_A$. This method however will generate technical problems. After solving the perturbative equivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}) the Lagrangian and therefore also the principal symbol will contain undetermined constants. Hence the entries of the principal symbol are not only expressions that are linear in $H_A$ but also contain additional constants. Using standard computer algebra to calculate an algebraic expression for the determinant of such a matrix that contains symbolic entries one usually reaches a limit of either the available memory or the required computation time once the dimension of the matrix exceeds $15 \times 15$ , with the precise limit obviously depending on the used machine, the precise form of the matrix and the specific algorithms at use. 

Note that all this complications only arise once we try to compute the closed form of the determinant. This is not even what we try to achieve as we are only interested in the expansion of the determinant up to some order. Hence we can at least partly avoid such technical problems by directly expanding the determinant. Recall that when interpreted as a map on the column vectors that specify a given matrix the determinant is multilinear. Therefore we can simply expand it (see for instance \cite{2008CoTPh..49..801Z} and also the following nice collection of matrix formulae \cite{IMM2012-03274}), with an expansion around the identity matrix taking the particularly simple form 
\begin{align}\label{detExp}
    \mathrm{det}(I+M) = 1 + \mathrm{Tr}(M) + \frac{(\mathrm{Tr}(M))^2- \mathrm{Tr}(M^2)}{2} + \mathcal{O}(3). 
\end{align}
As we required the chosen $(n-4)\times (n-4)$ sub matrix of the principal symbol\footnote{Obviously concretely finding such a sub matrix involves some trial and error. Nevertheless as here we are only interested in the distinction of whether the determinant is zero or not the involved computations do not yield technical problems. This is true as we can simply evaluate all symbolic entries of the matrix at some random integers and compute its rank using for instance a fraction free implementation of Gaussian elimination. The thus computed rank always provides a lower bound for the true symbolic rank as in the worst case by randomly evaluating the symbolic entries we generate additional linear dependencies of the rows or columns of the matrix. Hence when the randomly evaluated matrix has full rank we know that the symbolic matrix must have non zero determinant. This is a huge advantage over computing the rank symbolically as now the computation only involves integer arithmetic which is not only much more efficient than performing symbolic computations but also remarkably stable, as no round offs are performed.} to have non vanishing determinant, the constant order of this sub matrix will be invertible. We denote the chosen sub matrix and the corresponding expansion by 
\begin{multline}
    T_{(A_1...A_4)(B_1...B_4)}(v_A) = (T_0)_{(A_1...A_4)(B_1...B_4)} + (T_1)_{(A_1...A_4)(B_1...B_4)}^{C} H_C \\
    +(T_2)_{(A_1...A_4)(B_1...B_4)}^{CD} H_C H_D + \mathcal{O}(3),
\end{multline}
where the symmetric 4-tuples $(A_1...A_4)$ and $(B_1...B_4)$ denote the removed rows and columns respectively. The inverse of the constant order is then given by $(T_0)^{-1}_{(A_1...A_4)(B_1...B_4)}$. We can now compute the determinant of this sub matrix as follows:
\begin{multline}
    \mathrm{det}\left(T_{(A_1...A_4)(B_1...B_4)}(v_A)\right)
    = \mathrm{det}\left((T_0)_{(A_1...A_4)(B_1...B_4)}\right)\\ 
    \cdot \mathrm{det}\left (I +(T_0)^{-1}_{(A_1...A_4)(B_1...B_4)}
    \cdot \left (  (T_1)_{(A_1...A_4)(B_1...B_4)}^{C} H_C+(T_2)_{(A_1...A_4)(B_1...B_4)}^{CD} H_C H_D \right )  \right ) \\
    + \mathcal{O}(3)  
\end{multline}
Using now the expansion of the determinant around the identity matrix for the second factor in the above expression and the linearity and cyclicity of the trace we
find the following expansion for this order 4 sub determinant 
\begin{multline}
    \mathrm{det}\left(T_{(A_1...A_4)(B_1...B_4)}(v_A)\right) = (D_0)_{(A_1...A_4)(B_1...B_4)} + (D_1)^C_{(A_1...A_4)(B_1...B_4)}H_C\\
    +(D_2)^{CD}_{(A_1...A_4)(B_1...B_4)}H_CH_D
    + \mathcal{O}(3),
\end{multline}
with the contributions in the individual orders being given as 
\begin{align}\label{polyMatrices}
\begin{aligned}
  (D_0)_{(A_1...A_4)(B_1...B_4)} &=  \mathrm{det}\left((T_0)_{(A_1...A_4)(B_1...B_4)}\right) \\
  \\
  (D_1)^C_{(A_1...A_4)(B_1...B_4)} &= \mathrm{det}\left((T_0)_{(A_1...A_4)(B_1...B_4)}\right) \cdot \mathrm{Tr} \left ( (T_0)^{-1}_{(A_1...A_4)(B_1...B_4)}
    \cdot (T_1)_{(A_1...A_4)(B_1...B_4)}^{C} \right) \\
    \\
    (D_2)^{CD}_{(A_1...A_4)(B_1...B_4)} &= \mathrm{det}\left((T_0)_{(A_1...A_4)(B_1...B_4)}\right)
     \cdot \Bigl [ \mathrm{Tr} \left ( (T_0)^{-1}_{(A_1...A_4)(B_1...B_4)}
    \cdot (T_2)_{(A_1...A_4)(B_1...B_4)}^{CD} \right ) \\
     &\hphantom{=}
    + \frac{1}{2} \cdot \Bigl \{ \mathrm{Tr}\left ( (T_0)^{-1}_{(A_1...A_4)(B_1...B_4)} \cdot (T_1)_{(A_1...A_4)(B_1...B_4)}^{C} \right )\\
     &\hphantom{=} \cdot \mathrm{Tr}\left ( (T_0)^{-1}_{(A_1...A_4)(B_1...B_4)} \cdot (T_1)_{(A_1...A_4)(B_1...B_4)}^{D} \right )  \\
      &\hphantom{=} 
    - \mathrm{Tr}\Bigl  (((T_0)^{-1}_{(A_1...A_4)(B_1...B_4)})^2 \cdot (T_1)_{(A_1...A_4)(B_1...B_4)}^{C} \cdot (T_1)_{(A_1...A_4)(B_1...B_4)}^{D}  \Bigr )    \Bigr \} \Bigr ]
    \end{aligned}
\end{align}
On the other hand using (\ref{diffeoMinor}) we can express the different order contributions to the determinant of such a submatrix by means of an expansion of the gravitational principal polynomial
\begin{align}
    \mathcal{P}_{grav}(v_A) = (P_{grav})_{0} + (P_{grav})^A_1 H_A+ (P_{grav})^{AB}_2 H_A H_B +\mathcal{O}(3).
\end{align}
Together with the expansion of the prefactor (\ref{prefacExp}) we thus get from (\ref{diffeoMinor}) the following contributions in the different orders
\begin{align}\label{minorPoly}
    \begin{aligned}
    (D_0)_{(A_1...A_4)(B_1...B_4)}  &= (f_0)_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_0 \\
    \\
    (D_1)^C_{(A_1...A_4)(B_1...B_4)}  &= (f_0)_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})^C_1 + (f_1)^C_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_0  \\
    \\
    (D_2)^{CD}_{(A_1...A_4)(B_1...B_4)}  &=  (f_0)_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_2^{CD} \\
     & \hphantom{=} +
     (f_1)^C_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_1^D +(f_2)^{CD}_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_0 
    \end{aligned}
\end{align}
Hence once we know the removed rows and columns $(A_1...A_4)$ and $(B_1...B_4)$ we can compute the constant, linear and quadratic order contributions to $f_{(A_1...A_4)(B_1...B_4)}(v_A)$, and the constant, linear and quadratic order contributions to the determinant of the sub matrix and from these obtain the constant order of the gravitational principal polynomial as:
\begin{align}\label{POLY1}
(P_{grav})_0 = \frac{(D_0)_{(A_1...A_4)(B_1...B_4)}}{(f_0)_{(A_1...A_4)(B_1...B_4)}}.
\end{align}
Using now the constant order principal polynomial that we thus obtain we get the linear order by
\begin{align}\label{POLY2}
    (P_{grav})^C_1= \frac{(D_1)^C_{(A_1...A_4)(B_1...B_4)} - (f_1)^C_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_0}{(f_0)_{(A_1...A_4)(B_1...B_4)}}.
\end{align}
And finally from this expression for the linear order we can obtain the quadratic order of the gravitational principal polynomial by 
\begin{multline}\label{POLY3}
    (P_{grav})_2 = \\
    \frac{(D_2)^{CD}_{(A_1...A_4)(B_1...B_4)}-\left [ (f_1)^C_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})^D_1  +(f_2)^{CD}_{(A_1...A_4)(B_1...B_4)} \cdot (P_{grav})_0 \right ]}{(f_0)_{(A_1...A_4)(B_1...B_4)}}.
\end{multline}
In total proceeding as outlined above we there obtain the expansion of the gravitational principal polynomial from the power series expansion of the Lagrangian.
Note that we could no proceed along the same lines to obtain the corresponding contributions to the principal polynomial in higher order. This is in principal straight forward with the only exception being that the involved expressions get more involved. For that reason we did not provide these here. When tackling practical problems in most constructing the perturbative Lagrangian beyond fourth order is computationally intractable and hence providing the formulae for a computation of the principal polynomial up to second order almost always suffices.\\
%do this more precise
Before we proceed further we would like to quickly remark on some consequences of the Lorentz invariance of the chosen expansion point $p_0$ on the possible values that we can obtain for the contributions to the gravitational principal polynomial in the individual orders. Recall that for second derivative order diffeomorphism invariant theories the the principal polynomial is a homogeneous polynomial in the components of some 1-form $k_m$ on $M$ in degree $r := 2n-16$. We hence can write it as 
\begin{align}
    \mathcal{P}_{grav} = \mathcal{P}_{grav}^{{p_1}...{p_{r}}} k_{p_1} \cdot ... \cdot k_{p_r}.
\end{align}
Where $\mathcal{P}_{grav}^{{p_1}...{p_r}}$ is are function on $F$ that is totally symmetric in $(p_1...p_r)$. As a consequence of the required equivariance of the Lagrangian it holds that:
\begin{align}\label{polyEqn}
    0 = \partial^A\mathcal{P}_{grav}^{{p_1}...{p_r}}C_{An}^{Bm}v_B - r \cdot \mathcal{P}_{grav}^{({p_1}...\vert m} \delta_{n}^{\vert p_r) }  + (r/2) \cdot \mathcal{P}_{grav}^{{p_1}...{p_r}} \delta^m_n.
\end{align}
%check sign !!!
%
%
%
This can be seen by starting from the PDE that the EOM have to satisfy as an implication of the equivariance (\ref{EOM}) and applying the total derivative $D_{AI}$ to obtain a similar PDE for sub expression $\left ( \frac{\partial E^A}{\partial V_{BI}} \right )$ that is involved in the principal symbol. Proceeding along the same lines for the remaining steps that are involved in the computation of the principal polynomial one then finds the above PDE. \\

The numerical factor $r/2 = (n-8)$ in front of the $\delta^m_n$ term is obtained as this factor encodes the density weight of the function at hand. In this language PDE (\ref{EOM}) then states that the EOM $E^A$ defines a density of weight 1 and further prolonging this PDE w.r.t. $v_{AI}$ reveals that also the principal symbol has density weight 1. Computing the determinant of a $(n-4) \times (n-4)$ sub matrix of the principal symbol changes the weight to $n-4-2$. This can for instance be seen when expressing the determinant of the principal symbol w.r.t. the Levi-Civita symbols and then obtain the order $4$ sub determinant by deriving $4$ times w.r.t. the components of the principal symbol as explained in (\ref{MinorDef}). To achieve this  we need $n$ copies of the principal symbol each of which has weight 1 and two covariant Levi-Civita symbols of dimension $n$ that carry density weight $-1$. This yields a total weight of $n-2$. Deriving $4$ times w.r.t. the components of the principal symbol reduces the factor to $n-6$. Finally according to (\ref{diffeoMinor}) this sub determinant equals the product between principal symbol and the corresponding prefactor. This prefactor involves two $4$-dimensional contravariant Levi-Civita symbols that hence contribute $+2$ to the weight. Hence the weight of the principal polynomial is given by $n-8$ which thus explains the origin of the numerical factor in the above PDE. \\ 

The all important observation is now that any perturbative expansion of the principal symbol around the Lorentz-invariant expansion point $p_0$ that we might obtain necessarily provides a power series solution of the form
\begin{align}
    \mathcal{P}_{grav}^{{p_1}...{p_{r}}} = (P_{grav})^{{p_1}...{p_{r}}}_0 + (P_{grav})_1^{C{p_1}...{p_{r}}} H_C + \mathcal{O}(2)
\end{align}
to the above PDE ($\ref{polyEqn}$). Therefore we can apply the same prolongation trick as before to deduce that the expansion coefficients $(P_{grav})_0, (P_{grav})_1^C,...$ must by given as components of constant Lorentz invariant tensors. Hence as before we can try to construct them by means of $\eta_{ab}$, $\epsilon_{abcd}$ and the corresponding contravariant expressions. As now however the expansion coefficients are totally symmetric in the indices $p_1...p_r$ the requirement of Lorentz invariance is much stronger than in the general case. The total symmetry for instance hugely restricts possible ways $\epsilon_{abcd}$ can contribute to the expansion coefficients. In particular for any theory at hand $(P_{grav})_0^{{p_1}...{p_{r}}}$ cannot contain any contribution from $\epsilon^{abcd}$ and in fact one readily finds that the only Lorentz invariant totally symmetric tensor is a totally symmetrized product of inverse Minkowski metrics $\eta^{ab}$. Hence due to the required diffeomorphism invariance of the theory of gravity, for any gravitational field at wish already the Lorentz invariant expansion point fixes the constant order contribution to the principal polynomial and therefore the causal structure of the linearized EOM uniquely.\\

Note that although the above discovery nicely illustrates the astonishing influence of the required diffeomorphism invariance on the causal structure of the linearized theory of gravity --- reacall that such a linearized theory of gravity for instance already contains enough information to predict the propagation of gravitational waves --- there is no way how we can use this fact to obtain a computational advantage. In contrast to the case before, where we noticed that we might use the Lorentz invariance of the expansion coefficients of the gravitational Lagrangian to reduce the dimensionality of the arising linear systems now the further Lorentz invariance of the expansion coefficients of $\mathcal{P}^{p_1...p_r}$ is not an additional requirement but a result of the diffeomorphism invariance. If up to this point we made no mistakes this simply will come out. It thus can serve as a nice consitency check of any perturbative EOM that we might construct.\\

Given now the expansion of the gravitational principal polynomial we can then simply compute the vanishing set, where obviously we also have to compute this perturbatively, by dropping terms of higher than the desired order. Doing the same also for expansion of the matter principal polynomial we finally can require that the thus obtained perturbative vanishing sets of matter and gravity polynomial coincide in the required perturbative order 
\begin{align}
    \sum_{i=0}^{k-2} (V_{mat})_i = \sum _{i=0}^{k-2}(V_{grav})_i + \mathcal{O}(k-2),
\end{align}
where $k$ is the desired order of the power series expansion of the gravitational Lagrangian. This then ultimately also includes our second requirement, the causal compatibility between matter and gravitational dynamics perturbatively. 

We complete the chapter by providing an explicit recipe (\ref{Algo2}) for the perturbative construction of diffeomorphism invariant gravitational theories that are compatible with a given matter theory.\\

%maybe do it up to arbitrary order, should be possible
%
%
%
\begin{algorithm}[ht]
\SetAlgoLined
\KwData{Matter theory $\mathcal{L}_{mat} : F_{grav} \times J^1F_{mat} \rightarrow \Lambda^4M$, expansion order $k \in \mathbb{N}$, flat Lorentz invariant expansion point $p_0 \in F_{grav}$}
\KwResult{Most general diffeomorphism invariant, causal compatible gravitational Lagrangian expanded as finite power series $\mathcal{L}_{grav,per}$ to order $k$ around $p_0$}
Compute the constant tensors $C^{Bm}_{An}$ \\
Expand the Lagrangian as described in (\ref{generalPowerSL}) around $p_0 \equiv (x_0^m,N_A,0,0)$ with expansion coefficients $a^{\mathcal{I}_k}$\\
Restrict to those $a^{\mathcal{I}_k}$ that generate $2^{\text{nd}}$ derivative order EOM \\
Insert the most generall Lorentz invariant expressions for the $a^{\mathcal{I}_k}$\\
Solve the perturbative equivariance equations for the $a^{\mathcal{I}_k}$ by plugging in $L_{grav,per}$ in (\ref{DiffeoEqnFormal}) and all necessary prolongations and evaluating the resulting expressions at $p_0$\\
Compute the induced expansion of the principal Symbol\\
Chose a $(n-4) \times (n-4)$ full ranked sub matrix of the principal symbol by removing rows $(A_1...A_4)$ and columns $(B_1...B_4)$ from it \\
Compute the expansion of the determinant of the chosen sub matrix\\
Compute the induced expansion of the corresponding prefactors $f_{(A_1...A_4)(B_1...B_4)}$ \\
Compute the expansion of the gravitational principal polynomial with the use of the general expression (\ref{diffeoMinor}) with expansion (\ref{minorPoly}), i.e. by solving (\ref{POLY1}), (\ref{POLY2}), (\ref{POLY3}) and similar for higher orders \\
Compute the expansion of the matter principal polynomial up to order $k-2$\\
Solve $\sum_{i=0}^{k-2} (V_{mat})_i = \sum _{i=0}^{k-2}(V_{grav})_i + \mathcal{O}(k-2)$ w.r.t. the remaining undetermined constants in $L_{grav,per}$
 \caption{Construction of perturbative Gravitational Lagrangian}\label{Algo2}
\end{algorithm}

Summing up the achievements of this chapter, firstly with the help of several notions of formal PDE theory we have discovered a second and final fundamental requirement that we wish to pose on any gravitational Lagrangian, namely its compatibility with a given matter theory. As one might argue that the prediction of future processes really lies at the heart of theoretical physics this causal compatibility really is a indispensable requirement that any meaningful coupled matter gravity theory must incorporate. As stated in the last chapter this requirement also compromises a cornerstone of the framework that was contributed in (\cite{2018PhRvD..97h4036D}). 

Besides of the second main ingredient of our framework we mainly focused on techniques of solving the equivariance equations with the concentration in particular lying on the construction of power series solutions. To justify this perturbatice approach it was essential to prove the involution of the equivariance PDE. In addition to that the further developed techniques from formal PDE theory that we provided in this chapter ultimately allowed us prescribe a detailed instruction how one will always obtain perturbative solutions to the problem of constructive gravity.  All that is left for this thesis consists now in concretely testing the thereby developed framework. This is exactly what we intend to do in the next chapter.

%arbitrary of the general solution!!!

\chapter{Concrete Applications of the Developed Framework}
\dictum{
In this chapter we investigate two concrete examples of possible gravitational fields each one motivated by a certain matter theory that exploits the thereby provided spacetime geometry as background. Concretely we will study the well known case of gravity being described by a metric tensor field but also deviate from this by considering the case of spacetime geometry being provided by a so called area metric. For each case we will then apply the previously developed framework to not only qualitatively examine possible gravitational dynamics but also explicitly construct the most general meaningful perturbative theory of gravity.
}
\section{Perturbative Metric Gravity around Minkowski Spacetime}
As a first example we consider the case where the gravitational field is provided by a symmetric $(0,2)$ tensor field, the metric tensor $g_{ab}$. The field bundle is hence given by the bundle of such tensors and as before labeled as $F_{GR} := S^0_2M \subset T^0_2M$. For some of the following considerations we might more properly restrict to the sub bundle of $(0,2)$ tensors that non degenerate in the sense that the induced map between tangent and cotangent bundle that is obtained by partially applying the metric to a vector field defines an isomorphism. Furthermore we additionally might restrict to those non degenerate metrics that have signature $(-,+,+,+)$. As both of these restrictions define open conditions this does not cause any problems.

Note that the case of gravity being described by a metric tensor field not only is the standard example and hence excellent for testing our framework, from the point of view of constructive gravity it is also unique as the metric tensor is the "smallest" structure --- in the sense that the corresponding field bundle has the least dimensional fibres --- that allows for the support of a non trivial matter theory.

In the simplest possible case the matter field is described by a scalar field, i.e. a spacetime function $\phi \in C^{\infty}(M,\mathbb{R})$ which corresponds to the trivial matter field bundle $M \times \mathbb{R}$. We again restricting to matter theories that are described by first derivative order Lagrangians and hence generate second derivative order EOM. In the simplest case the matter EOM are linear. This correspond to a quadratic matter Lagrangian. Denoting adapted coordinates on $J^1(M \times \mathbb{R})$ by $(x^m,u,u_m)$ the most general such matter Lagrangian is given as
\begin{align}\label{KGL}
    \mathcal{L}_{KG} = \frac{1}{2} \left ( B^{ab} u_a u_b - m^2 u^2\right )\omega \mathrm{d}^4x.
\end{align}
This is the so called Lagrangian \textit{\textbf{Klein-Gordon}} Lagrangian. It is important to observe that this really described the most general quadratic, first order Lagrangian for a scalar field. 
In particular note that specifying such a Lagrangian forces us to prescribe $10$ spacetime functions $B^{ab}$ that tell us how the product $u_a\cdot u_b$ has to be contracted and furthermore one scalar density of weight $1$ $\omega$ that ensures that the Lagrangian really defines volume form valued bundle map and thus provides us with a well defines action functional and EOM.
These additional ingredient are precisely provided by a metric tensor field $g_{ab}$. We then simply take $B^{ab}$ as the corresponding inverse metric $(g^{-1})^{ab}$. Note that in the case of non degenerate metrics mapping a metric to its inverse is a smooth bundle isomorphism. Furthermore one can in fact show that up to an overall constant there is exactly one scalar density of weight $1$ that can be obtained form the metric, namely $\omega = \sqrt{ \vert \mathrm{det}(g) \vert }$. 
Thus we see that the simplest possible non trivial Lagrangian one can write down for a scalar field forces us to additionially provide a metric tensor field. If this metric is not considered to be specified by hand we need further equations that allow us to determine it.  

To that end we are going to apply the previously developed perturbative framework to construct a perturbative expansion of a diffeomorphism invariant gravitational Lagrangian $\mathcal{L}_{GR} = L_{GR} \mathrm{d}^4x$ that is causally compatible with the Klein-Gordon Lagrangian (\ref{KGL}) and generates second derivative order EOM. 
We have already remarked that for the field bundle $F_{GR}$ there exist exactly 14 functionally independent curvature invariants that each solve the homogeneous invariance equation. Due to Lovelock (\cite{Lovelock1969}, \cite{doi:10.1063/1.1665613} and also \cite{doi:10.1063/1.1666069}) we know that out of the 14 independent gravitational Lagrangians that can be constructed from these up to contributions that vanish once we compute the EOM and equate to zero the Einstein-Hilbert Lagrangian is the only one that generates second derivative order EOM. 
Hence we expect from our perturbative construction recipe to precisely recover a perturbative expansion of the Einstein-Hilbert Lagrangian from first principles. Note that thereby we get an excellent test for our framework. \\


We start by introducing the necessary structure on $F_{GR}$. We have already seen that a pair of intertwiners for this field bundle can simply be obtained by the matrices  (\ref{interIMet}) and (\ref{interJMet}). These are of course exactly the same intertwiners as the ones we use for describing second spacetime derivatives. We thus obtain coordinates on $J^2F_{GR}$ as $(x^m,v_A,v_{Ap},v_{AI})$ where now both $A$ and $I$ run from 0 to 9. For a metric tensor, i.e. a section $g \in \Gamma(F_{GR}))$ we further get the relations:
\begin{align}
    g_{ab} = I^A _{ab} g_A \ \ \text{and} \ \ g_A = J^{ab}_{A} g_{ab},
\end{align}
and similar for the inverse metric $g^{-1} \in \Gamma(F_{GR}^{\ast})$.
Following along the lines described in (\ref{Algo2}) we choose $J^2F_{GR} \ni p_0 \equiv (x_0^m,\eta_A,0,0) =: (N_{AI})$ where $\eta_A = J^{ab}_A \eta_{ab}$ as $\eta$-induced expansion point. Furthermore we choose construct the perturbative gravitational Lagrangian up to order $k=3$. 
Next we compute the coordinate expression of the Lie derivative of a metric tensor field to obtain the constant tensors $C_{An}^{Bm}$. We get
\begin{align}
    \mathcal{L}_{\xi} g_{ab} = \partial_m g_{ab} \cdot \xi^m + \left (-2 \delta_n^{(c\vert} \delta_{(a}^m \delta_{b)}^{\vert d)} \right ) g_{cd} \partial_m \xi^n.
\end{align}
Inserting $g_{cd} = I^B_{cd} g_B$ further contracting the whole expression with $J^{ab}_A$ to convert the free $ab$ indices to a $A$ index we find.
\begin{align}
    \mathcal{L}_{\xi} g_A = \partial_m g_A \xi^m + \left (-2 I^B_{nb}J^{mb}_{A} \right )g_B \partial_m \xi ^n. 
\end{align}
Here we also used that the two intertwiners $I^A_{ab}$ and $J_A^{ab}$ are symmetric in their spacetime indices. Thus we can simply read of from the last expression
\begin{align}
    C_{An}^{Bm} = -2 I^B_{nb}J_A^{mb}.
\end{align}
As before we define $(H_{AI}) := (v_{AI}) - (N_{AI})$ to obtain the most general expansion of the Lagrangian that generates second derivative order EOM by
\begin{align}\label{LGR}
    L_{GR,per} =  a_0 + a^A H_A + a^{AI}H_{AI} + a^{AB} H_{A}H_{B} + a^{ApBq} H_{Ap}H_{Bq} + a^{ABI} H_{A} H_{BI} \\
    + a^{ABC} H_a H_B H_C + a^{ABpCq} H_{A}H_{Bp}H_{Cq} +
    + a^{ABCI} H_A H_B H_{CI} 
    + \mathcal{O}(4).
\end{align}
We again restrict attention to those expansion coefficients with an even number of spacetime indices, as any coefficients with an odd number of such cannot be Lorentz invariant and thus are forbidden by the equivariance equations.

According to (\ref{Algo2}) the next step now consists of inserting the most general Lorentz invariant tensors with appropriate index structure for the expansion coefficients. This is done entirely in terms of the developed computer algebra. The explicit expressions can be found in the appendix (\ref{LorentzGR1}) and (\ref{LorentzGR2}).
Nevertheless providing an illustrative example we consider the expansion coefficient $a^{AB} = I^{A}_{ab}I^{B}_{cd}a^{abcd}$. We clearly see the symmetries in the pairs $(ab)$ and $(cd)$ and the additional symmetry under the exchange $(ab) \leftrightarrow (cd)$ as the coefficient only appears contracted against $H_AH_B$ in $L_{GR,per}$. Writing down the most general Lorentz invariant tensor with these symmetries we readily find that we cannot obtain non zero contributions from $\epsilon^{abcd}$ to any tensor that features these symmetries. The inverse Minkowski metric a priory allows for three different expressions with 4 upper case indices:
\begin{itemize}
    \item[(i)] $\eta^{ab} \eta^{cd}$ 
    \item[(ii)] $\eta^{ac} \eta^{bd}$ 
    \item[(iii)] $\eta^{ad} \eta^{bc}$.
\end{itemize}
Enforcing the symmetries on these three expressions we find that (i) already features the desired symmetries and (ii) and (iii) both provide the symmetry once we symmetrize in $(ab)$. These two terms then are precisely the same. We thus find that 
\begin{align}\label{ansatzExample}
    a^{AB} = I^{A}_{ab}I^{B}_{cd} \left ( 8\mu_3 \cdot \eta^{ab}\eta^{cd} + 8\mu_4 \cdot \eta^{c(a} \eta^{b)d}   \right ).
\end{align}
Here the factor $8$ is a result of the factorless symmetrization that we implement in our computer program, i.e. when symmetrizing the expression w.r.t. $(ab),(cd)$ and $(ab) \leftrightarrow (cd) $ we do not divide by $2$ each time and hence get an overall factor of $2^3=8$. This approach is equivalent to the symmetrization including these factors as we are free to absorb such factors by redefining the constants. It furthermore provides the advantage that we can then deal with integer factors throughout the whole computation. Details are explained in section (\ref{LorentzGen}).
Thus we have obtained an expression that features 2 constants for $a^{AB}$.
Note that computing the remaining expansion coefficients works exactly along the same lines as the above with the only difference being that the expressions will become increasingly involved and thus are best obtained by using computer algebra.
Doing this the dimensions, i.e. number of arbitrary constants in the expansion coefficients are displayed in table (\ref{GRExp}).
\begin{table}
\centering 
\begin{tabular}{c|c|c}
    expansion coefficient & dimension & constants   \\
    \hline 
    $a_0$ & 1 & $\{\mu_1\}$ \\
    $a^A$ & 1 & $\{\mu_2\}$ \\
    $a^{AI}$ & 2 & $\{\nu_1, \nu_2\}$ \\
    $a^{AB}$ & 2 & $\{\mu_3, \mu_4 \} $ \\
    $a^{ApBq}$ & 6 & $\{\nu_3,...,\nu_8\}$ \\
    $a^{ABI}$ & 5 & $\{ \nu_9,...,\nu_{13} \}$ \\
    $a^{ABC}$ & 3 & $\{ \mu_5,...\mu_7 \}$\\
    $a^{ABpCq}$ & 21 & $\{\nu_{14},...,\nu_{34} \}$ \\
    $a^{ABCI}$ & 13 & $\{ \nu_{35},...,\nu_{47}\}$
\end{tabular}
\caption{Dimensions of the Lorentz invariant expansion coefficients for $\mathcal{L}_{GR,per}$.}\label{GRExp}
\end{table}
Here we separated the appearing constants in those that appear in front of terms including derivatives of the metric tensor field and those that appear in terms that do not include such. We adopt some terminology more present in QFT related subjects and refer to the former expressions as \textbf{\textit{kinetic terms}} ans the latter as \textit{\textbf{mass terms}}. The constants are then called \textit{\textbf{kinetic parameters}} $(\nu_1,...)$ and \textit{\textbf{masses}} $(\mu_1,...)$ respectively.   \\

According to algorithm (\ref{Algo2}) the next step consists of inserting these expansion coefficients and the previously computed expression for the constant tensor $C_{An}^{Bm}$ into the perturbative equivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}). These are then linear equations for the unknown constants $\{ \mu_1,...,\mu_7\}$ and $\{\nu_1,...,\nu_{47}\}$. The solution we get from doing this by using our computer program is displayed in equation (\ref{GRSol}) in the appendix. Note that the expansion (\ref{LGR}) in generall features $15.906$ undetermined constants in the $9$ expansion coefficients. The restriction to those that define the components of Lorentz invariant tensor that is demanded by the perturbative equivariance equations reduces these to $54$ constants. Solving the perturbative equivariance equations then further decreases the number of undetermined parametes to $2$. Also observe that in the solution there is no contribution from terms that involve $\epsilon^{abcd}$. In fact the constant parameters in front of expansion coefficients that contain the Levi-Civita symbol are the only ones that the equivariance equations set to zero. \\

We continue with construction algorithm (\ref{Algo2}) by computing the perturbative expansion of the gravitational principal polynomial. Inserting the solution for the expansion coefficients (\ref{GRSol}) into (\ref{LGR}) now yields the most general diffeoorphism invariant perturbative Lagragian. Normally we would now proceed by plugging the solved expansion coefficients into the perturbative EOM (\ref{EOMPert}) and then compute the corresponding expansion of the principal symbol and polynomial as outlined in the last chapter. It is however worth noting that for the simple case of the gravitational field being provided by a metric tensor it is actually not necessary to follow along these general steps. We can in fact save ourselves much work by taking a closer look at equation (\ref{polyEqn}) that the principal polynomial has to satisfy as consequence of the diffeomorphism equivariance. For the metric tensor field this equation takes the particular simple form
\begin{align}\label{metricPoly}
    0 = \partial^A \mathcal{P}_{GR}^{abcd} C_{An}^{Bm} v_B - 4\mathcal{P}_{GR}^{(abc\vert m} \delta_n^{\vert d)} + 2 \mathcal{P}_{GR}^{abcd} \delta^m_n.
\end{align}
We now expand $\mathcal{P}_{GR}^{abcd}$ around $(N_{A}) = (x_0^m, \eta_A)$ and construct a power series solution to (\ref{metricPoly}). From before we know that given the expansion of the Lagrangian up to third order we only need to expand the polynomial up to first order and hence also the totally symmetric expression $\mathcal{P}_{GR}^{abcd}$ is expanded up to linear order. We get 
\begin{align}
    \mathcal{P}_{GR}^{abcd} = (P_0)_{GR}^{abcd} + (P_1)_{GR}^{abcdA} H_A + \mathcal{O}(2).
\end{align}
Evaluating equation (\ref{metricPoly}) and its first prolongation at the expansion point $\eta_A$ and contracting it against the Lorentz generators yields additional zero and first order equations that state that the expansion constants must be the components of constant Lorentz invariant tensors. One readily finds that for the constant order expansion coefficient there exists exactly one such expression namely:
\begin{align}
   (P_0)_{GR}^{abcd} = a \cdot \eta^{(ab} \eta^{cd)}. 
\end{align}
For the linear order coefficients we obtain the following two possible expression:
\begin{align}
    (P_1)_{GR}^{abcdA} = I^A_{ef} \left (b \cdot \eta^{(ab} \eta^{cd)}  \eta^{ef} + c \cdot \eta^{(ab} \eta^{c \vert e} \eta^{f \vert d)} \right ).
\end{align}
Upon inserting the thereby obtained expansion in (\ref{metricPoly}) and evaluating at the expansion point $(N_A)$ we get:
\begin{align}
    b = a \ \ \text{and} \ \ c = -2a.
\end{align}
Inserting this into $\mathcal{P}_{GR}^{abcd}$ and contracting again against $k_a\cdot ...\cdot k_d$ in total we find the following expression for the most general linear order expansion of the principal polynomial that is consistent with the diffeomorphism equivariance of $L_{GR,per}$
\begin{align}
\begin{aligned}
    \mathcal{P}_{GR} &= a \cdot  \eta(k) \cdot \left [\eta(k) + H \cdot \eta(k)  -
    2 \cdot  \eta(k) H(k) \right ] + \mathcal{O}(2)\\
    &= a \cdot (1 + H) \cdot (\eta(k) - H(k))^2 + \mathcal{O}(2).
\end{aligned}
\end{align}
where  $H:=\eta^{ab} I^A_{ab} H_{A}$ and $H(k) := \eta^{ap} \eta^{bq} I_{pq}^A H_A k_a k_b$ and $\eta(k) := \eta^{ab}k_a k_p$. 
\begin{comment}
Taking a section $g \in \Gamma(F_{GR})$ and the corresponding inverse metric $g^{-1} \in \Gamma(F_{GR}^{\ast})$ with $(g^{-1})^{ab}=:g^{ab}$ and the usual relation $g^{ab}g_{bc}= \delta^a_c$ and evaluatting the thus obtained most general expression for the principal polynomial we find
\begin{align}
    \mathcal{P}_{GR,per} (g_{ab}) = -a \cdot \mathrm{det}(g) g^{ab} g^{cd} k_ak_bk_ck_d + \mathcal{O}(2).
\end{align}
%check notation v vs g
\end{comment}
Note in particular that the vanishing set of this principal polynomial is completely independent of the choice of the remaining parameter $a$. Thus the causal structure of the perturbatice metric gravity EOM is already uniquely fixed by requiring diffeomorphism invariance.
Furthermore one readily finds that the principal polynomial of the Klein-Gordon equation yields the following linear order expansion
\begin{align}
    \mathcal{P}_{KG} = (1 + H/2) \cdot (\eta(k) - H(k)) + \mathcal{O}(2) .
\end{align}
To relate the perturbative vanishing sets of $\mathcal{P}_{GR,per}$ and $\mathcal{P}_{KG}$ note that we can change the first factor of the two polynomials $(1+H)$ and $(1+H/2)$ respectively at wish by multiplying with the perturbative expansion of the following non vanishing scalar density of weight $2n$:
\begin{align}
    f_{dens}(n) := \vert \mathrm{det}\left (I^A_{pq}v_A \right )\vert ^n = 1 + nH + \mathcal{O}(2).
\end{align}
This obviously does not alter the vanishing set described by either one of the polynomials. Thus multiplying the gravitational polynomial by $f_{dens}(-1)$ and dividing by the prefactor $a$ and multiplying the matter polynomial by $f_{dens}(-1/2)$ we find
\begin{align}
    \mathcal{P}_{GR,per} = (\eta(k)-H(k))^2 = \mathcal{P}_{KG}^2.
\end{align}
Therefore the matter and gravitational polynomial $\mathcal{P}_{KG}$ and $\mathcal{P}_{GR,per}$ define the same vanishing set in linear order.
Hence the two theories are causal compatible and our construction procedure is finished. We are left with a perturbative Lagrangian that contains 2 undetermined constants describing a 2 parameter family of perturbative metric gravity theories.  \\

As mentioned earlier in this section Lovelock has shown that the only diffeomorphism invariant metric theory of gravity with second order EOM is provided by the \textit{\textbf{Einstein-Hilbert}} Lagrangian (\cite{Lovelock1969}, \cite{doi:10.1063/1.1665613}, \cite{doi:10.1063/1.1666069}) that reads:
\begin{align}
    L_{EH} := \kappa \sqrt{\vert \mathrm{det} \left ( I^A_{pq}v_A \right ) \vert }  \left( R + \Lambda \right ),
\end{align}
where the two constant parameters $\kappa$ and $\Lambda$ are called gravitational constant and cosmological constant respectively. Here $R$ is the so called Ricci scalar. As we have mentioned earlier in this thesis one usually first constructs the Riemann tensor, the curvature two form of the unique metric compatible connection on the frame bundle over $M$. From this one obtaines the Ricci tensor by computing its trace. Finally one contracts the Ricci-tensor with the inverse metric to end up with the Ricci scalar $R$. From the point of view of classical field theory all the deeper connections to Riemanian geometry that are present the above procedure are however actually not necessary. The only important fact one really needs is that the Ricci scalar defines a function on $J^2F_{GR}$. To that end reacall that any non degenerate metric defines a smooth bundle isomorphism 
\begin{align}\label{music}
\begin{aligned}
\flat : TM \longrightarrow T^{\ast}M\\
X \longmapsto g(X,-) .
\end{aligned}
\end{align}
Through this construction we therefore also obtain an isomorphism between the field bundle $F_{GR}$ and its dual $F_{GR}^{\ast}$ that with a slight abuse of notation will also be called $\flat$. 
Using the dual coordinates $(x^m,v^A)$ on $F_{GR}^{\ast}$ we define $v^A_{\flat} := v^A \circ \flat$. Thus applying $v^A_{\flat}$ on any metric exactly yields the components of its inverse. Using this we get the following explicit expression for $R$ understood as a map on $J^2F$ as:
\begin{align}
R = v_{\flat}^A J_A^{ab} \left ( D_p \Gamma^p_{ba} - D_b \Gamma^p_{pa} + \Gamma^p_{pq} \Gamma^q_{ba} - \Gamma^p_{bq} \Gamma^q_{pq} \right ),
\end{align}
where we also defined the Christoffel symbols that are given by the following functions on $J^1F_{GR}$:
\begin{align}
\Gamma^a_{bc} = \frac{1}{2} v_{\flat}^A J_A^{ap} \left ( I^B_{bq}\delta^p_b + I^B_{cq}\delta^p_c - I^B_{bc}\delta^p_q  \right ) v_{Bq}.
\end{align}
In order to compare the computed gravitational Lagrangian with the Einstein-Hilbert Lagrangian expand $L_{EH}$ to third order around the chosen eta-induced expansion point $(N_{AI})$. The expansion of the determinant can be obtained from (\ref{detExp}). Doing so we find that:
\begin{align}
   \sqrt{\vert \mathrm{det} \left ( I^A_{pq}v_A \right ) \vert } = 1 + H/2 +1/8 \cdot H^2 - 1/4 \cdot I^A_{ab}I^B_{cd} \eta^{ac} \eta^{bd} H_A H_B + \mathcal{O}(3).   
\end{align}
In order to compute the expansion of $v^A_{\flat}$ one uses that $-1/2 \cdot v_{\flat}^AC_{An}^{Bm}V_B = v_{\flat}^A J_A^{nb}I^B_{bm}v_B = \delta^m_n$. Deriving this equation w.r.t. $v_B$ and performing some simplifications one thus finds the usual expression:
\begin{align}
    \partial^Bv_{\flat}^A = 1/4 \cdot C_{Cn}^{Am}C_{Dm}^{An}v_{\flat}^c v_{\flat}^{D}.
\end{align}
Using this one obtains the following expansion for $v_{\flat}^A$:
\begin{align}
    v_{\flat}^A = \eta^A - \eta^{ap}\eta^{bq} I^B_{ab} I^A_{pq} \cdot H_B + \eta^{ac}\eta^{bd}\eta^{ef} I^B_{ab} I^C_{ce} I^A_{df} \cdot H_BH_C + \mathcal{O}(3).  
\end{align}
With these results computing this expansion is straight forward yet quite laborious.
Starting with the linear order of the Einstein-Hilbert Lagrangian we find:
\begin{align}
        L_{EH} = \kappa \cdot (\eta^{ap}\eta^{bq} - \eta^{ab}\eta^{pq}) I^{A}_{ab}I^{I}_{pq} H_{AI} + \kappa \Lambda \cdot (1 + 1/2 \cdot \eta^{ab} I_{ab}^A H_A) + \mathcal{O}(2).
\end{align}
On the other hand inserting the explicit Lorentz invariant expressions for the expansion coefficients (\ref{LorentzGR1}) with the appropriate multiplicities from the factor free symmetrizations and the computed solution of the perturbative equivariance equations (\ref{GRSol}) into $\mathcal{L}_{GR,per}$ we find:
\begin{align}
    \mathcal{L}_{GR,per} = \mu_1(1 + 1/2 \cdot \eta^{ab} I_{ab}^A H_A) - 8 \nu_1 \left(\eta^{ap}\eta^{bq} - \eta^{ab}\eta^{pq} \right )I^{A}_{ab}I^{I}_{pq} H_{AI} + \mathcal{O}(2).
\end{align}
Thus we see that the obtained expansion for $\mathcal{L}_{GR,per}$ up to linear order precisely yields the Einstein-Hilbert Lagrangian with gravitational constant $\kappa = -8 \nu_1$ and cosmological constant $\Lambda = -1/8 \cdot \frac{\mu_1}{\nu_1}$. Carefully proceeding with the comparison one then finds that also in second and third order the computed perturbative Lagrangian exactly coincides with the expansion of $\mathcal{L}_{EH}$.  
%make this more precise
\section{Perturbative Area Metric Gravity around a Flat Minkowski Induced Spacetime}
Whereas the first example theory we constructed mainly served of testing our framework the perturbative theory of gravity that we are going to construct now really is intended to provide a viable alternative to GR in describing the geometry of our universe.  
To that end we are going to construct a perturbative theory of gravity that is compatible with the most general linear theory of Electrodynamics. 

We start by providing some results from the premetric treatement of Electrodynamics that can be found in \cite{1999PhLB..458..466O}, \cite{1999gr.qc....11096H}, (\cite{hehl2003foundations}, \cite{2006physics..10221H}, \cite{2004PhRvD..70j5022L} and also \cite{Hehl2005}). The basic idea of this premetric approach lies in describing all fields that are involved in the treatment of Electrodynamics without referring to an additional spacetime geometry. This is most easily illustrated using the calculus of exterior forms. One starts by introducing the 4 dimensional electric charge current density $J$. Aa one wishes to integrate $J$ over 3 dimensional submanifold $\Omega$ of the spacetime $M$ to obtain the total electric charge current --- the total electrical current flowing through the boundary of $\Omega$ and the electric charge inside --- that is confined therein the current density is most naturally described as a three form $J \in \Gamma(\Lambda^3M)$. Demanding charge conservation in the sense that the integral of $J$ over any closed 3 dimensional submanifold of $M$ vanishes, by using the theorem of stokes on arrives at the requirement $dJ =0$. By de Rham's first theorem we then find that $J$ is necessarily given as exterior derivative of some 2 form $H \in \Gamma(\Lambda^2M)$:
\begin{align}
    J = d H.
\end{align}
From considerations involving the Lorentz force one similarly obtaines that the magnetic analogue to $J$ the field strength $F$ which plays the role of a 4 dimensional magnetic flux current density should be described by a 2 form $F \in \Gamma(\Lambda^2M)$. Its integral over a 2 dimensional submanifold $C \subset M$ describes the total magnetic flux current in terms of the total magnetic flux flowing through $C$ and the total magnetic flux current\footnote{Note that by this interpretation rewriting $F$ in terms of a 3+1 split of space time and the usual $E$ and $B$ field the magnetic flux is as usual given by $B$ whereas $E$ is now interpreted as the corresponding magnetic flux current (see \cite{2006physics..10221H}).} flowing through the boundary of $C$. The corresponding conversation law deduced in similar fashion as before then reads $dF =0$. Thus we conclude using again de Rham's first theorem the existance of a 1 form $A \in \Gamma(\Lambda^1M)$ that satisfies:
\begin{align}
    F = d A.
\end{align}
Finally one assumes that the two 2 forms $F$ and $H$ are related via a local and linear spacetime relation. Writing the 2 forms in terms of some chart induced basis $F = F_{ab} = \mathrm{d}x^a \otimes \mathrm{d}x^b$ with $F_{ab} = - F_{ba}$ and similar for $H$ specifying such a relation requires an additional $(0,4)$ tensorfield $G$ that obeys the following symmetries
\begin{align}\label{areaSym}
    G_{abcd} = -G_{bacd} = G_{cdab}.
\end{align}
Note that such a tensor field at each spacetime point $p\in M$ defines a symmetric inner product on the space of two forms $\Lambda^2_pM$. Consequently we will call such a tensor field an \textbf{\textit{area metric}} tensor field and denote the bundle of these area metric tensors by $F_{Area}$. Due to (\ref{areaSym}) one need $21$ independent component functions to uniquely specify such an area metric. Thus $F_{Area}$ has $21$ dimensional fibres.   In the following we restrict the bundle of such area metric tensor fields $G$ by requiring that the inner product they define is non degenerate for all $p$ in the sense that $G$ provides us with a bundle isomorphism 
\begin{align}
\flat_{Area} : \Lambda^2M \longrightarrow (\Lambda^2M)^{\ast} = A^2_0M,
\end{align}
that is defined exactly the same way as in the metric case (\ref{music}). Here $A^2_0M$ denotes the bundle of antisymmetric $(2,0)$ tensors. 
The linear relation between $H$ and $F$ can now obtained by first using the isomorphism provided by the area metric to map the 2 form $F$ to an antisymmetric $(2,0)$ vectorfield and then applying the Hodge star operator\footnote{Details regarding the construction and properties of the Hodge start operator can be found in \cite{Abraham:1988:MTA:50877}} to again end up with a 2 form. In coordinates we therefore get:
\begin{align}
    H_{ab} = 1/2 \omega_G \epsilon_{abcd} G^{cdef} F_{ef},
\end{align}
Note that in absence of a (pseudo)-Riemanian metric the Hodge star operator requires the choice of a volume form (or equivalently a scalar density of weigh 1) $\omega_G$. One possible such choice is for instance given by $\omega_G = \epsilon^{abcd}G_{abcd}$ which obviously only works if this expression is nowhere vanishing.

From considering the appropriate dimensions of $F$ and $H$ on finds that the only viable Lagrangian constructed from $F$ and $H$ is given by $F \wedge H$. Expressing this in terms of the constitutive relation between $H$ and $F$ we finally arrive at the Lagrangian of so called \textbf{\textit{General Linear Electrodynamics}} (in short GLED):
\begin{align}
    \mathcal{L}_{GLED} = \omega_G G^{abcd}F_{ab}F_{cd}\mathrm{d}^4x.
\end{align}
Note that this really defines the most general quadratic Lagrangian which thus generates linear EOM that one can specify in terms of the  electromagnetic field strength 2 form $F$. In particular this Lagrangian contains the standard Maxwell electrodynamics on a metric background for the special case $G^{abcd} = 2 g^{c^[a}g^{b]d}$ and $\omega_{G}=\sqrt{\vert -\mathrm{det}(g) \vert}$. Just as it was the case for the Klein-Gordon Lagrangian for the Lagrangian of GLED there exist now two possibilities: either one specifies the area metric tensor field by hand or --- and this is the route that we want to take --- one supplements the GLED matter theory with a suitable theory of area metric gravity that then allows one to determine the area metric by solving the corresponding area metric gravity EOM. 
In the following we are going to construct the perturbative expansion of such a theory of are metric gravity by following our construction recipe (\ref{Algo2}).\\

To that end we quickly want to translate $\mathcal{L}_{GLED}$ into the language of jet bundles. Note that the electromagnetic field strength can be expressed in terms of the 1 form $A \in \Gamma(\Lambda^1(M))$ as:
\begin{align}
F = F_{ab} \mathrm{d}x^a \otimes \mathrm{d}x^b = 2 \partial_{[a} A_{b]}\mathrm{d}x^a \otimes \mathrm{d}x^b.
\end{align}
Thus $F$ can be understood as a function on the first order jet bundle $J^1(\Gamma^1M)$. We again use adapted coordinate $(x^m,u_a,u_{am})$ on this bundle. Together with the area metric field bundle $F_{Area}$ and the bundle isomorphism $\flat_{Area}$ we thus see that the GLED Lagrangian describes a bundle map $\mathcal{L}_{GLED} : F_{Area} \times J^1(\Lambda^1M) \rightarrow \Lambda^4M $, where now also $\omega_G$ is understood as a bundle map on $\Lambda^1M$.  

We start the derivation of perturbative area metric gravity by constructing the pair of intertwiners for the area metric field bundle $F_{Area}$, i.e. the subbundle of $T^0_4M$. Recall from chapter 1 that given coordinates $(x^m)$ on $M$ one can obtain a pair of intertwiners for such a subbundle of $T^0_4M$ by dividing the set of the $4^4$ fibre coordinates of $T^0_4M$ $v_{abcd}=\frac{\partial}{\partial x^a} \otimes ... \otimes \frac{\partial}{\partial x^d}$ in equivalence classes defined by the symmetries (\ref{areaSym}) sorting them according to some order relation and then constructing the pair of intertwiners as described in (\ref{defI}) and (\ref{defJ}). We will use the following ordered set of $21$ equivalence classes $[v]$ that are specified in terms of their representative $v$:
\begin{align}
\begin{aligned}
    \bigl [ [v_{0101}], [v_{0102}], [v_{0103}], [v_{0112}], [v_{0113}], [v_{0123}], [v_{0202}], [v_{0203}], [v_{0212}], [v_{0213}], [v_{0223}]\\
    , [v_{0303}], [v_{0313}], [v_{0323}], [v_{1212}], [v_{1203}], [v_{1213}], [v_{1223}], [v_{1313}], [v_{1323}], [v_{2323}]  \bigr ].
\end{aligned}
\end{align}
We label the equivalence classes by indices $A$ that now run from $0$ to $20$.
There are now two possibilities for the individual classes: either we have an equivalence class with representative of the form $v_{ijij}$. Such a class then consists of the following 4 elements
\begin{align}
    [v_{ijij}] = \{ v_{ijij}, v_{jiji}, -v_{ijji}, -v_{jiij} \}.
\end{align}
Thus the multiplicity $\sigma$ is 4 for these classes.
The other possibility is given when the representative of the equivalence class takes the general form $v_{ijkl}$ where $i<j$ or $i=j \land k<l$. The equivalence class consists then of 8 elements:
\begin{align}
    [v_{ijkl}] = \{v_{ijkl},v_{jikl}, v_{klij}, v_{lkji}, -v_{jikl}, -v_{ijlk}, -v_{lkij}, v_{klji} \}.
\end{align}
Such classes have multiplicity 8. The intertwiners $I^A_{abcd}$ and $J_A^{abcd}$ can now be constructed according to (\ref{defI}) and (\ref{defJ}). The explicit non vanishing components of these intertwiners are displayed in the appendix (\ref{AreaI}) and (\ref{AreaJ}). Using these we define the new adapted coordinate functions on $F_{Area}$ and its dual $F_{Area}^{\ast}$
\begin{align}
    \begin{aligned}
    v_A = J_A^{abcd}v_{abcd} \\
    v^A = I^A_{abcd}v^{abcd}.
    \end{aligned}
\end{align}
As usual we are going to label the corresponding adapted coordinates on $J^2F_{Area}$ by $(x^m,v_A,v_{Ap},v_{AI})$. 
Note that from the fact that the fibre dimension of $F_{Area}$ equals $21$ whereas $F_{GR}$ only has fibre dimension $10$ one readily sees that the are metric indeed compromises a richer structure than the metric. It is however not a priory clear if this is also reflected in the corresponding theory of gravity. Recall that the metric allowed for $14$ functionally independent curvature scalars on $J^2F_{GR}$ and hence also for $14$ functionally independent diffeomorphism invariant Lagrangians. With theorem (\ref{GeneralSol}) at hand we can now imediately compute the corresponding quantity for the area metric. Doing so we find that there exist $179$ functionally independent diffeomorphism invariant Lagrangians that can be constructed from the area metric and its first two derivatives. Thus in general we expect the perturbative theory of area metric gravity to contain more undetermined parameters than the gravitational and cosmological constant that we obtained for the metric case.


Next we are going to chose the $\eta$-induced expansion point $(N_{AI}) = (x_0^m, N_A, 0,0)$ to be provided by $N_A := 2 \eta_{c[a} \eta_{b]d} - \epsilon_{abcd}$. Not that assuming that $\omega_G(N_A) = const \neq 0$ inserting this into $\mathcal{L}_{GLED}$ one really obtains the Lagrangian of standard Maxwell Electrodynamics on a flat Minkowskian background. Thus expanding the to-be-constructed Lagrangian of area metric gravity around $(N_{AI})$ serves the purpose of interpreting the result as perturbative theory around a flat spacetime. Note that we included the totally antisymmetric $\epsilon_{abcd}$ in the definition of $N_A$ to ensure that  the possible density choice $\omega_G = \epsilon^{abcd}G_{abcd}$ does not vanish when evaluated at the flat Minkowski $\eta$-induced background. 

We have already stated that we want to construct the perturbative are metric gravity theory up to order $k=3$. Following (\ref{Algo2}) we proceed by computing the coordinate expression of the Lie derivative of $G_{abcd}$ to read of the constant tensor $C_{An}^{Bm}$:
\begin{align}\label{LieArea}
    \mathcal{L}_{\xi} G_{abcd} = \partial_m G_{abcd} \cdot \xi^n + \left (-4 \delta_n^{[e\vert} \delta_{[\mathnode{a}}^m \delta_{b]}^{\vert \mathnode{f} ]} \delta_{[c}^{[g} \delta_{\mathnode{d}]}^{\mathnode{h}]} \right ) \cdot G_{efgh} \partial_m \xi^n.
\end{align}
\begin{tikzpicture}[overlay]
   \path [>=stealth, <->, shorten <= 3pt, shorten >=3pt]
     (a) edge [bend left=-80] (d);
    \path [>=stealth, <->, shorten <= 2pt, shorten >=2pt]
     (f) edge [bend left=80] (h);
\end{tikzpicture}
Here the additional arrows denote the symmetrization w.r.t. the interchange of the antisymmetric pairs $[ab] \leftrightarrow [cd]$ and $[ef] \leftrightarrow [gh]$ respectively.
% check lie derivative signs
%
%
%
Similar to before we now insert several intertwiners $I^A_{abcd}$ and $J_A^{abcd}$ to rewrite the second term in (\ref{LieArea}) in terms of abstract area metric indices $A$. Here we can use the fact that the two intertwiners already feature the full set of area metric symmetries.  Doing so we simply can read of the constant tensor as:
\begin{align}\label{areaGotayMInter}
    C_{An}^{Bm} = -4 I^B_{nbcd} J_A^{mbcd}.
\end{align}
The most general power series expansion of $\mathcal{L}_{Area,per} = L_{Area,per}\mathrm{d}^4x$ that generates second derivative order EOM is again given by:
\begin{align}\label{LArea}
    L_{Area,per} =  a_0 + a^A H_A + a^{AI}H_{AI} + a^{AB} H_{A}H_{B} + a^{ApBq} H_{Ap}H_{Bq} + a^{ABI} H_{A} H_{BI} \\
    + a^{ABC} H_a H_B H_C + a^{ABpCq} H_{A}H_{Bp}H_{Cq} +
    + a^{ABCI} H_A H_B H_{CI} 
    + \mathcal{O}(4),
\end{align}
where $(H_{AI}) = (x^m,H_A,H_{Ap},H_{AI}) = (v_{AI}) - (N_{AI})$. Note that although we use the same symbols as in $L_{GR,per}$ to denote $L_{Area,per}$ this Lagrangian defines a fundamentally different object. In particular the $21$ dimensional fibres of  $F_{Area}$ result in the fact that there will be more freedom in constructing the expansion coefficients of (\ref{LArea}). Again the perturbative equivariance equations demand that these expansion coefficients define the components of constant Lorentz invariant tensors. The corresponding expressions that we obtained by means of our computer program are displayed in the appendix in (\ref{LorentzArea1}), (\ref{LorentzArea2}), (\ref{LorentzArea3}) and (\ref{LorentzArea4}). The dimensions and paramters of the individual expansion coefficients are displayed in table (\ref{AreaExp}).
\begin{table}
\centering 
\begin{tabular}{c|c|c}
    expansion coefficient & dimension & constants   \\
    \hline 
    $a_0$ & 1 & $\{\mu_1\}$ \\
    $a^A$ & 2 & $\{\mu_2,\mu_3\}$ \\
    $a^{AI}$ & 3 & $\{\nu_1,..., \nu_3\}$ \\
    $a^{AB}$ & 6 & $\{\mu_4,..., \mu_9 \} $ \\
    $a^{ApBq}$ & 15 & $\{\nu_4,...,\nu_{18}\}$ \\
    $a^{ABI}$ & 16 & $\{ \nu_{19},...,\nu_{34} \}$ \\
    $a^{ABC}$ & 15 & $\{ \mu_{10},...\mu_{24} \}$\\
    $a^{ABpCq}$ & 110 & $\{\nu_{35},...,\nu_{144} \}$ \\
    $a^{ABCI}$ & 72 & $\{ \nu_{145},...,\nu_{216}\}$
\end{tabular}
\caption{Dimensions of the Lorentz invariant expansion coefficients for $\mathcal{L}_{Area,per}$.}\label{AreaExp}
\end{table}
In total we thus get from solving this first implication of the perturbative equivariance equations, the Lorentz invariance of the expansion coefficients, $240$ yet undetermined constant parameters. These can be divided into $24$ masses and $216$ kinetic parameters. It is worth noting that the perturbative equaivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}) decouple into the two sub systems containing only the mass term and kinetic term coefficients respectively. Once we insert the obtained Lorentz invariant expression the former yields a linear system that is compromised of $14$ independent equations for the $24$ masses. The latter contains $174$ independent equations for the $216$ kinetic parameters. Thus in total we get $188$ indendent linear equations that therefore reduce the number of undetermined parameters that are contained in $\mathcal{L}_{Area,per}$ from $240$ to $52$. We get a total of $10$ masses and $42$ kinetic parameters. The solution of the linear system in terms of the $52$ parameters (\ref{AreaParas}) is displayed in the equations (\ref{AreaSol1}) to (\ref{AreaSol20}). Although the perturbative Lagrangian that we thus obtain is clearly clearly more complicated then the expansion of the Einstein-Hilbert action and in particular now contains $52$ undetermined parameters compared to $2$ gravitational constants that contribute to $\mathcal{L}_{GR,per}$ we can now see the true strength of the required diffeomorphism invariance that is encoded in the perturbative equivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}). To that end it is important to observe that a priori the expansion (\ref{LArea}) allows for a total of $133694$ undetermined parameters. It is thus remarkable that solely the requirement of diffeomorphism invariance reduces this number to the $52$ parameters that are left in the obtained solution. \\

Following (\ref{Algo2}) we now compute the principal polynomials of the EOM generated from $\mathcal{L}_{GLED}$ and $\mathcal{L}_{Area,per}$ to deduce whether the two theories are already causally compatible or this second requirement yields further conditions for the $52$ gravitational constants in the perturbative area metric gravity Lagrangian. This time we start by computing the GLED polynomial. This was first computed by Rubilar (see \cite{2009JPhA...42U5402I}) in the context of premetric Electrodynamics and is given by the following expression with corresponding first order expansion: 
\begin{align} \label{GLEDPoly}
\begin{aligned}
    \mathcal{P}_{GLED} &= -\frac{1}{24}\omega_G^2\epsilon_{mnpq}\epsilon_{rstu}J_A^{mnra}J_B^{bpsc}J_C^{dqtu} v_{\flat_{Area}}^A v_{\flat_{Area}}^B v_{\flat_{Area}}^C k_ak_bk_ck_d \\
                &= \bigl[ 1 -  A \eta(H)- \frac{1}{2} A \epsilon(H) + \frac{1}{12} \epsilon(H) \bigr] \eta(k)^2 - H(k)\eta(k) + \mathcal{O}(2)\\
                &= \bigl\{  \bigl[ 1 - \frac{1}{2} A \eta(A) - \frac{1}{4} A \epsilon(H) +  \frac{1}{24} \epsilon(H) \bigr] \eta(k) - \frac{1}{2} H(k)       \bigr\}^2 + \mathcal{O}(2).
\end{aligned}
\end{align}
Here we defined $v_{\flat_{Area}}^A = v^A \circ \flat_{Area}$ and used $v_{\flat_{Area}}^A J_A^{abcd} I^B_{cdef}v_B = = 4 \delta^{[a}_e \delta^{b]}_f$ to expand $v^A_{\flat_{Area}}$ around $N_A$. Further we defined
\begin{align}
\begin{aligned}
\eta(H) &:= \eta^{ac}\eta^{bd} I^A_{abcd} H_A, \\ 
\epsilon(H) &:=\epsilon^{abcd}I^A_{abcd}H_{A},\\ H(k) &:=\eta^{ac}\eta^{bp}\eta^{cq} I^A_{abcd}H_Ak_pk_q, \\ \eta(k)&:=\eta^{pq}k_pk_q.
\end{aligned}
\end{align}
Besides of that $A$ denotes a constant that depends on the specific choice of density $\omega_G$ in the GLED Lagrangian.  
We now compute the principal polynomial of the constructed diffeomorphism invariant perturbative theory of area metric gravity. To that end we start by inserting the calculated solution of the expansion coefficients in to (\ref{EOMPert}) to obtain the explicit expression of the perturbative EOM. From this we can simply read of the principal symbol. Following further along the lines described in the last chapter we find the following expression for the area metric gravity polynomial:
\begin{align} \label{AreaPoly}
\begin{aligned}
    \mathcal{P}_{Area} &= \bigl[1 - \frac{1}{2} \tilde{C}\eta(H) - \frac{1}{4} \tilde{C} \epsilon(H) + \frac{7}{12} \epsilon(H) \bigr] \eta(k)^{13} - \frac{13}{2}H(k) \eta(k)^{12} + \mathcal{O}(2) \\
    &=\bigl\{  \bigl[ 1 - \frac{1}{2} C \eta(H) - \frac{1}{4} C \epsilon(H) +  \frac{7}{12\cdot13} \epsilon(H) \bigr] \eta(k) - \frac{1}{2} H(k)       \bigr\}^{13} + \mathcal{O}(2).
\end{aligned}
\end{align}
Here $C$ is a constant the depends on the parametes that are left in (\ref{LArea}) and $\Tilde{C} = 13 C$.
There is a slight caveat in the computation of this expression for the area metric gravity polynomial. 
To explain this further first note that due to the fact that there still appear $52$ constant parameters in rather complicated fashion in the area metric gravity Lagrangian it is virtually impossible to derive the principal polynomial without employing efficient computer algebra. Once we have selected a $17 \times 17$ submatrix of the $21 \times 21$ principal symbol matrix we have to compute the two expressions $D_0$ and $D_{1}^C$ in (\ref{polyMatrices}) in order to find the constant and linear order of the expansion of the principal polynomial. Note that the involved matrices no longer depend on $H_A$. To compute $D_0$ and $D_1^A$ we now need the determinant and the inverse of the constant order $T_0$ of the chosen $17 \times 17$ symbol submatrix. This two steps precisely represent the bottleneck of the computation of the polynomial. Although $T_0$ does not depend on $H_A$ it still includes some of the $52$ parameters, namely exactly those that are left in $a^{ApBq}$ and $a^{ABI}$ after solving the equivariance equations and contribute to the principal symbol. We find that these still consist of $7$ parameters. Although this might not sound much computing the determinant or the inverse of a $17 \times 17$ matrix with $7$ symbolic parameters that appear in rather complicated fashion turns out to be surprisingly hard. We tried to tackle this problem by using a fraction free implementation of Gaussian elimination in the computer algebra system Maple\footnote{We expect that one might achieve even superiour performance in computing the determinant by using techniques from multivariat polynomial interpolation as it is for instance suggested in \cite{Qin2018}, \cite{MARCO2004749} and also \cite{articleDet}} \cite{Maple}. Nevertheless with the available resources the program did not terminate in adequate time. Thus the only way we nevertheless were able to obtain an expression for the polynomial was by evaluating the $7$ parameters that appear in $T_0$ at randomly generated integer values and then only computing the principal polynomial with the remaining parameters from the two third order coefficients $a^{ABpCq}$ and $a^{ABCI}$. Taking a closer look at (\ref{polyMatrices}) these parameters only appear in traces not in determinants or inverses. We then proceeded by performing this computation to obtain the principal polynomial several times, each time taking newly obtained random integers for the $7$ parameters in $T_0$. Doing so we observed that they only contribute an overall factor to the principal Polynomial (\ref{AreaPoly}). We have to admit that this was however not done often enough to rigorously interpolate the precise way the prefactor depends on these $7$ parameters. This is however not necessary as we are only interested in the vanishing set of the area metric gravity polynomial. \\

Note that, as we are only interested in the vanishing set of the principal polynomial, we can multiply a given polynomial with a non vanishing scalar density of arbitrary weight. The expansion to linear order of such a density admits the general form
\begin{align}\label{dens}
\omega = 1+ b_1 \cdot (\eta(H) + \frac{1}{2} \epsilon(H)) + \frac{b_2}{12}\epsilon(H) + \mathcal{O}(2),
\end{align}
for arbitrary constants $b_1$ and $b_2$.
Multiplying  (\ref{GLEDPoly}) with such a density we see that perturbatively the GLED polynomial describes the same vanishing set as 
\begin{align} \label{GLEDPoly2}
\begin{aligned}
    \widetilde{\mathcal{P}}_{GLED}(k) = \omega \cdot \mathcal{P}_{GLED} = 
    \bigl\{  \bigl[ 1 - \frac{1}{2} (A-b_1) \eta(H) - \frac{1}{4} (A-b_1) \epsilon(H)) + \\ \frac{1+b_2}{24} \epsilon(H) \bigr] \eta(k)
    -\frac{1}{2} H(k)       \bigr\}^2 + \mathcal{O}(2).
\end{aligned}
\end{align}
Comparing this expression with the previously obtained area metric gravity polynomial (\ref{AreaPoly}) we find that multiplying the GLED polynomial with a density with constants $b_1 = A -C$ and $b_2 = \frac{1}{13}$, the two polynomials are products of the same factor and therefore in particular describe the same vanishing set in $\mathcal{O}(2)$. Thus also for the case of area metric gravity requiring causal compatibility with the GLED mater theory in addition to diffeomorphism invariance at least perturbatively yields no further conditions for the parameters of $\mathcal{L}_{Area,per}$.\\

Finally we wish to investigate if this was in fact a coincidence of the chosen GLED matter theory or a general feature of perturbative are metric gravity independent on the specific matter theory. To this end we  take a closer look at the consequences of the required diffeomorphism invariance on the causal structure of the corresponding area metric gravity EOM by studying general solutions to (\ref{polyEqn}). This equation now takes the following form:
\begin{align}\label{AreaPolyEqn}
    0 = \partial^A \mathcal{P}_{Area}^{p_1...p_26} C_{An}^{Bm} v_B - 26\mathcal{P}_{Area}^{(p_1...p_{25}\vert m} \delta_n^{\vert p_{26})} + 13 \mathcal{P}_{Area}^{p_1...p_21} \delta^m_n.
\end{align}
Proceeding as before for the metric in computing the general power series solution up to linear order in $H_A$ for this equation and contracting it against $k_{p_1} \cdot k_{p_{26}}$ we get the most general linear order expansion of $\mathcal{P}_{Area}$ that is consistent with the required diffeomorphism invariance:
\begin{align}
\begin{aligned}
    \mathcal{P}_{Area} &= a \cdot \bigl \{  \eta(k)^{13} + \tilde{b} \cdot \eta(H) \cdot \eta(k) - \frac{13}{2} \cdot  H(k) \cdot \eta(k)^{12} + (\frac{\tilde{b}}{2}+\frac{14}{24}) \cdot \epsilon(H) \eta(k)^{13}  \bigr \} + \mathcal{O}(2)\\
    &=a \cdot \bigl\{  \bigl[ 1 + b \eta(H) + \frac{b}{2} \epsilon(H) +  \frac{7}{12\cdot13} \epsilon(H) \bigr] \eta(k) - \frac{1}{2} H(k)       \bigr\}^{13} + \mathcal{O}(2),
\end{aligned}
\end{align}
with constants $a$, $b$ and $\tilde{b} = 13b$. It is clear that the overall constant $a$ does not influence the vanishing set. Additionally we can multiply this expression with with the general form of a non vanishing scalar density of arbitrary weight (\ref{dens}) that in linear order allows for $2$ arbitrary parameters $b_1$ and $b_2$. Doing so we find:
\begin{multline}
    \tilde{\mathcal{P}}_{Area} = \bigl\{  \bigl[ 1 + (b+ b_1) \eta(H) + \frac{b+b_1}{2} \epsilon(H) +  (\frac{7}{12\cdot13}+b_2) \epsilon(H) \bigr] \eta(k) - \frac{1}{2} H(k)       \bigr\}^{13} \\
    + \mathcal{O}(2).
\end{multline}
This polynomial now obviously perturbatively describes the same vanishing set. However now we see that we can in fact specify the remaining constant $b$ at wish just by multiplying with an appropriate density and hence in particular without changing the vanishing set. 
Thus the fact that the requirement of causal compatibility between are metric gravity and GLED let to no further condition was no coincidence but a general feature of the perturbative expansion of area metric gravity
We conclude that for the case of the third perturbative area metric gravity Lagrangian already the required diffeomorphism invariance determines the vanishing set of the principal polynomial and thereby the causal structure if the theory uniquely. \\

Hence the construction procedure is completed. The most general meaningful third order expansion of the Lagrangian of area metric gravity thus involves $52$ undetermined paramters. Its explicit expression can be obtained by inserting the relations between the paramters (\ref{AreaSol1}) to (\ref{AreaSol20}) together with the computed Lorentz invariant expansion coefficients in the third order expansion (\ref{LArea}). It is of great importance to be aware of the meaning of this result. This really compromises the most general perturbative, diffeomorphism invariant, second derivative order theory of gravity that is compatible with a linear theory of electrodynamics. As such it is a particular interesting candidate to test against nature, for instance by predicting gravitational wave emission of some known binary system and then comparing this prediction with experiments. It is essential to note that such a prediction of the emission of gravitational waves really requires the third order of the gravitational lagrangian. Although often stated differently his is already true for standard GR (see for instance \cite{1984grra.book.....S} chapter 4.5).

Summing up the results that we developed in this chapter, not only did our perturbative constructiv gravity framework pass its first test in succesfully recovering the third order expansion of the Einstein-Hilbert Lagrangian, with the perturbative expansion of area metric gravity we also have developed the most general perturbative theory of gravity that is consistent with linear Electrodynamics. Although the expansion of such a theory to second order in the Lagrangian has already been computed in \cite{2017arXiv170803870S}, the expansion to third order that we derived here for the first time provides access to the computation of gravitational wave in such a theory of gravity and thereby allows us to concretely test the area metric against measurements. For exactly that reason the computation of the emission of gravitational waves should compromise the main focus of future developements in this area of research. 

\chapter{Computer Algebra}\label{computerAlg}
\dictum{
After providing the reader with a concise introduction to functional programming in Haskell, focusing on the main differences with more traditional imperative programming languages we proceed by illustrating the developed Haskell package sparse-tensor in detail. Not only will we explain the basic functionality contained therein, the following chapters are also intended as a short introduction manual for potential future users. Finally we also clarify how the construction of Lorentz invariant basis tensors is implemented in sparse-tensor. }   

\section{A Concise Introduction to Functional Programming in Haskell}
For the following chapter to constitute a self contained description of the developed computer algebra also for readers that are unfamiliar with the programming language Haskell \cite{Marlow_haskell2010} we now furnish a quick introduction to it. We mainly treat its particularities compared with more traditional imperative languages.
Details can be found in any good textbook as for instance in \cite{Thompson99thecraft}, \cite{bird_2014}, \cite{hutton_2007}. Introductions to Haskell that rather resemble user guides aiming to rapidly allow the reader to write his own programs can be found in  \cite{OSullivan2008} and  \cite{Lipovaca:2011:LYH:2018642}. Much information is also collected in the HaskellWiki \cite{wiki:xxx}.

Compared with traditional imperative programming languages as for instance $C$ the most striking difference of Haskell lies in the fact that it employs a \textbf{\textit{purely functional}} programming paradigm. In imperative languages a computer program consists of a sequence of commands for the computer to execute step by step. Thus not only the individual steps but also the specific order of their execution, control flow, is explicitly provided by the programmer. Typically this is achieved by using control structures such as loops or conditional statements. At each time during its execution the computer program hence can be said to occupy a given state that can be described by the explicit information that is stored in the entirety of its variables. A single computation step then precisely tells the program how to transfer from a given state to the subsequent one. This corresponds to a change in the values of some of the variables that describe the programs state. Apart from invoking the desired modifications of its variables that are necessary for the specific computation additionally such a computer program might undesirably change values of further variables. This is referred to as \textit{\textbf{side effects}}. As a consequence executing a such a computer program multiple times might not return the same result each time as side effects that possibly occur during the execution can alter the outcome. Needless to say that unexpected side effects compromise a broad source of potential errors and are hence best avoided. One way of avoiding side effects is by employing a functional programming paradigm. In contrast to an imperative programming style a computer program no longer consists of a step by step instruction of state changes but is given by a mathematical function that maps some input data to the desired output. The execution of the program then simply corresponds to the evaluation of this function. As such a function obviously only depends on the data it is given as input it is completely free of side effects. In practise one can build a computer program using such a functional programming style by composing it of several individual functions that describe sub parts of the program via usual function composition. Besides of composition in most functional languages functions of multiple variables can also be  \textit{\textbf{curried}} to obtain an equivalent higher order function that only takes a single argument but now yields a function as result. 
\begin{center}
    \mintinline{haskell}{f :: (A,B) -> C} $\ \xrightarrow{ \ currying \ } \ $
    \mintinline{haskell}{f' :: A -> (B -> C)}.
\end{center}
One can hence equivalently represent functions that take multiple arguments by their curried form as functions that only take a single argument but return again a function. In fact in Haskell all functions take precisely one argument, i.e. functions that take several arguments are always represented in curried form. The general form of a function definition in Haskell reads:
\begin{center}
\begin{cminted}{haskell}
f :: A -> (B -> C)
f a b = ...  
\end{cminted}
\end{center}
\mintinline{haskell}{f a b = (f a) b} denotes the evaluation of the function \mintinline{haskell}{f} first on \mintinline{haskell}{a} and then applying the resulting function to \mintinline{haskell}{b}. Note that the function evaluation is left associative.
In the above we already used the second Haskell particularity to some extend, the Haskell \textbf{\textit{type system}}. Every Haskell expression has a type that is denoted by \mintinline{haskell}{expr :: type}. Hence the above function  has the type \mintinline{haskell}{f :: A -> (B -> C)}. Whereas the evaluation of functions behaves left associative the corresponding types are right associative. The type of the above function can thus also be written as \mintinline{haskell}{f :: A -> B -> C}.
New data types can be defined by specifying \textbf{\textbf{data constructors}}, functions that describe how the new data type is constructed in terms of known data types. We can illustrate this by constructing a data type that represents a pair of integers:
\begin{center}
\begin{cminted}{haskell}
data IntPair = IntPair Int Int 
\end{cminted}
\end{center}
Note that the constructor thus by definition defines a function that when applied to two integers yields an expression with type IntPair. When defining functions in Haskell one can \textbf{\textit{pattern match}} against data constructors. In other words one may define the function value in terms of the values that are given to the constructor of the input argument. To provide an example we can define a function that computes the some of the two integers contained in an IntPair:
\begin{center}
\begin{cminted}{haskell}
sumIntPair :: IntPair -> Int 
sumIntPair (IntPair x y) = x + y
\end{cminted}
\end{center}
where we used the infix notation for the addition of integers. 
The special property of the Haskell type system however is that already when compiling Haskell code the compiler checks whether the types of the various function applications contained therein match. If not the program is immediately rejected by the compiler. Thus potential type errors are already avoided at compile time and therefore cannot lead to runtime errors. Languages that employ this form of type checking are also called \textit{\textbf{statically typed}}. Also our developed tensor algebra package intensively relies on the Haskell type system. It is built such that typos that might occur when using it to compute tensorial expressions almost always yield a type error. Especially when dealing with longer computations this is a huge advantage as such typos or similar mistakes are then already detected when compiling the code and not after possibly several hours of runtime.  

Finally the last special feature that Haskell admits is its \textit{\textbf{lazy evaluation}} strategy. Functions are only explicitly evaluated on their arguments once the result is needed. This does not only allow for performance improvements as employing lazy evaluation unnecessary function calls can be avoided it also enables the use of conditional control structures in purely functional style. The usual conditional if statement for instance could simply be given by a function
\begin{center}
\begin{cminted}{haskell}
if' :: Bool -> a -> a -> a
if' False _ y = y 
if' True x _ = x 
\end{cminted}
\end{center}
Note that when the boolean evaluates to \mintinline{haskell}{True} this function simply returns the expression \mintinline{haskell}{x}. Thus when using lazy evaluation in this case \mintinline{haskell}{y} is never evaluated. Similarly \mintinline{haskell}{x} is not evaluated when the boolean is \mintinline{haskell}{False}. 

Besides of the above particularities of Haskell as programming language in order to fully grasp the essence of the algorithms that will be outlined in the following section it is essential to accustom oneself with functions that are heavily used in functional programming languages to implement control flow by means of recursion over data structures. These techniques can be understood best by considering a particular example. Amongst the simplest data structures that can be used for the recursion are list. In Haskell the list data structure is defined as follows 
\begin{center}
\begin{cminted}{haskell}
data [a] = [ ] | a : [a] 
\end{cminted}
\end{center}
Thus it has two constructors \mintinline{haskell}{[ ]} constructing the empty list and \mintinline{haskell}{(:)} appending an element to the front of the list. Note that \mintinline{haskell}{(:)} is written as infix operator.
The first heavily used function that can be used to structure control flow is the \mintinline{haskell}{map}function:
\begin{center}
\begin{cminted}{haskell}
map :: (a -> b) -> [a] -> [b]
map f [] = [] 
map f (x:xs) =  f x : map f xs 
\end{cminted}
\end{center}
Hence the map function takes a function and a list and applies he function to each element of a list. The second important concept is the fold operators \mintinline{haskell}{foldl} and \mintinline{haskell}{foldr}. It exists in several versions. The main idea consists of taking a binary function a start argument and a list of additional arguments and then successively reducing the list by first applying the function to the start argument and the first argument of the list and then proceeding by evaluating the function on the thus obtained result and the next list element. One can distinguish folds by whether they reduce the list starting from its first (left fold) or its last (right fold) value. A left fold operator can be implement in Haskell according to:
\begin{center}
\begin{cminted}{haskell}
foldl :: (a -> b -> a) -> a -> [b] -> a 
foldl f x [] = x 
foldl f x (y:ys) = foldl f (f x y) ys
\end{cminted}
\end{center}
Obviously there exist much more such functions that allow each particular problem at hand to be treated in a suitable way using a purely functional programming paradigm. Additional many of these concepts can also be defined for data structures more general than lists. Details can be found in the provided literature.

\section{sparse-tensor: A Haskell Package for Sparse Abstract Tensor Calculus}
With the short introduction to Haskell at hand we now explain the main ideas that underlie the developed computer program sparse-tensor. Ultimately the aim consists of solving the perturbative equivariance equations (\ref{order1}), \ref{order2} and \ref{order3}. Recall that these three systems are merely compromised of linear equations. In principal it should thus be straight forward to re-express these in suitable matrix from and use standard matrix computer algebra to solve them. Practically however extracting the appropriate matrices from the systems is a different question as for doing so one would need to evaluate the tensorial expressions in all free indices and to that end also
explicitly express all occurring contractions and symmetrizations. On the other hand existing tensor algebra systems (such as Cadabra (see \cite{cadabra1} and \cite{cadabra2}) and the Physics package currently provided by the computer algebra system Maple to name a few) are mostly tailored towards a symbolic treatment of tensors and thus not entirely suited for our specific purpose. 
This can be seen from the observation that except for parameters that label the independent Lorentz invariant expressions in the expansion coefficients and thus occur linearly in the perturbative equivariance equations all remaining expressions only contain rational numbers. In particular the components of the tensors involved in these equations are explicitly known. Besides of that when solving the perturbative equivariance equations we need to evaluate the tensorial equation for all its free indices and thus the explicit components are also all needed. Hence in fact we never have to treat the occurring tensors symbolically as abstract objects but can think of them as containers that are simply used to store values in a particular way. 
Furthermore note that the use of abstract indices $I, A, ...$ leads to a reduction in the computation costs. For instance contracting two blocks of $4$ indices each that both admit the symmetry of an area metric tensor against each other requires the summation of $4^4 = 256$ expressions. On the other hand introducing an abstract index $A = 0,...,20$ that runs over the independent components such a are metric block contains the contraction only requires the summation of $21$ expressions. Finally we can additionally derive a benefit from the fact that the tensors that occur in the treatment of the perturbative equivariance equations are to a large extent only sparsely occupied. Providing an example the area metric intertwiner with components displayed in (\ref{AreaI}) only contains $144$ non vanishing components out of a total of $21 \cdot 4^4 = 5376$ total components which admits to an occupation level of around $2.68 \%$.
Summing up we want to incorporate the following functionallity in our computer program:
\begin{itemize}
    \item Treatment of tensor with multiple abstract indices each running over an individual index range. 
    \item Sparse storage of tensor components.
    \item Optimization towards computing explicit component operations over abstract algebraic manipulation of tensors.
\end{itemize}
Finally we are going to encode the generalized rank of a given tensor --- for standard tensors described by contravariant and covariant indices the rank is expressed as pair ($\#$ of contravariant indices, $\#$ of covariant indices), consequently we express the rank of a tensor that takes $n$ contravariant and $n$ covariant indices as the corresponding $2n$-tuple of numbers of indices --- directly in its type. Doing so Haskell's type system guarantees a prevention of possible errors that might for instance occur when by mistake adding tensors of different ranks. The detection of such errors then already happens during compile time. We start with the datatype of a tensor of arbitrary rank that only takes a single index type. Using Haskell's generalised algebraic data types, in short \textit{\textbf{GADT}}s the data type definition of such a single index tensor looks as follows: 

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG
]{haskell}
data Tensor n k v where 
    Scalar :: v -> Tensor 0 k v 
    Tensor :: TMap k (Tensor n k v) -> Tensor (n+1) k v
    ZeroTensor :: Tensor n k v
\end{minted} 
\end{samepage}

Hence the tensor data type \mintinline{haskell}{Tensor n k v} contains additional type information consisting of a type level natural number \mintinline{haskell}{n} representing the rank of the tensor and types \mintinline{haskell}{k} and \mintinline{haskell}{v} that encode the type of index the tensor takes and the type of values it stores. The data type then provides three constructors: \mintinline{haskell}{Scalar} constructs a rank 0 tensor, i.e. a scalar out of a given value, \mintinline{haskell}{Tensor} takes an ordered list of (index, subtensor) pairs \mintinline{haskell}{type TMap k v = [(k,v)]}, where the subtensors have rank \mintinline{haskell}{n} and constructs from it a \mintinline{haskell}{Tensor (n+1) k v} with rank \mintinline{haskell}{n+1} and \mintinline{haskell}{ZeroTensor} simply represents a tensors of arbitrary rank with all components being identical zero. Hence a tensor is represented as ordered forest\footnote{A colletion of disjoint trees.} with additional type information regarding rank and stored values.
Thus to retrieve a value of a tensor of rank \mintinline{haskell}{n} we need to specify \mintinline{haskell}{n} values of the appropriate type \mintinline{haskell}{k}. It is important that the distinction between the individual indices is not provided by some abstract labels that are attached to them but simply by their position. For instance figure \ref{ExampleTens} displays the forest structure of a rank $2$ tensor that uses \mintinline{haskell}{Ind3} types, i.e. spacetime indices with potential values between $0$ and $3$ as index type and stores rational numbers as values.
\begin{figure}[ht]
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG, very thick, minimum size=7mm},]
\node[roundnode]  (I1) at (0,0) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (I2) at (0,-3) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (I3) at (0,-7) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J1) at (4,1) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (J2) at (4,-1) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J3) at (4,-3) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J4) at (4,-5) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J5) at (4,-7) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (J6) at (4,-9) {\mintinline{haskell}{Ind3 3}};
\node  (K1) at (8,1) {\mintinline{haskell}{Scalar 1 % 2}};
\node  (K2) at (8,-1) {\mintinline{haskell}{Scalar 3}};
\node  (K3) at (8,-3) {\mintinline{haskell}{Scalar -17}};
\node  (K4) at (8,-5) {\mintinline{haskell}{Scalar 1 % 3}};
\node  (K5) at (8,-7) {\mintinline{haskell}{Scalar 1}};
\node  (K6) at (8,-9) {\mintinline{haskell}{Scalar 1 % 2}};


\draw [-] (I1) -- (J1);
\draw [-] (I1) -- (J2);
\draw [-] (I2) -- (J3);
\draw [-] (I3) -- (J4);
\draw [-] (I3) -- (J5);
\draw [-] (I3) -- (J6);
\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);


\end{tikzpicture}
\caption{Forest structure of rank 2 tensor with spacetime indices and rational values.}
\label{ExampleTens}
\end{figure}
 Providing the non vanishing components of the displayed tensor forest explicitly the tensor would read:
\begin{align}
\begin{alignedat}{3}
T^{01} &= \frac{1}{2}, \ \  &  \ \ T^{02} &= 3, \ \  & \ \ T^{22} &= -17,\\
T^{30} &= \frac{1}{3}, & T^{31} &= 1, & T^{33} &= \frac{1}{2}.
\end{alignedat}
\end{align}
In order for tensors represented this way to satisfy the usual tensor algebra some restrictions on the possibly stored values are necessary. In particular we need to be able to add, subtract and scale the stored values as this is necessary when adding subtracting or scaling tensors. Thus we collect types that allow for these operations in a \textit{\textbf{type class}}:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
class (Eq a) => TScalar a where 
        addS :: a -> a -> a 
        subS :: a -> a -> a
        scaleS :: Rational -> a -> a 
        scaleZero :: a
\end{minted} 
\end{samepage}

Furthermore when forming the product of two tensors we also need to compute the product of the values that are stored therein. This requirement os encoded in the additional type class:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
class TAlgebra v v' where 
        type TAlg v v' :: * 
        prodA :: v -> v' -> TAlg v v'
\end{minted} 
\end{samepage}

Note that the above type class not only requires the existence of a function \mintinline{haskell}{prodA} that computes the products of any two stored values of types \mintinline{haskell}{v} and \mintinline{haskell}{v'}, it also requires its instances to provide a type level function that computes the new type of such a product \mintinline{haskell}{TAlg v v'} once the types \mintinline{haskell}{v} and \mintinline{haskell}{v'} are specified. Restricting to these type classes we can define the usual tensor algebra operations. We start with the addition:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
(&+) :: (TIndex k, TScalar v) => Tensor n k v -> Tensor n k v -> 
                                 Tensor n k v 
(&+) (Scalar a) (Scalar b) = Scalar $ addS a b 
(&+) (Tensor m1) (Tensor m2) = Tensor $ addTMaps (&+) m1 m2     
(&+) t1 ZeroTensor = t1
(&+) ZeroTensor t2 = t2 
\end{minted} 
\end{samepage}

Here \mintinline{haskell}{addTMaps} adds the two pair lists of the two tensors with a combiner function, i.e. it combines the two pair lists ensuring the order of the indices is still valid, if an index is present in both lists the corresponding subtensor is provided by the combiner function. In the case above the combiner function is again the addition of tensors, this time however only applied to the subtensors. 

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
addTMaps :: (Ord k) => (v -> v -> v) -> TMap k v -> TMap k v ->
                                        TMap k v 
addTMaps f m1 [] = m1 
addTMaps f [] m2 = m2 
addTMaps f ((k1,v1):xs) ((k2,v2):ys) 
            | k1 < k2 = (k1,v1) : (addTMaps f xs ((k2,v2):ys))
            | k2 < k1 = (k2,v2) : (addTMaps f ((k1,v1):xs) ys)
            | k1 == k2 = (k1, f v1 v2) : (addTMaps f xs ys) 
\end{minted} 
\end{samepage}

Scalar multiplication and subtraction can now be implemented straight forwardly.

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
(&.) :: (TIndex k, TScalar v) => Rational -> Tensor n k v ->
                                 Tensor n k v 
(&.) scalar = fmap (scaleS scalar)

(&-) :: (TIndex k, TScalar v) => Tensor n k v -> Tensor n k v ->
                                 Tensor n k v
(&-) t1 t2 = t1 &+ (-1) &. t2 
\end{minted} 
\end{samepage}

Where \mintinline{haskell}{fmap} is provided by the functor instance of the tensor data type:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
instance Functor (Tensor n k) where 
        fmap f (Scalar x) = Scalar (f x)
        fmap f (Tensor m) = Tensor (mapTMap (fmap f) m)
        fmap f ZeroTensor = ZeroTensor 
\end{minted} 
\end{samepage}

The product of two tensors can be implemented by appending the second tensor to each of the leafs of the first tensor that is specified:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
(&*) :: (TIndex k, TAlgebra v v', TScalar v, TScalar v',
        TScalar (TAlg v v'))
        => Tensor n k v -> Tensor m k v' -> Tensor (n+m) k (TAlg v v') 
(&*) (Scalar x) (Scalar y) = let newVal = prodA x y 
                                 in if newVal == scaleZero
                                        then ZeroTensor 
                                        else Scalar newVal 
(&*) (Scalar x) t2 = fmap (prodA x) t2 
(&*) (Tensor m) t2 = Tensor $ mapTMap (&* t2) m 
(&*) t1 ZeroTensor = ZeroTensor 
(&*) ZeroTensor t2 = ZeroTensor
\end{minted} 
\end{samepage}

Here \mintinline{haskell}{mapTMap} maps a function over the values of the key value pairs \mintinline{haskell}{(k,v)} stored in the \mintinline{haskell}{TMap k v}.
Note that not only the rank of the resulting tensor depends on the ranks of the two input tensors, also the types that are stored in it as values clearly depend on the respective types of the intput tensors. This is exactly where the second typeclass \mintinline{haskell}{TAlgebra v v'} comes to use. 

Finally when symmetrizing a given tensor we need to be able to swap the position of some of the tensors indices. This is done by the following function:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
tensorTrans :: (TIndex k, TScalar v) => (Int,Int) ->
               Tensor n k v -> Tensor n k v
tensorTrans (0, j) t = fromListT l
                    where 
                        l = map (\(x,y) -> (swapHead j x, y)) $ toListT t
tensorTrans (i, j) (Tensor m) = Tensor $ 
                                mapTMap (tensorTrans (i-1, j-1)) m 
tensorTrans (i ,j) ZeroTensor = ZeroTensor
\end{minted} 
\end{samepage}

The two indices w.r.t. which the tensor is transposed are provided by the integer tuple, where the indices are labeled w.r.t. their position starting from 0. The function first traverses the tensor until the level\footnote{We will call the distance of a given node to the root node of the appropriate tree the level of the node.} specified by the first, i.e. the smaller integer is reached. Then all corresponding sub tensors are flattened to indices value pairs. This is precisely what \mintinline{haskell}{toListT} does. Now in each of the thus obtained indices value lists of the various sub tensors by using the function \mintinline{haskell}{swapHead} the first and the j-th entry of the indices are swaped. Finally by using  \mintinline{haskell}{fromListT} the indices value lists are again transformed to a tensor.  Applying this function to the previously provided rank 2 example tensor with the integer pair given as \mintinline{haskell}{(0,1)} we obtain the following transposed rank 2 tensor:
\begin{figure}[ht]
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG, very thick, minimum size=7mm},]
\node[roundnode]  (I1) at (0,0) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (I2) at (0,-2) {\mintinline{haskell}{Ind3 1}};
\node[roundnode]  (I3) at (0,-4) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (I4) at (0,-6) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J1) at (4,0) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J2) at (4,-1) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J3) at (4,-2.5) {\mintinline{haskell}{Ind3 3}};
\node[roundnode]  (J4) at (4,-3.5) {\mintinline{haskell}{Ind3 0}};
\node[roundnode]  (J5) at (4,-5) {\mintinline{haskell}{Ind3 2}};
\node[roundnode]  (J6) at (4,-6) {\mintinline{haskell}{Ind3 3}};

\node  (K1) at (8,0) {\mintinline{haskell}{Scalar 1%3}};
\node  (K2) at (8,-1) {\mintinline{haskell}{Scalar 1%2}};
\node  (K3) at (8,-2.5) {\mintinline{haskell}{Scalar 1}};
\node  (K4) at (8,-3.5) {\mintinline{haskell}{Scalar 3}};
\node  (K5) at (8,-5) {\mintinline{haskell}{Scalar -17}};
\node  (K6) at (8,-6) {\mintinline{haskell}{Scalar 1%2}};


\draw [-] (I1) -- (J1);
\draw [-] (I2) -- (J2);
\draw [-] (I2) -- (J3);
\draw [-] (I3) -- (J4);
\draw [-] (I3) -- (J5);
\draw [-] (I4) -- (J6);
\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);


\end{tikzpicture}
\caption{Forest structure of the rank 2 tensor obtained when applying \mintinline{haskell}{tensorTrans (0,1)} on the former example.}
\label{ExampleTensTrans}
\end{figure}
similar to the transposition of to indices we can now obviously construct functions that transpose a given tensor in several indices or even completely reorder the positions of the various indices.  For instance a function that completely reorders the indices of a tensor can be obtained as follows

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
resortTens :: (SingI n, TIndex k, TScalar v) => [Int] -> Tensor n k v ->
              Tensor n k v 
resortTens perm t = fromListT $ map (\(x,y) -> (resortInd perm x, y))
                    $ toListT t 
\end{minted} 
\end{samepage}

The function takes as argument a list of integers where the i-th elements specifies the position on which the i-th index in the tensor shall be sorted, flattens the tensor to a list of indices value pairs, resorts the indices according to this and combines the pairs again to obtain a tensor. 
Knowing how we can transpose and resort the indices of a given tensor we can easily construct arbitrary symmetrization functions, simply by rearranging the indices of a given tensor accordingly and then adding this newly obtained tensor to the previous one. We only provide the example of the standard symmetrization w.r.t. the exchange of too individual indices explicitly:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
symTensFac :: (TIndex k, TScalar v) => (Int,Int) -> Tensor n k v ->
              Tensor n k v 
symTensFac inds t = (1%2) &. symTens inds t
\end{minted} 
\end{samepage}

Any other symmetrization can be constructed in similar ways.
Of course we can also evaluate a given tensor by inserting a particular value for one of its indices. This can be achieved as follows:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
evalTens :: (SingI (n+1), TIndex k, TScalar v) => Int -> k ->
            Tensor (n+1) k v -> Tensor n k v 
evalTens ind indVal (Tensor m)
        | ind > size -1 || ind < 0 = error "wrong index to evaluate" 
        | ind == 0 = fromMaybe ZeroTensor $ lookup indVal m
        | otherwise = fromMaybe ZeroTensor $
                      lookup indVal (getTensorMap newTens)
        where 
            size = length $ fst $ head $ toListShow (Tensor m)
            l = [1..ind] ++ 0 : [ind+1..size -1]
            newTens = resortTens l (Tensor m)
\end{minted} 
\end{samepage}

If the tensor is to be evaluated for its 0-th index we simply look up the corresponding value in the top level of the forest. In any other case we first shift the corresponding level of the forest that is to be evaluated to the front to then again proceed as described in the prior case.\\

Up to now we only treated tensors with a single type of indices, for instance in the example of figure \ref{ExampleTens} the tensor only had contravariant spacetime indices. We can however easily generalize the above such that also covariant indices can be incorporated. This is simply achieved by additionally appending a tensor of the given index type as values
to the contravariant tensor.
\mintinline{haskell}{Tensor n k v} itself is instance of \mintinline{haskell}{TScalar} and \mintinline{haskell}{TAlgebra}:
\begin{center}
\begin{cminted}{haskell}
type Tensor2 n1 n2 k v = Tensor n1 k (Tensor n2 k v)
\end{cminted}
\end{center}
Thus \mintinline{haskell}{Tensor2 n1 n2 k v} describes as before a contravariant tensor of rank \mintinline{haskell}{n1}, but this time with values being each provided by an additional tensor that encodes additional covariant indices. In particular the the two index types are the same as of course the additional covariant indices run over the same index range as the contravariant counterparts. 
Note that in the definition of most of the above functions the types that tensors might store as values where constrained to be instances of \mintinline{haskell}{TScalar} and \mintinline{haskell}{TAlgebra}. 
In order to be able to also us these functions for  \mintinline{haskell}{Tensor2}, i.e. for the case where the stored values are itself tenors it is necessary that the tensor type satisfies these two instances.
\mintinline{haskell}{Tensor n k v} itself is instance of \mintinline{haskell}{TScalar} and \mintinline{haskell}{TAlgebra}:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
instance (TIndex k, TScalar v) => TScalar (Tensor n k v) where
    addS = (&+)
    subS = (&-)
    scaleS = (&.)
    scaleZero = ZeroTensor

instance (TIndex k, TAlgebra v v', TScalar v, TScalar v', 
         TScalar (TAlg v v')) 
          => TAlgebra (Tensor n k v) (Tensor m k v') where 
    type TAlg (Tensor n k v) (Tensor m k v') = Tensor (n+m) k (TAlg v v')
    prodA = (&*)
\end{minted} 
\end{samepage}

The contraction of such a \mintinline{haskell}{Tensor2} in one contravariant and one covariant index can be achieved as follows:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
tensorContr :: (TIndex k, TScalar v) => (Int,Int) -> Tensor2 n1 n2 k v ->
               Tensor2 (n1-1) (n2-1) k v 
tensorContr (0,j) t = fromListT tensList 
    where
        l = map (\(x,y) -> (x, toListT y)) $ toListT t
        l2 = map (\(x,y) -> (tailInd x,mapMaybe (removeContractionInd j
            (headInd x)) y)) l
        l3 = filter (\(_,y) -> not (null y)) l2 
        tensList = map (\(x,y) -> (x, fromListT y)) l3
tensorContr (i,j) (Tensor m) = Tensor $ mapTMap (tensorContr (i-1,j)) m
tensorContr inds ZeroTensor = ZeroTensor 
tensorContr inds (Scalar s) = error "cannot contract scalar!"
\end{minted} 
\end{samepage}

We specify which indices are to be contracted by an integer pair of the corresponding positions. If the first integer representing the position of the  contravariant index that shall be contracted is not zero we traverse the tensor forest until we reach the appropriate level. Next the remaining sub tensors are all flattened to indices value pairs. We filter those entries that admit equal values for the two indices that are to be contracted. Then these two indices are removed such that the pairs with same value of the two contraction indices now all have the same indices. This is all done by the function \mintinline{haskell}{removeContractionInd}. Finally we reconstruct the tensor ensuring that the values of these pairs with same indices are summed up. \\

Note that with this framework there are no limitations regarding the number of indices that one might possibly use. For instance we can now easily construct a type for tensors that are described by two different types of indices each one appearing in contravariant and covariant fashion by appending the two different \mintinline{haskell}{Tensor2} types:
\begin{center}
\begin{cminted}{haskell}
type AbsTensor4 n1 n2 n3 n4 k1 k2 v = AbsTensor2 n1 n2 k1 
                                     (Tensor2 n3 n4 k2 v)
\end{cminted}
\end{center}
where \mintinline{haskell}{AbsTensor2 n1 n2 k v = Tensor2 n1 n2 k v} is simply a type synonym. It is important to note that as the tensor type itself is an instance of \mintinline{haskell}{TScalar} and \mintinline{haskell}{TAlgebra} the tensor algebra operations \mintinline{haskell}{(&+),(&-),(&.),(&*)} are always the same no matter how many different indices the tensor at hand uses. Furthermore also the symmetrization, transposition ans the contract still works exactly the same the only thing that one additionally needs to specify now is at which level the function should be applied. This can be done by noting that \mintinline{haskell}{fmap} takes a function and applies it to the leaves of a given tensor. Thus applying fmap succesively several times we can decent a fixed number of tensor levels in the forest. This can be used to apply functions to tensors that are stored as leaves of other tensors. We provide the following example of the function that precisely decents $3$ tensor levels:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
mapTo3 :: (TScalar v1, TScalar v2, TIndex k1, TIndex k2) => (v1 -> v2) ->
          AbsTensor3 n1 n2 n3 k1 k2 v1 -> AbsTensor3 n1 n2 n3 k1 k2 v2 
mapTo3 f = fmap (fmap (fmap f))
\end{minted} 
\end{samepage}

Using this we can for instance symmetrize a given tensor in the 4-th type of indices it stores as follows:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch = 1.2, bgcolor=LG]{haskell}
symATens5 :: (TIndex k1, TIndex k2, TIndex k3, TScalar v) =>
                 (Int,Int) ->
                 AbsTensor5 n1 n2 n3 n4 n5 k1 k2 k3 v ->
                 AbsTensor5 n1 n2 n3 n4 n5 k1 k2 k3 v
symATens5 inds = mapTo4 (symTens inds) 
\end{minted} 
\end{samepage}

Similarly all other functions that we encountered so far can be defined for tensors of arbitrarily many different indices as well.
This completes the basic functionality that is provided by the sparse-tensor Haskell library. Further details can be found in (Cite own hackage here !!!)
\section{Generation of Lorentz Invariant Basis Tensors}\label{LorentzGen}
Amongst the entire functionallity provided by the developed Haskell library the generation of Lorentz invariant basis tensors requires the most careful discussion. We have already seen that any such Lorentz invariant tensor must be constructed solely from the Minkowski metric and the Levi-Civita symbol. For the following discussion we restrict to the case where the Lorentz invariant tensor only possesses contravarinat indices other cases can obviously be treated analogously. Any general such Lorentz invariant tensor is thus given by a linear combination of sums of products that are formed from $\eta^{ab}$ and $\epsilon^{abcd}$.
We will simply call the individual terms of such a linear combination \textit{\textbf{ansätze}}. Note that each ansatz necessarily features the same symmetry that is required from the Lorentz invariant tensor. The number of factors in the individual products that are included in the various ansätze is obviously solely determined by the required rank of the Lorentz invariant tensor.
Due to the well known identity $\epsilon^{abcd}\epsilon^{efgh} = 24 \eta^{[a\vert e}\eta^{\vert b \vert f}\eta^{\vert c \vert g}\eta^{\vert d] h}$ we only need to treat the two cases where in the individual products either no $\epsilon^{abcd}$ is present or precisely one $\epsilon^{abcd}$ is included, any other case then reduces to one of these as we can use the identity to pairwise eliminate Levi-Civita symbols by means of Minkowski metrics. 

As we do not only want to construct the most general Lorentz invariant tensor possible by means of including all possible ansätze in the linear combination but also want the individual ansätze to form a basis for the space of such Lorentz invariant tensors of given rank and symmetry we obviously need to get rid of linear dependencies amongst the individual ansätze. Clearly a set of ansätze is linear dependent if we find a non trivial linear combinations of them that yielsd zero. 
There is however an additional dimension dependent mechanism that generates further linear dependencies between ansätze that are at the first place not as obvious. Such additional linear dependencies might occur for instance if due to the involved required symmetry of the ansätze we might end up with linear combinations of ansätze that are not strictly zero but yield an expression that is totally antisymmetric in $5$ or more of its indices. When working in $4$ spacetime dimension such an expression evaluates to zero on all possible index combinations. Thus the ansätze in consideration are in fact linear dependent, although at first glance they might not seem so. In order to distinguish such additional linear dependencies that are generated in this fashion, from the former case we introduce the following terminology: We call a set of ansätze \textit{\textbf{algebraically linear dependent}} if there exits a non trivial linear combination of these that yields identical zero. In particular when investigating algebraic linear dependencies there is no need to evaluate the the components of the expressions explicitly. Algebraic linear dependency is therefore completely independent of the given dimension that we work in. We call it \textbf{\textit{numerically linear dependent}} if there exist a non trivial linear combination that vanishes when evaluated on all possible index combinations. This no clearly is a dimension dependent notion as for instance an expression that is totally antisymmetric in $5$ indices evaluates to zero on all possible index combinations when working in $4$ dimension, it however does not so if working in $5$ dimensions or higher. Obviously every algebraically linear dependent set of ansätze is also numerically linear dependent. The converse is however clearly not true.

The above considerations are best seen when working out a particular example: We take the two expressions $\epsilon^{abcd} \eta^{pq}$ and $\epsilon^{abcp} \eta^{dq}$ and symmetrize s.t. the expressions admit the area metric symmetry in $abcd$, i.e. are antisymmetric in $ab$ and $cd$ and additionally obey the block symmetry $(ab) \leftrightarrow (cd)$. Doing so we get the two ansätze: 
\begin{itemize}
\item[(i)] $\epsilon^{abcd} \eta^{pq}$ 
\item[(ii)] $\epsilon^{abcp} \eta^{dq} - \epsilon^{abdp} \eta^{cq} + \epsilon^{cdap} \eta^{bq} - \epsilon^{cdbp} \eta^{aq}$.
\end{itemize}
Clearly these two ansätze are not algebraically linear dependent. Subtracting the second ansatz form the first one we nevertheless find that the result is totally anti symmetric in the indices $abcdp$ and thus evaluated for any possible index combination yields zero. Thus the two expressions are numerically linear dependent. Summing up, if the goal not only consists of computing the most general Lorentz invariant tensor of given symmetry but if the individual building blocks the ansätze are further required to be linear independent it does not suffice to take into account algebraic linear dependencies but we also have to ensure that the ansätze are numerically linear independent.  \\

In order to achieve performance it is essential to pick data structures for representing a linear combination of ansätze. As usual the data structures must be tailored towards the specific needs that are in this case the symmetrization and evaluation of linear combinations of ansätze. 
The individual tensors $\eta^{ab}$ and $\epsilon^{abcd}$ can simply be represented as follows:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
data Epsilon = Epsilon {-# UNPACK #-} !Int {-# UNPACK #-} !Int
               {-# UNPACK #-} !Int {-# UNPACK #-} !Int
               deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData)

data Eta = Eta {-# UNPACK #-} !Int {-# UNPACK #-} !Int 
           deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData)

data Var = Var {-# UNPACK #-} !Int {-# UNPACK #-} !Int 
           deriving (Show, Read, Eq, Ord, Generic, Serialize, NFData )
\end{minted} 
\end{samepage}

Note that the integer values of the the data types \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon} are used to label the abstract spacetime indices, i.e. the first index will simply be labels by $1$ the second index by $2$, etc. in particular they do not refer to values that these indices might admit. Furthermore we also defined a basic variable data type to encode the parameters that will later multiply the individual ansätze. The \mintinline{haskell}{Var}
data type simply takes two integer values where the first one refers to an integer factor that multiplies the variable\footnote{When using factor less symmetrization it actually turns out that it suffices to use integers for representing the factors in front of the different variables.} and the second one provides an identifier that labels the different variables. 
Using these data types we can encode a general linear combination of ansätze as:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
data AnsatzForestEta = ForestEta (M.Map Eta AnsatzForestEta)| Leaf !Var
                       | EmptyForest 
                       deriving (Show, Read, Eq, Generic, Serialize)

type AnsatzForestEpsilon = M.Map Epsilon AnsatzForestEta
\end{minted} 
\end{samepage}

Here \mintinline{haskell}{AnsatzForestEta} is the datatype of an expression that solely involves $\eta^{ab}$ whereas \mintinline{haskell}{AnsatzForestEpsilon} involves exactly one $\epsilon^{abcd}$ in each product.
Clearly there exist not algebraic linear dependencies between ansätze that involve an $\epsilon^{abcd}$ and those that do not. 
It is important to note that there also cannot exist numeric linear dependencies that mix the two types of ansätze. The reason for this is that for a product of $\eta^{ab}$ to yield non vanishing contributions when evaluated on a list of values from $\{0,1,2,3 \}$ for the individual indices each value must occur an even number of times as the individual $\eta^{ab}$ factor have only diagonal entries. In contrast to that evaluating a product of several Minkowski metrics and one $\epsilon^{abcd}$ one additionally needs each possible value for a spacetime index precisely once such that the Levi-Civita symbol yields a non vanishing contribution. Thus for the types of ansätze that include $\epsilon^{abcd}$ to yield non zero each value for the indices must occur an odd number of times. In total this shows that whenever the an ansatz that features no Levi-Civita symbol evaluates to a non zero contribution all possible ansätze that incorporate an $\epsilon^{abcd}$ evaluate to zero and vice versa. Thus the different types of ansätze are also mutually numerically linear independent. In particular we see that the problem of finding a list of ansätze that constitute a basis for the space of Lorentz invariant tensors of given rank and symmetry decouples into the two sub problems of finding those that incorporate an $\epsilon^{abcd}$ and those that do not. The two types of ansätze can hence be treated completely independently. To that end we also chose to represent them using different data types.

The data type \mintinline{haskell}{Map k v} represents a finite Map between keys and values that is internally implemented as binary tree\footnote{Details regarding the implementation can be found in \cite{adams_1993}. The \mintinline{haskell}{Map k v} \cite{HackageMap} and many more datatypes can be found in Haskell's central package archive \cite{Hackage}.}. Thus the \mintinline{haskell}{AnsatzForestEta} data type is a forest with nodes being provided by an \mintinline{haskell}{Eta} value and leafs given by a \mintinline{haskell}{Var} value. Similarly the \mintinline{haskell}{AnsatzForestEpsilon} type is given by such a forest with the difference that compared to \mintinline{haskell}{AnsatzForestEta} now the first level nodes are occupied by \mintinline{haskell}{Epsilon} values. Note that the individual sibling sets\footnote{We call a collection of nodes that have a common parent node a sibling set.} of these forest themselves are not provided by a "linear" data structure s.t. lists but by the binary structure of the \mintinline{haskell}{Map k v}. This enhances performance as some of the ansatz forests that we will treat have up to several thousand elements in a single sibling set and the binary structure allows for faster insertion and lookup of elements. Such a binary structure for instance features $\mathcal{O}(\mathrm{log}(n)$ lookup whereas a single linked list in the worst case only provides $\mathcal{O}(n)$ lookup. 
Furthermore when constructing an \mintinline{haskell}{AnsatzForestEta} we will always ensure that parent nodes are smaller than all of their children, where the order relation is obtained by comparing the individual integers that are contained in a value of type \mintinline{haskell}{Eta}. All further functions will always produce such sorted forests when evaluated on sorted forests as input. 
Finally the forest structure is also particularly suited for evaluating a given linear combination of ansätze on specific index values. note that only 4 out of 16 possible index value pairs yield a non zero contribution to an $\eta^{ab}$ and only 24 out of 256 4 tuples contribute to an $\epsilon^{abcd}$. Thus when explicitly evaluating an ansatz forest on specific index values a large number of nodes will actually evaluate to zero. Further note that the forest structure roughly speaking represents a fully factored product. Thus whenever a given node in the forest evaluates to zero we do not have to evaluate the subforest that is attached to it as the whole expression is then multiplied by zero anyway. This observation allows for very fast evaluation of ansatz forest when implemented by using a tree bases data structure. 

To provide an example of the data types that are used to encode such expressions we consider the following linear combination of ansätze:
\begin{multline}\label{AnsatzExprEx}
3a_1 \cdot \left (\eta^{ab}\eta^{cd}\eta^{ef} + \eta^{ab}\eta^{ce}\eta^{df} + \eta^{ab}\eta^{cf}\eta^{de} \right ) + a_2 \cdot \left ( \eta^{ac} \eta^{bd} \eta^{ef} + \eta^{ac} \eta^{be} \eta^{df} -2 \eta^{ad} \eta^{be} \eta^{cf} \right ) \\
+ a_3 \cdot \left ( \eta^{ad} \eta^{bc} \eta^{ef} - \eta^{ad} \eta^{bf} \eta^{ce} \right ) + a_4 \cdot \left ( \epsilon^{abcd} \eta^{ef} + \epsilon^{abce} \eta^{df}  \right )   .
\end{multline}
The corresponding representation using the \mintinline{haskell}{AnsatzForestEta} and \mintinline{haskell}{AnsatzForestEta} data types can be seen in figure \ref{AnsatzExprExForest}.
\begin{figure}
\centering
\begin{tikzpicture}[roundnode/.style={rectangle, draw=black, fill=LG, very thick, minimum size=7mm},]
\node  (I0) at (0,2) {\large{\mintinline{haskell}{AnsatzForestEta}}};
\node[roundnode]  (I1) at (0,-2) {\mintinline{haskell}{Eta 1 2}};
\node[roundnode]  (I2) at (0,-5) {\mintinline{haskell}{Eta 1 3}};
\node[roundnode]  (I3) at (0,-8) {\mintinline{haskell}{Eta 1 4}};

\node  (I02) at (0,-13) {\large{\mintinline{haskell}{AnsatzForestEpsilon}}};
\node[roundnode]  (I4) at (1,-14.5) {\mintinline{haskell}{Epsilon 1 2 3 4}};
\node[roundnode]  (I5) at (1,-16.5) {\mintinline{haskell}{Epsilon 1 2 3 5}};


\node[roundnode]  (J1) at (4,2) {\mintinline{haskell}{Eta 3 4}};
\node[roundnode]  (J2) at (4,0) {\mintinline{haskell}{Eta 3 5}};
\node[roundnode]  (J3) at (4,-2) {\mintinline{haskell}{Eta 3 6}};
\node[roundnode]  (J4) at (4,-4) {\mintinline{haskell}{Eta 2 4}};
\node[roundnode]  (J5) at (4,-6) {\mintinline{haskell}{Eta 2 5}};
\node[roundnode]  (J6) at (4,-8) {\mintinline{haskell}{Eta 2 3}};
\node[roundnode]  (J7) at (4,-10) {\mintinline{haskell}{Eta 2 5}};
\node[roundnode]  (J8) at (4,-12) {\mintinline{haskell}{Eta 2 6}};
\node[roundnode]  (J9) at (8,-14.5) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (J10) at (8,-16.5) {\mintinline{haskell}{Eta 4 6}};


\node[roundnode]  (K1) at (8,2) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K2) at (8,0) {\mintinline{haskell}{Eta 4 6}};
\node[roundnode]  (K3) at (8,-2) {\mintinline{haskell}{Eta 4 5}};
\node[roundnode]  (K4) at (8,-4) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K5) at (8,-6) {\mintinline{haskell}{Eta 4 6}};
\node[roundnode]  (K6) at (8,-8) {\mintinline{haskell}{Eta 5 6}};
\node[roundnode]  (K7) at (8,-10) {\mintinline{haskell}{Eta 3 6}};
\node[roundnode]  (K8) at (8,-12) {\mintinline{haskell}{Eta 3 5}};


\node  (L1) at (12,2) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L2) at (12,0) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L3) at (12,-2) {\mintinline{haskell}{Leaf $ Var 3 1}};
\node  (L4) at (12,-4) {\mintinline{haskell}{Leaf $ Var 1 2}};
\node  (L5) at (12,-6) {\mintinline{haskell}{Leaf $ Var 1 2}};
\node  (L6) at (12,-8) {\mintinline{haskell}{Leaf $ Var 1 3}};
\node  (L7) at (12,-10) {\mintinline{haskell}{Leaf $ Var -2 2}};
\node  (L8) at (12,-12) {\mintinline{haskell}{Leaf $ Var -1 3}};
\node  (L9) at (12,-14.5) {\mintinline{haskell}{Leaf $ Var 1 4}};
\node  (L10) at (12,-16.5) {\mintinline{haskell}{Leaf $ Var 1 4}};




\draw [-] (I1) -- (J1);
\draw [-] (I1) -- (J2);
\draw [-] (I1) -- (J3);
\draw [-] (I2) -- (J4);
\draw [-] (I2) -- (J5);
\draw [-] (I3) -- (J6);
\draw [-] (I3) -- (J7);
\draw [-] (I3) -- (J8);
\draw [-] (I4) -- (J9);
\draw [-] (I5) -- (J10);




\draw [-] (J1) -- (K1);
\draw [-] (J2) -- (K2);
\draw [-] (J2) -- (K2);
\draw [-] (J3) -- (K3);
\draw [-] (J4) -- (K4);
\draw [-] (J5) -- (K5);
\draw [-] (J6) -- (K6);
\draw [-] (J7) -- (K7);
\draw [-] (J8) -- (K8);



\draw [-] (K1) -- (L1);
\draw [-] (K2) -- (L2);
\draw [-] (K3) -- (L3);
\draw [-] (K4) -- (L4);
\draw [-] (K5) -- (L5);
\draw [-] (K6) -- (L6);
\draw [-] (K7) -- (L7);
\draw [-] (K8) -- (L8);
\draw [-] (J9) -- (L9);
\draw [-] (J10) -- (L10);




\end{tikzpicture}
%make Maps at each level visible??
\caption{Forest structure of ansatz linear combination (\ref{AnsatzExprEx}). }
\label{AnsatzExprExForest}
\end{figure}
Due to the forest data structure also addition of two such expression can be implemented quite performant. One simply uses the \mintinline{haskell}{unionWith} function provided by the Data.Map.Strict package \cite{HackageMap}. The function combines two Maps calling a specified combiner function if a key is present in both. The addition of two ansatz forests can now be achieved by defining the addition of the leaf values in the obvious way and further defining the addition of two ansatz forests that themselves contain sub forests by combining the corresponding Maps with combiner function being again the addition of ansatz forests. Doing so one eventually recurses over the whole structure. 

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
addForests :: AnsatzForestEta -> AnsatzForestEta -> AnsatzForestEta
addForests ans EmptyForest = ans
addForests EmptyForest ans = ans
addForests (Leaf var1) (Leaf var2)
        | isZeroVar newLeafVal = EmptyForest
        | otherwise = Leaf newLeafVal
        where
            newLeafVal = addVars var1 var2
addForests (ForestEta m1) (ForestEta m2)
        | M.null newMap = EmptyForest
        | otherwise = ForestEta newMap
         where
            newMap = M.filter (/= EmptyForest) $
                     M.unionWith addForests m1 m2
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
addForestsEpsilon :: AnsatzForestEpsilon -> AnsatzForestEpsilon ->
                     AnsatzForestEpsilon
addForestsEpsilon m1 m2 = M.filter (/= EmptyForest) $ M.unionWith
addForests m1 m2
\end{minted} 
\end{samepage}
Symmetrization of such an expression can be achieved by simply swapping or permuting certain index indentifiers of the various \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon} values in the ansatz forest and then adding the result to the former ansatz forest.
\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
pairSymForestEta :: (Int,Int) -> AnsatzForestEta -> AnsatzForestEta
pairSymForestEta inds ans = addForests ans $ swapLabelFEta inds ans
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
pairSymForestEps :: (Int,Int) -> AnsatzForestEpsilon ->
                    AnsatzForestEpsilon
pairSymForestEps inds ans = addForestsEpsilon ans $ 
                            swapLabelFEps inds ans
\end{minted} 
\end{samepage}

As swapping labels of nodes in the forest will in general destroy the sorting of the forest we have to reenforce the correct ordering afterwards. This is simply done by flattening the forest to a list that contains the individual branches, sorting the list according to the required ordering and then reconstructing the forest from the list of branches. Thus such swap functions are given by:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
swapLabelFEta :: (Int,Int) -> AnsatzForestEta -> AnsatzForestEta
swapLabelFEta inds ans = sortForest.canonicalizeAnsatzEta $ swapAnsatz
        where
            f = swapLabelEta inds
            swapAnsatz = mapNodes f ans
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
swapLabelFEps :: (Int,Int) -> AnsatzForestEpsilon -> AnsatzForestEpsilon
swapLabelFEps inds ans = sortForestEpsilon.canonicalizeAnsatzEpsilon $ 
                        swapAnsatz
        where
            f = swapLabelEpsilon inds
            swapAnsatz = mapNodesEpsilon f $ M.map 
                                            (swapLabelFEta inds) ans
\end{minted} 
\end{samepage}

Similarly one can easily define symmetrization functions for arbitrary other kinds of symmetry. We define a \mintinline{haskell}{Symmetry} data type that collects the most used such, i.e. (pair symmetries, pair anti-symmetries, block symmetries, cyclic symmetries, cyclic block symmetries):
\begin{center}
\begin{cminted}{haskell}
type Symmetry = ([(Int,Int)],[(Int,Int)],[([Int],[Int])],[[Int]],[[[Int]]])
\end{cminted}
\end{center}
The various integers refer to the indices that are to be symmetrized. We now can collect the individual symmetrizer functions in one overall function:

\begin{samepage} 
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
symAnsatzForestEta ::Symmetry -> AnsatzForestEta -> AnsatzForestEta
symAnsatzForestEta (sym,asym,blocksym,cyclicsym,cyclicblocksym) ans =
    foldr cyclicBlockSymForestEta (
        foldr cyclicSymForestEta (
            foldr pairBlockSymForestEta (
                foldr pairASymForestEta (
                    foldr pairSymForestEta ans sym
                ) asym
            ) blockSymMap
        ) cyclicsym
    ) cyclicblocksym
    where
        blockSymMap = map swapBlockLabelMap blocksym
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
symAnsatzForestEps :: Symmetry -> AnsatzForestEpsilon ->
                      AnsatzForestEpsilon
symAnsatzForestEps (sym,asym,blocksym,cyclicsym,cyclicblocksym) ans =
      foldr cyclicBlockSymForestEps (
          foldr cyclicSymForestEps (
              foldr pairBlockSymForestEps (
                  foldr pairASymForestEps (
                      foldr pairSymForestEps ans sym
                  ) asym
              ) blockSymMap
          ) cyclicsym
      ) cyclicblocksym
      where
        blockSymMap = map swapBlockLabelMap blocksym
\end{minted} 
\end{samepage}

After all this summarizes the data structures that we use to represent the tensorial expression that we are going to construct. Further details and additional functions that were ommited here can be found in (ref own hackage package).  \\

With this choice of data structures at hand we now proceed with the construction of Lorentz invariant basis tensors.  
This process can in general be divide in three steps:
\begin{itemize}
    \item[(i)] Generating all possible products built either solely from $\eta^{ab}$ or including exactly one $\epsilon^{abcd}$ that feature the required index structure.
    \item[(ii)] Symmetrizing the individual expressions according to the demanded symmetry to obtain all possible ansätze.
    \item[(iii)] Reducing the resulting expressions w.r.t. algebraic and numeric linear dependencies.
\end{itemize}
We start with the first step, computing all possible products. Labeling the indices by integers \mintinline{haskell}{[1,...,n]} one readily obtains all possible such individual products be exhausting all possible ways the indices can be arranged, taking into account the symmetries of $\eta^{ab}$, $\epsilon^{abcd}$ if present and the symmetries that are induced by the product structure. We get for the list of all possible index sortings for the products that only involve $\eta^{ab}$:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
getEtaInds :: [Int] -> [[Int]]
getEtaInds [a,b] = [[a,b]]
getEtaInds (x:xs) = concatMap res firstEta
        where
            firstEta = map (\y -> ([x,y],delete y xs)) xs
            res (a,b) = (++) a <$> getEtaInds b 
\end{minted} 
\end{samepage}

%check points at end of code examples
Similar the list of all possible indices for the expressions that involve one $\epsilon^{abcd}$ can be computed to be given by:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
getEpsilonInds :: [Int] -> [[Int]]
getEpsilonInds inds = allInds
        where
            i = length inds 
            epsInds = [ [a,b,c,d] | a <- [1..i-3], b <- [a+1..i-2],
                      c <- [b+1..i-1], d <- [c+1..i] ] 
            allInds = map (\x -> map (x ++) $
                      getEtaInds (inds \\ x) ) epsInds 
\end{minted} 
\end{samepage}

Note that in (cite own hackage package) we further reduced these index lists by filtering already here some of the linear dependencies that will later arise due to the symmetrization. This is not necessary as we will treat linear dependencies later anyway it does however increase performance as the lists that are consumed by the algorithm are then smaller from the very beginning.  
%check the above 
We could now proceed with the next step, express each individual product by the appropriate chosen data structure, symmetrize all products individually and then at the very end reducing occurring linear dependencies. Following these lines however bares the problem that intermediate results, i.e. the entirety of the thus constructed ansätze occupies an unreasonable amount of memory. Only at the very end when linear dependent ansätze are removed this memory can be freed again. In fact in most practical cases the memory that is needed for this approach by far exceeds resources that are typically available. It is thus best to combine the steps two and three that are outlined above. The algorithm consumes the list of individual products step by step. For each product a distinction is made: if the product is already present in the ansatz forest it is simply discharged if it is not it is symmetrized to obtain the corresponding ansatz and then added to the ansatz forest. Doing so the intermediate memory swelling is avoided as now from the very beginning only linear independent ansätze are added to the forest.

For this approach it is incredible important to note that two individual products when symmetrized to the respective ansätze are either identically or completely disjoint in the sense that they do not share a single common summand. This can be seen from the fact that the permutations of indices that are involved in the symmetrization procedure constitute a subgroup of the permutation group of appropriate size $S_n$ that acts on the products via permutation of their indices. The individual products that contribute to a given symmetrized ansatz then precisely constitute the corresponding orbit under the action of the symmetry subgroup. As orbits of any group action are clearly disjoint so are the ansätze. Thus at a given step of the algorithm, when deciding if a new product should be added to the ansatz forest or discharged we do not need to symmetrize the individual product. If it is already present in the forest and thus linear dependent of it, already the non symmetrized individual product will be present, if the individual product is not present also all further expressions that are obtained from symmetrizing it will not be present. Hence the decision whether or not an individual product should be added to the forest can be made without ever having to symmetrize the individual product. Obviously when such a product is added to the forest we still have to symmetrize it but all products that are discharged can now be discharged without requiring an explicit symmetrization. 

The above allows for an efficient criteria that given a list of possible products only incorporates those into the ansatz forest that do not posses algebraic linear dependencies amongst each other. When further reducing the ansatz forest w.r.t. numeric linear dependencies there are in principal two approaches how this can be achieved. We can either first use the above technique to construct an ansatz forest that is algebraically linear independent, then evaluate this ansatz forest for enough index combinations to retrieve all independent components, write these in a matrix where the columns label the individual variables that occur and then use standard linear algebra to compute a basis of these column vectors. Variables that are not present in this basis can then be removed from the ansatz forest as these multiply exactly those ansätze that are linear dependent. 

Alternatively when constructing the ansatz forest from a list of individual products we can not only check for algebraic linear dependencies when deciding whether or not a new product should be added, but if a given product is algebraically independent of the  ansatz forest then also test for numeric linear dependencies. Doing so we would construct from the very beginnning an ansatz forest that consits of not only algebraically but also numerically linear independent ansätze.
It turns out that the first approach results in slightly faster computation times, whereas the second approach is superior in memory usage, thus we have implemented both algorithms. 
We start provided details regarding the first approach. \\

\noindent \textbf{Reducing linear dependencies I:  The fast way}

The idea is straight forward. Representing the individual products involving only $\eta^{ab}$ or also including one $\epsilon^{abcd}$ as lists with the individual list elements representing the factors of the product, we need a function that decides whether or not such a product is already present in a given ansatz forest. As we are always dealing with ordered forests and also sorted products we can already conclude that a given product is missing in the forest if any given node is missing in the appropriate level of the forest. Thus only when an product is actually present in the forest we really need to decent up to the leaf values of the forest.

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
isElem :: [Eta] -> AnsatzForestEta -> Bool
isElem [] (Leaf x) = True
isElem x EmptyForest = False
isElem  (x:xs) (ForestEta m) = case mForest of
                                Just forest -> xs `isElem` forest
                                _           -> False
            where
                mForest = M.lookup x m
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
isElemEpsilon :: (Epsilon, [Eta]) -> AnsatzForestEpsilon -> Bool
isElemEpsilon (eps,l) m = case mForest of
                            Just forest -> l `isElem` forest
                            _           -> False
            where
                mForest = M.lookup eps m  
\end{minted} 
\end{samepage}

Using these two functions we simply define two functions that when an product is missing symmetrizes it to obtain the corresponding ansatz and then adds it to the forest when the product is already present it should be discharged. These functions is then folded over the list of products taking an empty forest as start value.  Doing this we obtain functions that takes a list of products and a value of type \mintinline{haskell}{Symmetry} and constructs from it the two ansatz forests with ansätze being obtained by symmetrizing the various products that are provided by the list. Further these two functions now ensure that all ansätze that are included in the forests are algebraically linear independent. 
These two functions can then again be used to define the final functions that solely take the required symmetry and the rank of the to be constructed Lorentz invariant basis tensors as argument and then computes from it an ansatz forest of all possible algebraically linear independent ansätze that feature the correct number of indices and symmetries. We simply use the priorly defined functions \mintinline{haskell}{getEtaInds} and \mintinline{haskell}{getEpsInds} to compute lists of all possible indices of products of either solely Minkowski metrics or also including one Levi-Civita symbol, all providing the required number of indices. These two lists are then transformed to list of the appropriate types \mintinline{haskell}{Eta} and \mintinline{haskell}{Epsilon} and each list is further combined with a different variable. The two list are reduced as described above, by invoking the two functions \mintinline{haskell}{reduceAnsatzEta'} and \mintinline{haskell}{reduceAnsatzEpsilon'}. Finally the variables in the two ansatz forests that are thus constructed are relabeled.

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceAnsatzEta' :: Symmetry -> [([Eta],Var)] -> AnsatzForestEta
reduceAnsatzEta' sym = foldl' addOrRem' EmptyForest
        where
            addOrRem' f ans = if isElem (fst ans) f then f else
                              addForests f (symAnsatzForestEta sym 
                              $ mkForestFromAscList ans)
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceAnsatzEpsilon' :: Symmetry -> [(Epsilon, [Eta], Var)] ->
                        AnsatzForestEpsilon
reduceAnsatzEpsilon' sym = foldl' addOrRem' M.empty
        where
            addOrRem' f (x,y,z) = if isElemEpsilon (x,y) f then f else
                                  addForestsEpsilon f 
                                  (symAnsatzForestEps sym 
                                  $ mkForestFromAscListEpsilon (x,y,z))  
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
getEtaForestFast :: Int -> Symmetry -> AnsatzForestEta
getEtaForestFast ord syms = relabelAnsatzForest 1 $ reduceAnsatzEta' syms 
                            allForests
            where
                allInds = getEtaInds [1..ord] syms
                allVars = mkAllVars
                allForests = zipWith mkEtaList' allVars allInds
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
getEpsForestFast :: Int -> Symmetry -> AnsatzForestEpsilon
getEpsForestFast ord syms = if ord < 4 then M.empty else
relabelAnsatzForestEpsilon 1 $ reduceAnsatzEpsilon' syms allForests
            where
                allInds = getEpsilonInds [1..ord] syms
                allVars = mkAllVars
                allForests = zipWith mkEpsilonList' allVars allInd
\end{minted} 
\end{samepage}

The remaining work consists of reducing the two constructed ansatz forests w.r.t. numeric linear dependencies. This is best achieved when explicitly retrieving the values that a given ansatz forests admits for all possible values that one might insert for its indices. It is important to note that we do not need to evaluate a given ansatz forest for all possible index values as due to symmetries that might be present we already know that evaluating the ansatz forest on two different lists of index values that are connected via the symmetry we will obtain the same value. Thus we can avoid much work by only evaluating the ansatz forest on one representative out of the equivalence classes that are generated by identifying index lists that are connected via a symmetry. We can further reduce the amount of necessary evaluations by using the priorly noted fact that any product of etas and therefore in particular any \mintinline{haskell}{AnsatzForestEta} can only yield non zero values when evaluated on a list of index values with each value occurring an even number of times. In contrast to that when evaluating an \mintinline{haskell}{AnsatzForestEpsilon} we can savely restrict to those evaluation lists that have each index value appearing an odd number of times. Finally it is worth noting that the value that we retrieve when evaluating an ansatz forest on a specific index value list at most changes by a sign under arbitrary relabelings of the coordinate axes. Thus as we might rename the we further reduce the number of necessary evaluations by selecting one index value list out of every set of index value list that that are mutually related by index relabelings. The reason why all ansätze are admit this special property, i.e. their values at most changes by a sign under arbitrary coordinate relabelings can becomes obvious by recalling that all possible expressions that are build solely from $\eta^{ab}$ and $\epsilon^{abcd}$ are Lorentz invariant. Up to a sign the relabeling of coordinate axes clearly defines a Lorentz transformation, as obviously all those relabelings that keep $x_0$ fixed also keep $\eta_{ab}$ invariant, those that interchange $x_0$ and $x_{\alpha}$ keep eta invariant up to a sign where the sign depends on the precise number of swaps between $x_0$ and spatial coordinates. Thus the fact that the ansatz forests are up to a sign invariant under such coordinate relabelings is no surprise. 

Summing up all of the above considerations can be used to reduce the number of evaluations that are necessary to remove numerical linear dependencies. Clearly employing such techniques is not required as we could also simply evaluate the ansatz forest for all possible index value list. Using the above however severely reduces the computation time of the computer program. 

Storing the information regarding which index is set to which value in an \mintinline{haskell}{IntMap Int} \cite{HackageIntMap} (for details regarding the implementation see also \cite{Okasaki98fastmergeable}) , i.e. a implementation of a Map type that is tailored towards integer keys the evaluation of a given forest can be obtained as follows: 

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
evalAnsatzForestEta :: I.IntMap Int -> AnsatzForestEta -> I.IntMap Int
evalAnsatzForestEta evalM (Leaf (Var x y)) = I.singleton y x
evalAnsatzForestEta evalM (ForestEta m) = M.foldlWithKey' foldF I.empty m
    where
        foldF b k a = let nodeVal = evalNodeEta evalM k
                      in if isNothing nodeVal then b
                         else I.unionWith (+)
                              (I.map (fromJust nodeVal *)
                              (evalAnsatzForestEta evalM a)) b
evalAnsatzForestEta evalM EmptyForest = I.empty
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
evalAnsatzForestEpsilon :: I.IntMap Int -> AnsatzForestEpsilon ->
                           I.IntMap Int
evalAnsatzForestEpsilon evalM = M.foldlWithKey' foldF I.empty
    where
        foldF b k a = let nodeVal = evalNodeEpsilon evalM k
                      in if isNothing nodeVal then b
                         else I.unionWith (+) 
                              (I.map (fromJust nodeVal *)
                              (evalAnsatzForestEta evalM a)) b  
\end{minted} 
\end{samepage}

The \mintinline{haskell}{IntMap Int} that is returned as result in the above functions encodes the linear combinations of variables that is obtained when evaluating an ansatz forest, i.e. the integer keys of the int map represent the variable labels and the values the corresponding factors in the linear combination. 
When evaluating a given ansatz forest for multiple index value list we can gain further performance improvements by noting that the list of all evaluation int maps can in fact be divided in chunks that then can be processed parallel. This is achieved by the functions \mintinline{haskell}{evalAllEta} and \mintinline{haskell}{evalAllEpsilon}.

Once we have evaluated an ansatz forest for all necessary index value lists we can store the retrieved values in form of a matrix as described before with the columns labeling the individual variables and the rows labeling the index combination values that we evaluated the ansatz forest on. The ansatz forest can now be reduced w.r.t. numerical linear dependencies by removing linear dependent column vectors from this matrix. To that end we use Haskell bindings \cite{HackageEigen} to the C++ linear algebra library Eigen \cite{eigenweb}. Using eigen subroutines one can readily reduce linear dependencies amongst the column vectors a given matrix, for instance by means of a LU decomposition. Finally once the matrix is reduced we can proceed by removing all branches with leaf variables that correspond to columns that have been removed from the matrix. This then also removes the numerically linear dependent ansätze from the ansatz forest:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceLinDepsFastEta :: [I.IntMap Int] -> Symmetry ->
                        AnsatzForestEta -> AnsatzForestEta
reduceLinDepsFastEta evalM symL ansEta = newEtaAns
        where
            etaL = evalAllEta evalM ansEta
            etaVars = getPivots etaL
            allEtaVars = getForestLabels ansEta
            remVarsEta =  allEtaVars \\ etaVars
            newEtaAns = relabelAnsatzForest 1 $
                        removeVarsEta remVarsEta ansEta
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceLinDepsFastEps :: [I.IntMap Int] -> Symmetry ->
                        AnsatzForestEpsilon -> AnsatzForestEpsilon
reduceLinDepsFastEps evalM symL ansEps = newEpsAns
        where
            epsL = evalAllEpsilon evalM ansEps
            epsVars = getPivots epsL
            allEpsVars = getForestLabelsEpsilon ansEps
            remVarsEps =  allEpsVars \\ epsVars
            newEpsAns = relabelAnsatzForestEpsilon 1 $
                        removeVarsEps remVarsEps ansEps 
\end{minted} 
\end{samepage}

This finishes the first, computation time optimized method of constructing a basis of the space of Lorentz invariant tensors of given rank and symmetry that we have implemented in the sparse-tensor library. In order to use the thus computed result, the basis of Lorentz invariant tensors together with the previously presented functionallity of the sparse-tensor package we also provide functions that do not only construct the the ansatz forests but also return the computed Lorentz invariant basis tensor in form of compatible tensor data type. We represent the occuring variables that then necessarily also occur in the components of the tensors as int maps:
\begin{center}
\begin{cminted}{haskell}
newtype AnsVar a = AnsVar (I.IntMap a) deriving (Eq)
\end{cminted}
\end{center}

Further we introduce type synonyms for the typical used tensors types, the usual spacetime tensors that feature contravariant and covariant indices ranging between $0$ and $3$:

\begin{center}
\begin{cminted}{haskell}
type STTens n1 n2 v = AbsTensor2 n1 n2 Ind3 v
\end{cminted}
\end{center}

And also a more general tensor type that we heavily used for the area metric computations. This tensor type features 3 different index types each appearing in contravariant and covariant position. The first index type ranges over the area metric degrees of freedom and thus ranges from $0$ to $20$, the second index type labels second derivative pairs, and hence runs form $0$ to $9$ and the last type is again the usual space time index:

\begin{center}
\begin{cminted}{haskell}
type ATens n1 n2 n3 n4 n5 n6 v = 
     AbsTensor6 n1 n2 n3 n4 n5 n6 Ind20 Ind9 Ind3 v
\end{cminted}
\end{center}

The final functions that use the computation time optimized method and return the result as triplet of the two ansatz forest and a tensor that collects all their values are:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorFastSym :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                         [[Int]]-> (AnsatzForestEta, AnsatzForestEpsilon,
                         STTens n 0 (AnsVar Rational))
mkAnsatzTensorFastSym ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMaps symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens =
            evalToTensSym symmetries evalMEtaInds evalMEpsInds
                          ansEta ansEps
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--without explicit symmetriization in tens
mkAnsatzTensorFast :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                      [[Int]]-> (AnsatzForestEta, AnsatzForestEpsilon,
                      STTens n 0 (AnsVar Rational))
mkAnsatzTensorFast ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMaps symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTens evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--evaluation to a tensor that uses multiple index types
mkAnsatzTensorFastAbs :: Int -> Symmetry ->
                         [([Int], Int, [IndTupleAbs n1 0 n2 0 n3 0])] ->
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         ATens n1 0 n2 0 n3 0 (AnsVar Rational))
mkAnsatzTensorFastAbs ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMapsAbs symmetries evalL 
        (ansEta, ansEps) =
            mkAnsatzFast ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTensAbs evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\end{samepage}

The first function takes as arguments the order of the to be constructed Lorentz invaraint basis, i. the number of indices, the symmetry and also a list of all index value lists that are necessary for the evluation. The evaluation list must contain exactly one representative out of every set of index value lists that are the same under the present symmetry. Further reduction as described above must not incoorporated manually as this is achieved by the function \mintinline{haskell}{mkAllEvalMaps}. The first function returns the Lorentz invariant basis as explicitly symmetrized spacetime tensors. Note that for higher ranked tensors and more complicated symmetries the explicit symmetrization of the tensors might be expensive. Furthermore often it suffices to only store one representative of each symmetry equivalence class in the tensor, for instance if the tensor is contracted against symmetric objects that thus enforce the symmetries in later computations. To that end we also provided a function that returns the tensors without explicit symmetrization. Finally the last function does not require a list of index value lists as input but needs a triplet consisting of the index value list, the multiplicity of the given index value list that is computed as product of the individual multiplicities of the abstract indices used (for the definition of the multiplicity $\sigma$ see the discussion following definition \ref{interDef}), and also a list of all abstract index tuples that correspond to the the index value list. How these individual ingredients can be computed in detail will be discussed in the following section when we consider an example. The last function then computes a triple consisting of the two ansatz forest and an abstract tensor that uses multiple indices to store the corresponding values. 

Finally we also provide the first two of the above three functions in a form that does not require to specify the evaluation list explicitly. Paying the price of a slighty more expensive computation in the following two functions the evaluation list is constructed fully automatically from the symmetries and the rank of the given tensor: 

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorFastSym' :: forall (n :: Nat). SingI n => Int ->
                          Symmetry -> (AnsatzForestEta,
                          AnsatzForestEpsilon,
                          STTens n 0 (AnsVar Rational))
mkAnsatzTensorFastSym' ord symmetries = mkAnsatzTensorFastSym
                                            ord symmetries evalL
    where
        evalL = filter (`filterAllSym` symmetries) 
                        $ allList ord symmetries
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorFast' :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                       (AnsatzForestEta, AnsatzForestEpsilon,
                       STTens n 0 (AnsVar Rational))
mkAnsatzTensorFast' ord symmetries = mkAnsatzTensorFast
                                        ord symmetries evalL
    where
        evalL = filter (`filterAllSym` symmetries) 
                        $ allList ord symmetries
\end{minted} 
\end{samepage}

\noindent \textbf{Reducing linear dependencies II:  The memory optimised way}

The main idea of this second way of reducing the ansatz forest w.r.t linear dependencies is to construct an ansatz forest that is not only algebraically but also numerically linear independent from the very beginning. This can be achieved as follows: As before the algorithm consumes a list of all possible individual products and checks in any given step if the product at hand is missing in the ansatz forest and thus represents an ansatz that is algebraically linear independent or is already present and can hence be discharged. In contrast to the previous approach now when a product is missing we immediately evaluate the corresponding ansatz for all necessary index value lists. If in each step we do not only provide input data that consists of the present ansatz forest that is needed for deciding whether or not the new ansatz is algebraically linear independent but also includes its fully evaluated matrix --- this time with rows labeling the individual variables that are present in the forest and columns labeling the several index value list ---
%
%check if this is right ??
%
we can easily use the newly evaluated ansatz to decide whether or not the new ansatz is numerically independent from the ansatz forest or not. Thus we first test for algebraically linear dependency of a new ansatz, if the ansatz is algebraically linear independent we further test for numerically linear dependency. Only if the new ansatz is also numerically linear independent of the present ansatz forest we add it to the forest. If the ansatz is added to the forest it is obviously also added to the matrix that contains the all independent components of the forest. This total procedure is accomplished by the following two function:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
addOrDiscardEtaEig :: Symmetry -> [I.IntMap Int] ->
                      (AnsatzForestEta, RankDataEig) -> 
                      [Eta] -> (AnsatzForestEta, RankDataEig)
addOrDiscardEtaEig symList evalM (ans,rDat) etaL
            | isElem etaL ans = (ans,rDat)
            | otherwise = case newRDat of
                               Nothing          -> (ans,rDat)
                               Just newRDat'    -> (sumAns,newRDat')
             where
                newAns = getNewAns symList etaL rDat
                newRDat = getNewRDat evalM newAns rDat
                sumAns = addForests ans newAns
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
addOrDiscardEpsilonEig :: Symmetry -> [I.IntMap Int] ->
                          (AnsatzForestEpsilon, RankDataEig) ->
                          (Epsilon,[Eta]) ->
                          (AnsatzForestEpsilon, RankDataEig)
addOrDiscardEpsilonEig symList evalM (ans,rDat) (epsL,etaL)
            | isElemEpsilon (epsL,etaL) ans = (ans,rDat)
            | otherwise = case newRDat of
                               Nothing          -> (ans,rDat)
                               Just newRDat'    -> (sumAns,newRDat')
             where
                newAns = getNewAnsEps symList epsL etaL rDat
                newRDat = getNewRDatEps evalM newAns rDat
                sumAns = addForestsEpsilon ans newAns
\end{minted} 
\end{samepage}

The type alias \mintinline{haskell}{RankDataEig} represents the ansatz forest data that is necessary for deciding whether or not a new ansatz is numerically linear dependent from the current ansatz forest and simply is given by a tuple that consits of one Eigen matrix and one sparse Eigen Matrix:

\begin{center}
\begin{cminted}{haskell}
type RankDataEig = (Mat.MatrixXd, Sparse.SparseMatrixXd)
\end{cminted}
\end{center}

Note that usually the number of necessary evaluation index value lists by far exceeds the number of variables  that are present in a given ansatz forest. Thus the chosen way of evaluating a given forest to a matrix yields a matrix with a large number of columns compared to a small number of rows. In the following we will refer to this matrix as $M$. The second matrix that is included in \mintinline{haskell}{RankDataEig} is precisely this matrix $M$. To safe memory this matrix is stored in sparse format. The first matrix in \mintinline{haskell}{RankDataEig} is given by $M M^t$. This matrix is used to achieve performance when computing whether or not a new ansatz is numerically linear dependent from the current ansatz forest or not. To that end note that $\mathrm{rank}(MM^t) = \mathrm{rank}(M)$ (see for instance "Bemerkung 2.57" in \cite{LAKnab}). Thus once we have evaluated a new ansatz to a row vector $v$ we can compute the rank of the new ansatz forest that would be obtained by adding the given ansatz to the current forest by:
\begin{align}\label{RankDatmat}
    \mathrm{rank}\left (\begin{bmatrix}
        M \\
        \cmidrule(lr){1-1} 
        v
    \end{bmatrix} \right )
    = \mathrm{rank} \left ( \begin{bmatrix}
        M \\
        \cmidrule(lr){1-1}
        v
    \end{bmatrix} \cdot \begin{bmatrix}
        M^t \ \vert \  v^t 
    \end{bmatrix} \right ) = \mathrm{rank} \left (\begin{bmatrix}
        MM^t & \vline & M v^t \\
        \cmidrule(lr){1-3}
        (Mv^t)^t & \vline & vv^t 
    \end{bmatrix}  \right ).
\end{align}
If the ansatz is numerically linear independent from the forest appending its evaluated row vector to the current ansatz matrix will result in an increased rank.

Note that in each step of the algorithm once we have evaluated the new ansatz to the row vector $v$ we only have to compute its transpose $v^t$ the matrix vector product $Mv^t$ and the corresponding transpose $(Mv^t)^t$ and the vector vector product $v v^t$. In particular the upper left block in the last matrix in (\ref{RankDatmat}) is already provided from the previous step and thus only needs to be computed newly once an additional ansatz is added to the ansatz forest. Thus we achieve the desired result with a minimum of expensive matrix matrix operations involved. The rank computation itself is then as before carried out by relying on efficient Eigen subroutines.

The following function takes as arguments the current rank data and the newly evaluated ansatz row vector and computes form it the new rank data, that is it returns the rank data provided by the first matrix in (\ref{RankDatmat}) if the new ansatz is numerically linear independent from the ansatz forest in form of a \mintinline{haskell}{Just} value and it returns \mintinline{haskell}{Nothing} if the new ansatz is numerically linear dependent. 

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
checkNumericLinDepEig :: RankDataEig -> Maybe Sparse.SparseMatrixXd ->
                         Maybe RankDataEig
checkNumericLinDepEig (lastMat, lastFullMat) (Just newVec)
    | eigenRank < maxRank = Nothing
    | otherwise = Just (newMat, newAnsatzMat)
     where
        newVecTrans = Sparse.transpose newVec
        scalar = Sparse.toMatrix $ Sparse.mul newVec newVecTrans
        prodBlock = Sparse.toMatrix $ Sparse.mul lastFullMat newVecTrans
        prodBlockTrans = Mat.transpose prodBlock
        newMat = concatBlockMat lastMat prodBlock prodBlockTrans scalar
        eigenRank = Sol.rank Sol.FullPivLU newMat
        maxRank = min (Mat.cols newMat) (Mat.rows newMat)
        newAnsatzMat = Sparse.fromRows $ 
                       Sparse.getRows lastFullMat ++ [newVec]
checkNumericLinDepEig (lastMat, lastFullMat) Nothing = Nothing
\end{minted} 
\end{samepage}

With these functions defined we can now construct functions that construct algebraically and numerically linear independent ansatz forests out of a given list of individual products, the given symmetry and the evaluation list:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceAnsatzEtaEig :: Symmetry -> [[Eta]] -> [I.IntMap Int] ->
                      (AnsatzForestEta,Sparse.SparseMatrixXd)
reduceAnsatzEtaEig symL etaL evalM
    | null evalM = (EmptyForest, Sparse.fromList 0 0 [])
    | null etaL = (EmptyForest, Sparse.fromList 0 0 [])
    | otherwise = (finalForest, finalMat)
    where
        (ans1,rDat1,restEtaL) = mk1stRankDataEtaEig symL etaL evalM
        (finalForest, (_,finalMat)) = foldl' 
                                      (addOrDiscardEtaEig symL evalM)
                                      (ans1,rDat1) restEtaL
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
reduceAnsatzEpsilonEig :: Symmetry -> [(Epsilon,[Eta])] ->
                          [I.IntMap Int] ->
                          (AnsatzForestEpsilon,Sparse.SparseMatrixXd)
reduceAnsatzEpsilonEig symL epsL evalM
    | null evalM = (M.empty, Sparse.fromList 0 0 [])
    | null epsL = (M.empty, Sparse.fromList 0 0 [])
    | otherwise = (finalForest, finalMat)
    where
        (ans1,rDat1,restEpsL) = mk1stRankDataEpsilonEig symL epsL evalM
        (finalForest, (_,finalMat)) = foldl'
                                      (addOrDiscardEpsilonEig symL evalM)
                                      (ans1,rDat1) restEpsL
\end{minted} 
\end{samepage}

Finally now we can also provide functions that return a triplet consisting of two ansatz forests corresponding to a Lorentz invariant basis of given rank and symmetry and one tensor that incldes all their values, this time employing the memory optimzed algorithm:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorEigSym :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                        [[Int]] -> (AnsatzForestEta, AnsatzForestEpsilon,
                        STTens n 0 (AnsVar Rational))
mkAnsatzTensorEigSym ord symmetries evalL = (ansEta, ansEps, tens)
        where
            (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
                mkAllEvalMaps symmetries evalL 
            (ansEta, ansEps, _, _) =
                getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
            tens = evalToTensSym
                   symmetries evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorEig :: forall (n :: Nat). SingI n => Int -> Symmetry ->
                     [[Int]] -> (AnsatzForestEta, AnsatzForestEpsilon,
                     STTens n 0 (AnsVar Rational))
mkAnsatzTensorEig ord symmetries evalL = (ansEta, ansEps, tens)
        where
            (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
                mkAllEvalMaps symmetries evalL 
            (ansEta, ansEps, _, _) =
                getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
            tens = evalToTens evalMEtaInds evalMEpsInds ansEta ansEps
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--evaluation to a tensor that uses multiple abstract index types
mkAnsatzTensorEigAbs :: Int -> Symmetry ->
                        [([Int], Int, [IndTupleAbs n1 0 n2 0 n3 0])] ->
                        (AnsatzForestEta, AnsatzForestEpsilon,
                        ATens n1 0 n2 0 n3 0 (AnsVar Rational))
mkAnsatzTensorEigAbs ord symmetries evalL = (ansEta, ansEps, tens)
    where
        (evalMEtaRed, evalMEpsRed, evalMEtaInds, evalMEpsInds) =
            mkAllEvalMapsAbs symmetries evalL 
        (ansEta, ansEps, _, _) =
            getFullForestEig ord symmetries evalMEtaRed evalMEpsRed
        tens = evalToTensAbs evalMEtaInds evalMEpsInds ansEta ansEps

\end{minted} 
\end{samepage}

And as before we also provide versions of the first two functions that automatically construct the evaluation list from the rank and the symmetries specified:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--with explicit symmetrization in tens
mkAnsatzTensorEigSym' :: forall (n :: Nat). SingI n =>  Int ->
                         Symmetry ->
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         STTens n 0 (AnsVar Rational))
mkAnsatzTensorEigSym' ord symmetries = mkAnsatzTensorEigSym
                                       ord symmetries evalL
    where
        evalL =
            filter (`filterAllSym` symmetries) $ allList ord symmetries
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
--without explicit symmetrization in tens
mkAnsatzTensorEig' :: forall (n :: Nat). SingI n =>  Int ->
                      Symmetry ->
                      (AnsatzForestEta, AnsatzForestEpsilon,
                      STTens n 0 (AnsVar Rational))
mkAnsatzTensorEig' ord symmetries = mkAnsatzTensorEig
                                    ord symmetries evalL
    where
        evalL =
            filter (`filterAllSym` symmetries) $ allList ord symmetries
\end{minted} 
\end{samepage}

The developed sparse-tensor library also provides further functions that can be usefull when working with the ansatz forest data types. Details can again be found in (cite own hackage package !!!).

\section{A Step-By-Step Example}
In this section we are going to consider an example that shall nicely illustrate how the sparse-tensor package can be used to deal with the kind of problems that arise in the perturbative approach to constructive gravity. To that end we are going to consider the two linear order perturbative equivariiance equations (\ref{order1}) in the context of area metric gravity. 

The first step is determining the different indices and thus the abstract tensor type that is needed for the treatment of perturbative area metric gravity. 
We need one index type that runs over area metric degrees of freedom, one index type for second order spacetime derivatives and one index type for treating spacetime indices. The corresponding tesnor type was already introduced before:

\begin{center}
\begin{cminted}{haskell}
type ATens n1 n2 n3 n4 n5 n6 v = 
     AbsTensor6 n1 n2 n3 n4 n5 n6 Ind20 Ind9 Ind3 v
\end{cminted}
\end{center}

Here the different index types are defined all defined in the same fashion. As illustrative example we discuss \mintinline{haskell}{Ind3} in more detail:

\begin{center}
\begin{cminted}{haskell}
newtype Ind3 =  Ind3 {indVal3 :: Int}
    deriving (Ord, Eq, Show, Read, Generic, NFData, Serialize)
\end{cminted}
\end{center}

We choose to use the \mintinline{haskell}{newtype} declaration in order to ensure that functions that are defined for one particular index type cannot be applied to any other index type and thus preventing possible errors that might occur when mixing up index types.
Note that in order for the tensor algebra functions in sparse-tensor to work for the \mintinline{haskell}{ATens} type, the index types must all be instances of \mintinline{haskell}{TIndex}. This can be simply achieved by defining the function \mintinline{haskell}{toEnum}, \mintinline{haskell}{fromEnum} making them instances of the typeclass \mintinline{haskell}{Enum} and then further defining the index range function that simply says how many different index values of the given abstract index exist:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
instance TIndex Ind3 where
    indRange x = 4

instance Enum Ind3 where
    toEnum = Ind3
    fromEnum = indVal3
\end{minted} 
\end{samepage}

When taking a closer look at the equations (\ref{order1}) we see that albeit of the three Lorentz invariant basis tensors $a^{A}$, $a^{AI}$ and $a_0$ we further need the two area metric intertwiners $I^I_{abcd}$, $J_I^{abcd}$ with components being displayed in (\ref{AreaI}) and (\ref{AreaJ}), the J-intertwiner of a symmetric index pair whose components can be found in (\ref{interJMet})  , the flat background area metric $N_A$ and the 4 dimensional Kronecker delta $\delta^a_b$. 
The Kronecker delta can readily be obtained as 

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
delta3 :: ATens 0 0 0 0 1 1 Rational
delta3 = fromListT6 $ zip
         [(Empty, Empty, Empty, Empty,
         singletonInd (Ind3 i), singletonInd (Ind3 i)) | 
         i <- [0..3]] (repeat 1)
\end{minted} 
\end{samepage}
%further comment the use of fromListT6 ??

The three intertwiners can be constructed according to:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
interJ2 :: ATens 0 0 0 1 2 0 Rational
interJ2 = fromListT6 $ filter (\(i,k) -> k /= 0) $ map (\x -> (x,f x)) inds
        where
            trian2 = trianMap2
            inds = [ (Empty, Empty, Empty, singletonInd $ Ind9 a, Append (Ind3 b) $ singletonInd $ Ind3 c, Empty) | a <- [0..9], b <- [0..3], c <- [0..3]]
            f (_, _, _, ind1, ind2, _)
                | ind1 == (M.!) trian2 (sortInd ind2) = jMult2 ind2
                | otherwise = 0
\end{minted} 
\end{samepage}


\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
interIArea :: ATens 1 0 0 0 0 4  Rational
interIArea = fromListT6 $ filter (\(i,k) -> k /= 0) $
             map (\x -> (x,f x)) inds
    where
        trianArea = trianMapArea
        inds = [(singletonInd (Ind20 a), Empty, Empty, Empty, Empty,
               Append (Ind3 b) $ Append (Ind3 c) $ Append (Ind3 d) 
               $ singletonInd $ Ind3 e) |
               a <- [0..20], b <- [0..3], c <- [0..3], d <- [0..3],
               e <- [0..3], not (b == c || d == e)]
        f (ind1, _, _, _, _, ind2)
            | ind1 == (M.!) trianArea indArea = s
            | otherwise = 0
            where
                (indArea, s) = canonicalizeArea ind2
\end{minted} 
\end{samepage}

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
interJArea :: ATens 0 1 0 0 4 0 Rational
interJArea = fromListT6 $ filter (\(i,k) -> k /= 0) $
             map (\x -> (x,f x)) inds
    where
        trianArea = trianMapArea
        inds = [(Empty, singletonInd $ Ind20 a, Empty, Empty,
               Append (Ind3 b) $ Append (Ind3 c) $ Append (Ind3 d)
               $ singletonInd $ Ind3 e, Empty) |
               a <- [0..20], b <- [0..3], c <- [0..3], d <- [0..3],
               e <- [0..3], not (b == c || d == e)]
        f (_, ind1, _, _, ind2, _)
            | ind1 == (M.!) trianArea indArea = s * jMultArea indArea
            | otherwise = 0
            where
                (indArea, s) = canonicalizeArea ind2
\end{minted} 
\end{samepage}

Here \mintinline{haskell}{trianMap2} is a Map that relates the indices of a symmetric pair of spacetime indices to the corresponding abstract index of type \mintinline{haskell}{Ind9}, \mintinline{haskell}{trianMapArea} is a Map that relates the 4 spacetime indices of a given area metric to the corresponding abstract index, \mintinline{haskell}{canonicalizeArea} is a function that brings a set of 4 spacetime indices obeying the area metric symmetries to canonical order taking into account possible signs that might occur during the resorting, \mintinline{haskell}{jMult2} is a function that computes the multiplicity of the symmetric index pair and \mintinline{haskell}{jMultArea} computes the $\sigma$ multiplicity (see discussion following definition \ref{interDef}) of a given set of 4 spacetime indices that correspond to one area metric.

From the two area metric intetwiners we can compute the constant tensor $C^{Am}_{Bn}$ according to (\ref{areaGotayMInter}):

\begin{center}
\begin{cminted}{haskell}
interArea :: ATens 1 1 0 0 1 1 Rational
interArea = (-4) &. contrATens3 (1,1) (contrATens3 (2,2) $
            contrATens3 (3,3) $ interIArea &* interJArea)
\end{cminted}
\end{center}

The flat background area metric can simply be constructed as follows:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
flatArea :: ATens 0 1 0 0 0 0 Rational
flatArea = fromListT6 $ map (\(i,v) -> ( (Empty, singletonInd $ Ind20 i,
           Empty, Empty, Empty, Empty), v))
           [(0,-1),(5,-1),(6,-1),(9,1),(11,-1),
           (12,-1),(15,1),(18,1),(20,1)]
\end{minted} 
\end{samepage}

The next step is the computation of the three bases of Lorentz invariant tensors. Clearly $a_0$ is simply a single constant. As such it can be incorporated in our tensor framework by:

\begin{center}
\begin{cminted}{haskell}
let ans0 = fromListT6' [(([],[],[],[],[],[]), AnsVar $ 
           I.fromList [(1,1)] )] :: ATens 0 0 0 0 0 0 (AnsVar Rational)
\end{cminted}
\end{center}

The second expansion coefficient is given by $a^A = I^A_{abcd} a^{abcd}$, where the indices $(abcd)$ feature the area metric symmetries. In order to use our functions for constructing the appropriate Lorentz invariant basis tensors we need to provide the symmetry at hand as value of type \mintinline{haskell}{Symmetry} and also need to provide a list of index value lists that are necessary for the evaluation. Labeling the indices $(abcd)$ by the integers $(1,2,3,4)$ the symmetry is given as 

\begin{center}
\begin{cminted}{haskell}
symList4 :: Symmetry
symList4 = ([], [(1,2),(3,4)], [([1,2],[3,4])], [], [])
\end{cminted}
\end{center}

This simply corresponds to pair anti symmetries in the indices $(1,2)$ and $(3,4)$ and a block symmetry w.r.t. exchange of the two index pairs $(1,2)$ and $(3,4)$. The evaluation list can be constructed as

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
areaList4 :: [([Int], Int, [IndTupleAbs 1 0 0 0 0 0])]
areaList4 = list
      where
          trianArea = trianMapArea
          list = [ let a' = (I.!) trianArea a in (a', areaMult a',
                 [(singletonInd (Ind20 $ a-1), Empty, Empty, Empty,
                 Empty, Empty)]) | a <- [1..21] ]
\end{minted} 
\end{samepage}

As before \mintinline{haskell}{trianMapArea} is a Map that relates a block of 4 spacetime indices with area metric symmetries to an abstract area metric index and \mintinline{haskell}{areaMult} is a function that computes the multiplicity of a given set of 4 spacetime indices with area metric symmetry. Note that we do not only provide the index value list that are necessary for the evaluation of the ansatz forests but also include the multiplicity of the index values and the corresponding abstract index as we want to make use of the function \mintinline{haskell}{mkAnsatzTensorEigAbs} to evaluate the anastz forests to the chosen abstract tensor type \mintinline{haskell}{ATens}. We can now construct the basis for the expansion coefficient $a^{A}$ by invoking:

\begin{center}
\begin{cminted}{haskell}
let (eta4,eps4,ans4) = mkAnsatzTensorEigAbs 4 symList4 areaList4 :: 
                         (AnsatzForestEta, AnsatzForestEpsilon,
                         ATens 1 0 0 0 0 0 (AnsVar Rational))
\end{cminted}
\end{center}

If we wish to quickly check the constructed ansatz forest we can do so by using the functions \mintinline{haskell}{drawAnsatzEta} and \mintinline{haskell}{drawAnsatzEpsilon} that return a simply ASCII drawing of the forest structure. Doing so we get:

\begin{center}
\begin{BVerbatim}
(1,3)
|
`---- (2,4) * (4) * x[1]

(1,4)
|
`---- (2,3) * (-4) * x[1]

(1,2,3,4) * (8) * x[2]
\end{BVerbatim}
\end{center}

Proceeding along the same lines for the ansatz $a^{AI} = I^A _{abcd} I^I_{pq} a^{abcdpq}$ we first provide the symmetry:

\begin{center}
\begin{cminted}{haskell}
symList6 :: Symmetry
symList6 = ([(5,6)], [(1,2),(3,4)], [([1,2],[3,4])], [], [])
\end{cminted}
\end{center}

This simply correspond to an additional pair symmetry in the derivative indices $(5,6)$. The evaluation list can be obtained as:

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
areaList6 :: [([Int], Int, [IndTupleAbs 1 0 1 0 0 0])]
areaList6 = list
      where
          trian2 = trianMap2
          trianArea = trianMapArea
          list = [ let (a',i') = ((I.!) trianArea a, (I.!) trian2 i) in
                 (a' ++ i', areaMult a' * iMult2 i', 
                 [(singletonInd (Ind20 $ a-1), Empty,
                 singletonInd (Ind9 $ i-1),
                 Empty, Empty, Empty)]) | a <- [1..21], i <- [1..10]]
\end{minted} 
\end{samepage}

We now can again construct the ansatz forest 

\begin{center}
\begin{cminted}{haskell}
let (eta6,eps6,ans6) = mkAnsatzTensoreigAbs 6 symList6 areaList6 :: 
                       (AnsatzForestEta, AnsatzForestEpsilon,
                       ATens 1 0 1 0 0 0 (AnsVar Rational))
\end{cminted}
\end{center}

Note that when constructed the variables that are include in any pair of ansatz forests start from one. When one uses several ansatz forests in one equation it is thus necessary to relabel certain variables. The number of variables in a given pair of ansatz forest can for instance be obtained by applying the function \mintinline{haskell}{tensorRank6'} on the corresponding tensor. This function simply arranges the independent components of the tensor in a matrix with the variables labeling the columns and then invokes Eigen subroutines to compute the rank of the matrix. for instance applyig it on the ansatz tensor \mintinline{haskell}{ans4} we get \mintinline{haskell}{tensorRank6' ans4 = 2}. Hence we can use it to shift the labels of the variables that are included in the three tensors appropriately:

\begin{center}
\begin{cminted}{haskell}
let ans0' = shiftLabels6 5 ans0
let ans4' = shiftLabels6 3 ans4 
\end{cminted}
\end{center}

Now the first 3 variables are those contained in \mintinline{haskell}{ans6} the next 2 are those in \mintinline{haskell}{ans4} and the last variable is those from \mintinline{haskell}{ans0}.
We can now finally construct the two equations in (\ref{order1}):

\begin{samepage}
\begin{minted}[frame = lines, framesep = 2.5mm, baselinestretch =
1.2, bgcolor=LG]{haskell}
eqn1 :: ATens 0 0 0 0 0 0 (AnsVar Rational) ->
        ATens 1 0 0 0 0 0 (AnsVar Rational) ->
        ATens 0 0 0 0 1 1 (AnsVar Rational)
eqn1 ans0 ans4 = contrATens1 (0,0) (ans4 &* flatInter) &+
                 (ans0 &* delta3)
                 
eqn2 :: ATens 1 0 1 0 0 0 (AnsVar Rational) ->
        ATens 0 0 0 0 3 1 (AnsVar Rational)
eqn2 ans6 = contrATens2 (0,0) $ contrATens1 (0,0) $ ans6 &*
            contrATens1 (0,1) (interEqn5 &* flatArea)
    where 
        interEqn5 = cyclicSymATens5 [0,1,2] $
                    interJ2 &* interMetric
\end{minted} 
\end{samepage}

We can now for instance check the rank of the equations. This can be achieved by first computing the two equations:

\begin{center}
\begin{cminted}{haskell}
let eqn1Area = eqn1 ans0' ans4' 
let eqn2Area = eqn2 ans6  
\end{cminted}
\end{center}
Then we collect the two equations in a list:
\begin{center}
\begin{cminted}{haskell}
let tList = eqn2Area &.&> (singletonTList6 eqn1Area ::
            TensList6 Ind20 Ind9 Ind3 (AnsVar Rational)) 
\end{cminted}
\end{center}
We can now for instance compute the rank of the collective tensor equations that are included in such a list. This will then of course correspond to the rank of the equations. Doing so we find:
\begin{center}
\begin{cminted}{haskell}
tensorRank6 tList = 2 
\end{cminted}
\end{center}
We can also extract the information given by the two tensor equations in form of a matrix with as usual columns labeling the 6 variables and rows labeling independent equations. The sparse-tensor function \mintinline{haskell}{toMatList6} for instance returns this matrix in terms of a standard sparse matrix format \mintinline{haskell}{[((Int,Int),Rational)]}, where the pair of integers label row and column and the rational number is the corresponding value. Applying this function we find:
\begin{center}
\begin{cminted}{haskell}
toMatList6 tList = [((1,1),1152 % 1),((1,2),576 % 1),((1,3),(-2304) % 1),
                   ((2,4),(-96) % 1),((2,5),192 % 1),((2,6),1 % 1)]
\end{cminted}
\end{center}
Note that the relatively large values are a result of the used factorless symmetrization method. The thus generated output can now easily be used as input for standard computer algebra systems such as Maple or Mathematica that then can be used to actually solve the perturbative equivariance equations.  

\chapter{Conclusions}

\appendix

\chapter{Perturbative Lagrangians}
\dictum{
Here we display the additional results from the two exemplary applications of the developed framework for constructing perturbative diffeomorphism invariant theories of gravity causal compatible with a given matter theory.
}
\section{2nd Order Metric Gravity}\label{AppGR}
In the following the various expressions for the linearly independent Lorentz invariant expansion coefficients with the given index structure and symmetry that we obtained by means of the developed computer program are displayed. In order to allow for a concise notation we display these in terms of the inverse Minkowski metric $\eta^{ab}$ and the contravariant Levi-Civita symbol $\epsilon^{abcd}$ and furthermore do not denote their symmetries explicitly, as these can easily be read of from the way they appear in the expansion (\ref{LGR}). It is however important to keep in mind that their symmetrization yields further factors that therefore are present when we insert these in the perturbative equivariance equations. We find from the developed computer program:
\begin{align}\label{LorentzGR1}
\begin{alignedat}{2}
\boldsymbol{a_0}&: \ \ \ & a_0  &= \mu_1 \\
\\
\boldsymbol{a^{A}}&: \ \ \ & a^{ab}  &= \mu_2 \cdot \eta^{ab} \\
\\
\boldsymbol{a^{AI}}&: \ \ \ & a^{abpq}  &=  \nu_1 \cdot \eta^{ab}\eta^{pq} + \nu_2 \eta^{pa} \eta^{bq} \\
\\
\boldsymbol{a^{AB}}&: \ \ \ & a^{abcd}  &=  \mu_3 \cdot \eta^{ab}\eta^{cd} + \mu_4 \eta^{ca} \eta^{bd}\\
\\
\boldsymbol{a^{ApBq}}&: \ \ \ & a^{abpcdq}  &= \nu_3 \cdot \eta^{ab}\eta^{pc}\eta^{dq}+\nu_4 \cdot \eta^{ab}\eta^{pq}\eta^{cd}+\nu_5 \cdot \eta^{ap}\eta^{bc}\eta^{dq}+\nu_6 \cdot \eta^{ac}\eta^{bd}\eta^{pq} \\ &  &\hphantom{=} & 
+\nu_7 \cdot \eta^{ac}\eta^{bq}\eta^{pd} +\nu_8 \cdot \epsilon^{apcq}\eta^{bd} \\
\\
\boldsymbol{a^{ABI}}&: \ \ \ & a^{abcdpq}  &= \nu_9 \cdot \eta^{ab}\eta^{cd}\eta^{pq}+\nu_{10} \cdot \eta^{ab}\eta^{cp}\eta^{dq}+\nu_{11} \cdot \eta^{ac}\eta^{bd}\eta^{pq}+\nu_{12} \cdot \eta^{ac}\eta^{bp}\eta^{dq}\\
&  &\hphantom{=} & +\nu_{13} \cdot \eta^{ap}\eta^{bq}\eta^{cd}\\
\\
\boldsymbol{a^{ABC}}&: \ \ \ & a^{abcdef}  &= \mu_5\cdot \eta^{ab}\eta^{cd}\eta^{ef}+\mu_6\cdot \eta^{ab}\eta^{ce}\eta^{df}+\mu_7\cdot \eta^{ac}\eta^{be}\eta^{df}\\
\end{alignedat}
\end{align}
\begin{align}\label{LorentzGR2}
\begin{alignedat}{2}
\boldsymbol{a^{ABpCq}}&: \ \ \ & a^{abcdpefq}  &= \nu_{14}\cdot\eta^{ab}\eta^{cd}\eta^{pe}\eta^{fq}+\nu_{15}\cdot\eta^{ab}\eta^{cd}\eta^{pq}\eta^{ef}
+\nu_{16}\cdot\eta^{ab}\eta^{cp}\eta^{de}\eta^{fq}\\ &  &\hphantom{=} &
+\nu_{17}\cdot\eta^{ab}\eta^{ce}\eta^{df}\eta^{pq}+ 
\nu_{18}\cdot\eta^{ab}\eta^{ce}\eta^{dq}\eta^{pf}+\nu_{19}\cdot\eta^{ac}\eta^{bd}\eta^{pe}\eta^{fq} \\ &  &\hphantom{=} &
+\nu_{20}\cdot\eta^{ac}\eta^{bd}\eta^{pq}\eta^{ef}+\nu_{21}\cdot\eta^{ac}\eta^{bp}\eta^{de}\eta^{fq}+ 
\nu_{22}\cdot\eta^{ac}\eta^{bp}\eta^{dq}\eta^{ef} \\ &  &\hphantom{=} &
+\nu_{23}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fq}+
\nu_{24}\cdot\eta^{ac}\eta^{be}\eta^{df}\eta^{pq}+\nu_{25}\cdot\eta^{ac}\eta^{be}\eta^{dq}\eta^{pf} \\ &  &\hphantom{=} &
+\nu_{26}\cdot\eta^{ac}\eta^{bq}\eta^{dp}\eta^{ef}+\nu_{27}\cdot\eta^{ac}\eta^{bq}\eta^{de}\eta^{pf}+ 
\nu_{28}\cdot\eta^{ap}\eta^{bq}\eta^{cd}\eta^{ef} \\ &  &\hphantom{=} &
+\nu_{29}\cdot\eta^{ap}\eta^{bq}\eta^{ce}\eta^{df} + \nu_{30}\cdot\epsilon^{acpe}\eta^{bd}\eta^{fq}+\nu_{31}\cdot\epsilon^{acpe}\eta^{bf}\eta^{dq} \\ &  &\hphantom{=} &
+\nu_{32}\cdot\epsilon^{acpe}\eta^{bq}\eta^{df}+\nu_{33}\cdot\epsilon^{acpq}\eta^{bd}\eta^{ef} 
+\nu_{34}\cdot\epsilon^{acpq}\eta^{be}\eta^{df} \\
\\
\boldsymbol{a^{ABCI}}&: \ \ \ & a^{abcdefpq}  &= \nu_{35}\cdot\eta^{ab}\eta^{cd}\eta^{ef}\eta^{pq}+\nu_{36}\cdot\eta^{ab}\eta^{cd}\eta^{ep}\eta^{fq}+\nu_{37}\cdot\eta^{ab}\eta^{ce}\eta^{df}\eta^{pq} \\ &  &\hphantom{=} &
+\nu_{38}\cdot\eta^{ab}\eta^{ce}\eta^{dp}\eta^{fq}+\nu_{39}\cdot\eta^{ab}\eta^{cp}\eta^{dq}\eta^{ef}+\nu_{40}\cdot\eta^{ac}\eta^{bd}\eta^{ef}\eta^{pq}
\\ &  &\hphantom{=} &
+\nu_{41}\cdot\eta^{ac}\eta^{bd}\eta^{ep}\eta^{fq}+\nu_{42}\cdot\eta^{ac}\eta^{be}\eta^{df}\eta^{pq}+\nu_{43}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fq}
\\ &  &\hphantom{=} &
+\nu_{44}\cdot\eta^{ac}\eta^{bp}\eta^{dq}\eta^{ef}+\nu_{45}\cdot\eta^{ae}\eta^{bf}\eta^{cp}\eta^{dq}+\nu_{46}\cdot\eta^{ae}\eta^{bp}\eta^{cf}\eta^{dq} \\ &  &\hphantom{=} & 
+\nu_{47}\cdot\epsilon^{acep}\eta^{bf}\eta^{dq}
\end{alignedat}
\end{align}
From inserting these expansion coefficients into the perturbative equivariance equations we get the following relation between the parameters.
\begin{align}\label{GRSol}
\begin{alignedat}{3}
\mu_{{2}} &= 1/4\cdot \boldsymbol{\mu_1} , \hspace{2cm}    &  \mu_{{3}}   &= 1/32\cdot \boldsymbol{\mu_1} , \hspace{2cm}   & \mu_{{4}}   &= -1/16\cdot  \boldsymbol{\mu_1} , \\
%
\mu_{{5}} &= 1/384\cdot \boldsymbol{\mu_1} ,  &  \mu_{{6}}   &= -1/64\cdot \boldsymbol{\mu_1} ,  & \mu_{{7}}   &= 1/48\cdot \boldsymbol{\mu_1} , \\
%
\nu_{{2}} &= - \boldsymbol{\nu_1} ,  & \nu_{{3}}   &= - \boldsymbol{\nu_1} ,  & \nu_{{4}}   &= 1/4\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{5}} &= \boldsymbol{\nu_1} ,  & \nu_{{6}}   &= -3/4 \cdot  \boldsymbol{\nu_1} ,   & \nu_{{7}}    &= 1/2\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{8}} &=  0 ,  & \nu_{{9}}   &= 1/4\cdot \boldsymbol{\nu_1} ,   & \nu_{{10}}    &= -1/4 \cdot  \boldsymbol{\nu_1} , \\
%
\nu_{{11}} &= -1/2\cdot \boldsymbol{\nu_1} ,  &  \nu_{{12}}   &= \boldsymbol{\nu_1} ,   & \nu_{{13}}    &= -1/2\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{14}} &= -1/4\cdot \boldsymbol{\nu_1} ,  &  \nu_{{15}}   &= 1/16\cdot \boldsymbol{\nu_1} ,   & \nu_{{16}}    &= 1/4\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{17}} &= -3/16\cdot \boldsymbol{\nu_1} ,  &  \nu_{{18}}   &= 1/8\cdot \boldsymbol{\nu_1} ,   & \nu_{{19}}    &= 1/2\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{20}} &= -1/4\cdot \boldsymbol{\nu_1} ,  & \nu_{{21}}   &= - \boldsymbol{\nu_1} ,   & \nu_{{22}}    &= 1/2\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{23}} &= -1/2\cdot \boldsymbol{\nu_1} ,  & \nu_{{24}}   &= 3/4\cdot \boldsymbol{\nu_1} ,   & \nu_{{25}}    &= -1/4\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{26}} &= 1/2\cdot \boldsymbol{\nu_1} ,  & \nu_{{27}}   &= -1/2\cdot \boldsymbol{\nu_1} ,   & \nu_{{28}}    &= -1/8\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{29}} &= 3/8\cdot \boldsymbol{\nu_1} ,  & \nu_{{30}}   &=0 ,   & \nu_{{31}}    &= 0 , \\
%
\nu_{{32}} &= 0 ,  & \nu_{{33}}   &= 0 ,   &  \nu_{{34}}    &=  0 , \\
%
\nu_{{35}} &= 1/32\cdot \boldsymbol{\nu_1} ,  & \nu_{{36}}   &= -1/32\cdot \boldsymbol{\nu_1} ,   & \nu_{{37}}    &= -1/8\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{38}} &= 1/4\cdot \boldsymbol{\nu_1} ,  & \nu_{{39}}   &= -1/8\cdot \boldsymbol{\nu_1} ,   & \nu_{{40}}    &= -1/16\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{41}} &= 1/16\cdot \boldsymbol{\nu_1} ,  & \nu_{{42}}   &= 1/4\cdot \boldsymbol{\nu_1} ,   & \nu_{{43}}    &= -1/2\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{44}} &= 1/4\cdot \boldsymbol{\nu_1} ,  & \nu_{{45}}   &= 1/4\cdot \boldsymbol{\nu_1} ,   & \nu_{{46}}    &= -1/4\cdot \boldsymbol{\nu_1} , \\
%
\nu_{{47}} &= 0  &   &   &    & 
\end{alignedat}
\end{align}

\section{2nd Order Area Metric Gravity}
Following along the lines prescribed in (\ref{defI}) and (\ref{defJ}) we get the following non vanishing components for the $\boldsymbol{I^A_{abcd}}$ intertwiner on $F_{Area}$:
\begin{align}\label{AreaI}
    \begin{alignedat}{5}
   I^{0}_{0101} &= 1,   &  \hspace{1cm} 
I^{0}_{0110} &= -1,   &  \hspace{1cm} 
I^{0}_{1001} &= -1,     &  \hspace{1cm} 
I^{0}_{1010} &= 1,   &  \hspace{1cm} 
I^{1}_{0102} &= 1,   \\  
I^{1}_{0120} &= -1,       &
I^{1}_{0201} &= 1,   &  
I^{1}_{0210} &= -1,   &  
I^{1}_{1002} &= -1,       &
I^{1}_{1020} &= 1,   \\
I^{1}_{2001} &= -1,   &  
I^{1}_{2010} &= 1,       &
I^{2}_{0103} &= 1,   &  
I^{2}_{0130} &= -1,   &  
I^{2}_{0301} &= 1,       \\
I^{2}_{0310} &= -1,   &  
I^{2}_{1003} &= -1,   &  
I^{2}_{1030} &= 1,       &
I^{2}_{3001} &= -1,   &  
I^{2}_{3010} &= 1,  \\  
I^{3}_{0112} &= 1,       &
I^{3}_{0121} &= -1,   &  
I^{3}_{1012} &= -1,   &  
I^{3}_{1021} &= 1,       &
I^{3}_{1201} &= 1,   \\  
I^{3}_{1210} &= -1,   &  
I^{3}_{2101} &= -1,       &
I^{3}_{2110} &= 1,   &  
I^{4}_{0113} &= 1,   &  
I^{4}_{0131} &= -1,       \\
I^{4}_{1013} &= -1,   &  
I^{4}_{1031} &= 1,   &  
I^{4}_{1301} &= 1,       &
I^{4}_{1310} &= -1,   &  
I^{4}_{3101} &= -1,   \\  
I^{4}_{3110} &= 1,       &
I^{5}_{0123} &= 1,   &  
I^{5}_{0132} &= -1,   &  
I^{5}_{1023} &= -1,       &
I^{5}_{1032} &= 1,   \\  
I^{5}_{2301} &= 1,   &  
I^{5}_{2310} &= -1,       &
I^{5}_{3201} &= -1,   &  
I^{5}_{3210} &= 1,   &  
I^{6}_{0202} &= 1,       \\
I^{6}_{0220} &= -1,   &  
I^{6}_{2002} &= -1,   &  
I^{6}_{2020} &= 1,       &
I^{7}_{0203} &= 1,   &  
I^{7}_{0230} &= -1,   \\  
I^{7}_{0302} &= 1,       &
I^{7}_{0320} &= -1,   &  
I^{7}_{2003} &= -1,   &  
I^{7}_{2030} &= 1,       &
I^{7}_{3002} &= -1,   \\  
I^{7}_{3020} &= 1,   &  
I^{8}_{0212} &= 1,       &
I^{8}_{0221} &= -1,   &  
I^{8}_{1202} &= 1,   &  
I^{8}_{1220} &= -1,       \\
I^{8}_{2012} &= -1,   &  
I^{8}_{2021} &= 1,   &  
I^{8}_{2102} &= -1,       &
I^{8}_{2120} &= 1,   &  
I^{9}_{0213} &= 1,   \\  
I^{9}_{0231} &= -1,       &
I^{9}_{1302} &= 1,   &  
I^{9}_{1320} &= -1,   &  
I^{9}_{2013} &= -1,       &
I^{9}_{2031} &= 1,   \\  
I^{9}_{3102} &= -1,   &  
I^{9}_{3120} &= 1,       &
I^{10}_{0223} &= 1,   &  
I^{10}_{0232} &= -1,   &  
I^{10}_{2023} &= -1,       \\
I^{10}_{2032} &= 1,   &  
I^{10}_{2302} &= 1,   &  
I^{10}_{2320} &= -1,       &
I^{10}_{3202} &= -1,   &  
I^{10}_{3220} &= 1,   \\  
I^{11}_{0303} &= 1,       &
I^{11}_{0330} &= -1,   &  
I^{11}_{3003} &= -1,   &  
I^{11}_{3030} &= 1,       &
I^{12}_{0312} &= 1,   \\  
I^{12}_{0321} &= -1,   &  
I^{12}_{1203} &= 1,       &
I^{12}_{1230} &= -1,   &  
I^{12}_{2103} &= -1,   &  
I^{12}_{2130} &= 1,       \\
I^{12}_{3012} &= -1,   &  
I^{12}_{3021} &= 1,   &  
I^{13}_{0313} &= 1,       &
I^{13}_{0331} &= -1,   &  
I^{13}_{1303} &= 1,   \\  
I^{13}_{1330} &= -1,       &
I^{13}_{3013} &= -1,   &  
I^{13}_{3031} &= 1,   &  
I^{13}_{3103} &= -1,       &
I^{13}_{3130} &= 1,   \\  
I^{14}_{0323} &= 1,   &  
I^{14}_{0332} &= -1,       &
I^{14}_{2303} &= 1,   &  
I^{14}_{2330} &= -1,   &  
I^{14}_{3023} &= -1,       \\
I^{14}_{3032} &= 1,   &  
I^{14}_{3203} &= -1,   &  
I^{14}_{3230} &= 1,       &
I^{15}_{1212} &= 1,   &  
I^{15}_{1221} &= -1,   \\  
I^{15}_{2112} &= -1,       &
I^{15}_{2121} &= 1,   &  
I^{16}_{1213} &= 1,   &  
I^{16}_{1231} &= -1,       &
I^{16}_{1312} &= 1,   \\  
I^{16}_{1321} &= -1,   &  
I^{16}_{2113} &= -1,       &
I^{16}_{2131} &= 1,   &  
I^{16}_{3112} &= -1,   &  
I^{16}_{3121} &= 1,       \\
I^{17}_{1223} &= 1,   &  
I^{17}_{1232} &= -1,   &  
I^{17}_{2123} &= -1,       &
I^{17}_{2132} &= 1,   &  
I^{17}_{2312} &= 1,   \\  
I^{17}_{2321} &= -1,       &
I^{17}_{3212} &= -1,   &  
I^{17}_{3221} &= 1,   &  
I^{18}_{1313} &= 1,       &
I^{18}_{1331} &= -1,   \\  
I^{18}_{3113} &= -1,   &  
I^{18}_{3131} &= 1,       &
I^{19}_{1323} &= 1,   &  
I^{19}_{1332} &= -1,   &  
I^{19}_{2313} &= 1,       \\
I^{19}_{2331} &= -1,   &  
I^{19}_{3123} &= -1,   &  
I^{19}_{3132} &= 1,       &
I^{19}_{3213} &= -1,   &  
I^{19}_{3231} &= 1,   \\  
I^{20}_{2323} &= 1,       &
I^{20}_{2332} &= -1,   &  
I^{20}_{3223} &= -1,   &  
I^{20}_{3232} &= 1.   &  
    \end{alignedat}
\end{align}
Further we get the following non zero components for the $\boldsymbol{J_A^{abcd}}$ intertwiner:
\begin{align}\label{AreaJ}
    \begin{alignedat}{5}
    J_{0}^{0101} &= 1/4, & \hspace{1cm}
J_{0}^{0110} &= -1/4, & \hspace{1cm}
J_{0}^{1001} &= -1/4, & \hspace{1cm}
J_{0}^{1010} &= 1/4, & \hspace{1cm}
J_{1}^{0102} &= 1/8, \\ 
J_{1}^{0120} &= -1/8, & 
J_{1}^{0201} &= 1/8, & 
J_{1}^{0210} &= -1/8, & 
J_{1}^{1002} &= -1/8, & 
J_{1}^{1020} &= 1/8, \\ 
J_{1}^{2001} &= -1/8, & 
J_{1}^{2010} &= 1/8, & 
J_{2}^{0103} &= 1/8, & 
J_{2}^{0130} &= -1/8, & 
J_{2}^{0301} &= 1/8, \\ 
J_{2}^{0310} &= -1/8, & 
J_{2}^{1003} &= -1/8, & 
J_{2}^{1030} &= 1/8, & 
J_{2}^{3001} &= -1/8, & 
J_{2}^{3010} &= 1/8, \\ 
J_{3}^{0112} &= 1/8, & 
J_{3}^{0121} &= -1/8, & 
J_{3}^{1012} &= -1/8, & 
J_{3}^{1021} &= 1/8, & 
J_{3}^{1201} &= 1/8, \\ 
J_{3}^{1210} &= -1/8, & 
J_{3}^{2101} &= -1/8, & 
J_{3}^{2110} &= 1/8, & 
J_{4}^{0113} &= 1/8, & 
J_{4}^{0131} &= -1/8, \\ 
J_{4}^{1013} &= -1/8, & 
J_{4}^{1031} &= 1/8, & 
J_{4}^{1301} &= 1/8, & 
J_{4}^{1310} &= -1/8, & 
J_{4}^{3101} &= -1/8,  \\
J_{4}^{3110} &= 1/8, & 
J_{5}^{0123} &= 1/8, & 
J_{5}^{0132} &= -1/8, & 
J_{5}^{1023} &= -1/8, & 
J_{5}^{1032} &= 1/8, \\ 
J_{5}^{2301} &= 1/8, & 
J_{5}^{2310} &= -1/8, & 
J_{5}^{3201} &= -1/8, & 
J_{5}^{3210} &= 1/8, & 
J_{6}^{0202} &= 1/4, \\ 
J_{6}^{0220} &= -1/4, & 
J_{6}^{2002} &= -1/4, & 
J_{6}^{2020} &= 1/4, & 
J_{7}^{0203} &= 1/8, & 
J_{7}^{0230} &= -1/8, \\ 
J_{7}^{0302} &= 1/8, & 
J_{7}^{0320} &= -1/8, & 
J_{7}^{2003} &= -1/8, & 
J_{7}^{2030} &= 1/8, & 
J_{7}^{3002} &= -1/8, \\ 
J_{7}^{3020} &= 1/8, & 
J_{8}^{0212} &= 1/8, & 
J_{8}^{0221} &= -1/8, & 
J_{8}^{1202} &= 1/8, & 
J_{8}^{1220} &= -1/8, \\ 
J_{8}^{2012} &= -1/8, & 
J_{8}^{2021} &= 1/8, & 
J_{8}^{2102} &= -1/8, & 
J_{8}^{2120} &= 1/8, & 
J_{9}^{0213} &= 1/8, \\ 
J_{9}^{0231} &= -1/8, & 
J_{9}^{1302} &= 1/8, & 
J_{9}^{1320} &= -1/8, & 
J_{9}^{2013} &= -1/8, &
J_{9}^{2031} &= 1/8, \\ 
J_{9}^{3102} &= -1/8, & 
J_{9}^{3120} &= 1/8, & 
J_{10}^{0223} &= 1/8, & 
J_{10}^{0232} &= -1/8, &
J_{10}^{2023} &= -1/8, \\ 
J_{10}^{2032} &= 1/8, & 
J_{10}^{2302} &= 1/8, & 
J_{10}^{2320} &= -1/8, & 
J_{10}^{3202} &= -1/8, & 
J_{10}^{3220} &= 1/8, \\ 
J_{11}^{0303} &= 1/4, & 
J_{11}^{0330} &= -1/4, & 
J_{11}^{3003} &= -1/4, & 
J_{11}^{3030} &= 1/4, & 
J_{12}^{0312} &= 1/8, \\ 
J_{12}^{0321} &= -1/8, & 
J_{12}^{1203} &= 1/8, & 
J_{12}^{1230} &= -1/8, & 
J_{12}^{2103} &= -1/8, & 
J_{12}^{2130} &= 1/8, \\ 
J_{12}^{3012} &= -1/8, & 
J_{12}^{3021} &= 1/8, & 
J_{13}^{0313} &= 1/8, & 
J_{13}^{0331} &= -1/8, & 
J_{13}^{1303} &= 1/8, \\ 
J_{13}^{1330} &= -1/8, & 
J_{13}^{3013} &= -1/8, & 
J_{13}^{3031} &= 1/8, & 
J_{13}^{3103} &= -1/8, & 
J_{13}^{3130} &= 1/8, \\ 
J_{14}^{0323} &= 1/8, & 
J_{14}^{0332} &= -1/8, & 
J_{14}^{2303} &= 1/8, & 
J_{14}^{2330} &= -1/8, & 
J_{14}^{3023} &= -1/8, \\ 
J_{14}^{3032} &= 1/8, & 
J_{14}^{3203} &= -1/8, & 
J_{14}^{3230} &= 1/8, & 
J_{15}^{1212} &= 1/4, & 
J_{15}^{1221} &= -1/4, \\ 
J_{15}^{2112} &= -1/4, & 
J_{15}^{2121} &= 1/4, & 
J_{16}^{1213} &= 1/8, & 
J_{16}^{1231} &= -1/8, & 
J_{16}^{1312} &= 1/8, \\ 
J_{16}^{1321} &= -1/8, & 
J_{16}^{2113} &= -1/8, & 
J_{16}^{2131} &= 1/8, & 
J_{16}^{3112} &= -1/8, & 
J_{16}^{3121} &= 1/8, \\ 
J_{17}^{1223} &= 1/8, & 
J_{17}^{1232} &= -1/8, & 
J_{17}^{2123} &= -1/8, & 
J_{17}^{2132} &= 1/8, & 
J_{17}^{2312} &= 1/8, \\ 
J_{17}^{2321} &= -1/8, & 
J_{17}^{3212} &= -1/8, & 
J_{17}^{3221} &= 1/8, & 
J_{18}^{1313} &= 1/4, & 
J_{18}^{1331} &= -1/4, \\ 
J_{18}^{3113} &= -1/4, & 
J_{18}^{3131} &= 1/4, & 
J_{19}^{1323} &= 1/8, & 
J_{19}^{1332} &= -1/8, & 
J_{19}^{2313} &= 1/8, \\ 
J_{19}^{2331} &= -1/8, & 
J_{19}^{3123} &= -1/8, & 
J_{19}^{3132} &= 1/8, & 
J_{19}^{3213} &= -1/8, & 
J_{19}^{3231} &= 1/8, \\ 
J_{20}^{2323} &= 1/4, & 
J_{20}^{2332} &= -1/4, & 
J_{20}^{3223} &= -1/4, & 
J_{20}^{3232} &= 1/4  
    \end{alignedat}
\end{align}
%remove website links from bibliography
When computing the Lorentz invariant expansion coefficients that occur in (\ref{LArea}) using efficient compute algebra is now essential. In principal they are constructed exactly the same way as in the metric case. As a result of the increased fibre dimension of $F_{Area}$ compared with the metric field bundle $F_{GR}$ however the general expressions for these Lorentz invariant coefficients are strikingly more complicated. We again do not display the factorless symmetrizations explicitly to allow for more concise expressions.
\begin{align}\label{LorentzArea1}
\begin{alignedat}{2}
\boldsymbol{a_0}&: \ \ \ & a_0  &= \mu_1 \\
\\
\boldsymbol{a^{A}}&: \ \ \ & a^{abcd}  &= \mu_2 \cdot \eta^{ac} \eta^{bd} + \mu_{3} \cdot \epsilon^{abcd} \\
\\
\boldsymbol{a^{AI}}&: \ \ \ & a^{abcdpq}  &=  \nu_{1}\cdot\eta^{ac}\eta^{bd}\eta^{pq}+\nu_{2}\cdot\eta^{ac}\eta^{bp}\eta^{dq} + \nu_{3}\cdot\epsilon^{abcd}\eta^{pq}\\
\\
\boldsymbol{a^{AB}}&: \ \ \ & a^{abcdefgh}  &= \mu_{4}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}+\mu_{5}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}+\mu_{6}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh} \\
& & &\hphantom{=} +\mu_{7}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh} + \mu_{8}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}+\mu_{9}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\\
\\
\boldsymbol{a^{ApBq}}&: \ \ \ & a^{abcdpefghq}  &=
\nu_{4}\cdot\eta^{ac}\eta^{bd}\eta^{pe}\eta^{fg}\eta^{hq}+\nu_{5}\cdot\eta^{ac}\eta^{bd}\eta^{pq}\eta^{eg}\eta^{fh}+\nu_{6}\cdot\eta^{ac}\eta^{bp}\eta^{de}\eta^{fg}\eta^{hq}\\
& & &\hphantom{=}+\nu_{7}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{pf}\eta^{hq}+\nu_{8}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{pq}\eta^{fh}+\nu_{9}\cdot\eta^{ac}\eta^{be}\eta^{dq}\eta^{pg}\eta^{fh}\\
& & &\hphantom{=}+\nu_{10}\cdot\eta^{ap}\eta^{be}\eta^{cf}\eta^{dg}\eta^{hq}+\nu_{11}\cdot\eta^{ap}\eta^{be}\eta^{cg}\eta^{dh}\eta^{fq}+\nu_{12}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{pq}\\
& & &\hphantom{=} +\nu_{13}\cdot\epsilon^{abcd}\eta^{pe}\eta^{fg}\eta^{hq}+\nu_{14}\cdot\epsilon^{abcd}\eta^{pq}\eta^{eg}\eta^{fh}+\nu_{15}\cdot\epsilon^{abpe}\eta^{cf}\eta^{dg}\eta^{hq}\\
& & &\hphantom{=}+\nu_{16}\cdot\epsilon^{abpe}\eta^{cg}\eta^{dq}\eta^{fh}+\nu_{17}\cdot\epsilon^{abef}\eta^{cp}\eta^{dg}\eta^{hq}+\nu_{18}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{pq}\\
\\
\boldsymbol{a^{ABI}}&: \ \ \ & a^{abcdefghpq}  &=
\nu_{19}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{pq}+\nu_{20}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fp}\eta^{hq}+\nu_{21}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}\eta^{pq}\\
& & &\hphantom{=}+\nu_{22}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fp}\eta^{hq}+\nu_{23}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fg}\eta^{hq}+\nu_{24}\cdot\eta^{ac}\eta^{bp}\eta^{dq}\eta^{eg}\eta^{fh}\\
& & &\hphantom{=}+\nu_{25}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{pq}+\nu_{26}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dp}\eta^{hq}+\nu_{27}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh}\eta^{pq}\\
& & &\hphantom{=} +\nu_{28}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{pq}+\nu_{29}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fp}\eta^{hq}+\nu_{30}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{pq}\\
& & &\hphantom{=}+\nu_{31}\cdot\epsilon^{abef}\eta^{cg}\eta^{dp}\eta^{hq}+\nu_{32}\cdot\epsilon^{abep}\eta^{cf}\eta^{dg}\eta^{hq}+\nu_{33}\cdot\epsilon^{abep}\eta^{cg}\eta^{dh}\eta^{fq}\\
& & &\hphantom{=}+\nu_{34}\cdot\epsilon^{efgh}\eta^{ac}\eta^{bd}\eta^{pq}\\
\\
\boldsymbol{a^{ABC}}&: \ \ \ & a^{abcdefghijkl}  &=
\mu_{10}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jl}+\mu_{11}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jl}\\
& & &\hphantom{=}+\mu_{12}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}+\mu_{13}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fk}\eta^{gj}\eta^{hl}\\
& & &\hphantom{=}+\mu_{14}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hk}\eta^{jl}+\mu_{15}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{jl}\\
& & &\hphantom{=}+\mu_{16}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}+\mu_{17}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dk}\eta^{gj}\eta^{hl}\\
& & &\hphantom{=}+\mu_{18}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jl}+\mu_{19}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jl}\\
& & &\hphantom{=}+\mu_{20}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}+\mu_{21}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fk}\eta^{gj}\eta^{hl}\\
& & &\hphantom{=}+\mu_{22}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{ik}\eta^{jl}+\mu_{23}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{jl}\\
& & &\hphantom{=}+\mu_{24}\cdot\epsilon^{abef}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}
\end{alignedat}
\end{align}
\begin{align}\label{LorentzArea2}
\begin{alignedat}{2}
\boldsymbol{a^{ABpCq}}&: \ \ \ & a^{abcdefghpijklq}  &= \nu_{35}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{pi}\eta^{jk}\eta^{lq}+\nu_{36}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{pq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{37}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fp}\eta^{hi}\eta^{jk}\eta^{lq}+\nu_{38}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{pj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{39}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{pq}\eta^{jl}+\nu_{40}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hq}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{41}\cdot\eta^{ac}\eta^{bd}\eta^{ep}\eta^{fi}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{42}\cdot\eta^{ac}\eta^{bd}\eta^{ep}\eta^{fi}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{43}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{44}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}\eta^{pi}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{45}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}\eta^{pq}\eta^{ik}\eta^{jl}+\nu_{46}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fp}\eta^{hi}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{47}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fp}\eta^{hq}\eta^{ik}\eta^{jl}+\nu_{48}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hk}\eta^{pj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{49}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hk}\eta^{pq}\eta^{jl}+\nu_{50}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hq}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{51}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fg}\eta^{hi}\eta^{jk}\eta^{lq}+\nu_{52}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fg}\eta^{hq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{53}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fi}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{54}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fi}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{55}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fi}\eta^{gk}\eta^{hq}\eta^{jl}+\nu_{56}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hp}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{57}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hj}\eta^{pk}\eta^{lq}+\nu_{58}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{pj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{59}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{pl}\eta^{jq}+\nu_{60}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{pq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{61}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hq}\eta^{pk}\eta^{jl}+\nu_{62}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fp}\eta^{gj}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{63}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fp}\eta^{gk}\eta^{hl}\eta^{jq}+\nu_{64}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fj}\eta^{gp}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{65}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fk}\eta^{gj}\eta^{hq}\eta^{pl}+\nu_{66}\cdot\eta^{ac}\eta^{be}\eta^{dq}\eta^{fg}\eta^{hp}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{67}\cdot\eta^{ac}\eta^{be}\eta^{dq}\eta^{fg}\eta^{hi}\eta^{pk}\eta^{jl}+\nu_{68}\cdot\eta^{ac}\eta^{bp}\eta^{dq}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{69}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{pi}\eta^{jk}\eta^{lq}+\nu_{70}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{pq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{71}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dp}\eta^{hi}\eta^{jk}\eta^{lq}+\nu_{72}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dp}\eta^{hq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{73}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hp}\eta^{jk}\eta^{lq}+\nu_{74}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hj}\eta^{pk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{75}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pj}\eta^{lq}+\nu_{76}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{77}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pq}\eta^{jl}+\nu_{78}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{di}\eta^{hq}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{79}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dq}\eta^{hp}\eta^{ik}\eta^{jl}+\nu_{80}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dq}\eta^{hi}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{81}\cdot\eta^{ae}\eta^{bf}\eta^{cp}\eta^{di}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{82}\cdot\eta^{ae}\eta^{bf}\eta^{cp}\eta^{di}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{83}\cdot\eta^{ae}\eta^{bf}\eta^{cp}\eta^{di}\eta^{gk}\eta^{hq}\eta^{jl}+\nu_{84}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dj}\eta^{gp}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{85}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{86}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dk}\eta^{gp}\eta^{hj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{87}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh}\eta^{pi}\eta^{jk}\eta^{lq}+\nu_{88}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh}\eta^{pq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{89}\cdot\eta^{ae}\eta^{bp}\eta^{cf}\eta^{di}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{90}\cdot\eta^{ae}\eta^{bp}\eta^{cf}\eta^{di}\eta^{gk}\eta^{hq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{91}\cdot\eta^{ae}\eta^{bi}\eta^{cf}\eta^{dj}\eta^{gp}\eta^{hk}\eta^{lq}+\nu_{92}\cdot\eta^{ae}\eta^{bi}\eta^{cf}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}
\\
& & &\hphantom{=}+ \text{...}
\end{alignedat}
\end{align}

\begin{align}\label{LorentzArea3}
\begin{alignedat}{2}
\hphantom{\boldsymbol{a^{ABpCq}}}&\hphantom{:} \ \ \ & \hphantom{a^{abcdefghpijklq}} ...  &\hphantom{=} +
\nu_{93}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{pi}\eta^{jk}\eta^{lq}+\nu_{94}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{pq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{95}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fp}\eta^{hi}\eta^{jk}\eta^{lq}+\nu_{96}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{pj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{97}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{pq}\eta^{jl}+\nu_{98}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hq}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{99}\cdot\epsilon^{abcd}\eta^{ep}\eta^{fi}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{100}\cdot\epsilon^{abcd}\eta^{ep}\eta^{fi}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{101}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{102}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{pi}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{103}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{pq}\eta^{ik}\eta^{jl}+\nu_{104}\cdot\epsilon^{abef}\eta^{cg}\eta^{dp}\eta^{hi}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{105}\cdot\epsilon^{abef}\eta^{cg}\eta^{dp}\eta^{hq}\eta^{ik}\eta^{jl}+\nu_{106}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hp}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{107}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hj}\eta^{pk}\eta^{lq}+\nu_{108}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{109}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pl}\eta^{jq}+\nu_{110}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{pq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{111}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hq}\eta^{pk}\eta^{jl}+\nu_{112}\cdot\epsilon^{abef}\eta^{cg}\eta^{dq}\eta^{hp}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{113}\cdot\epsilon^{abef}\eta^{cg}\eta^{dq}\eta^{hi}\eta^{pk}\eta^{jl}+\nu_{114}\cdot\epsilon^{abef}\eta^{cp}\eta^{di}\eta^{gj}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{115}\cdot\epsilon^{abef}\eta^{cp}\eta^{di}\eta^{gk}\eta^{hl}\eta^{jq}+\nu_{116}\cdot\epsilon^{abef}\eta^{cp}\eta^{di}\eta^{gk}\eta^{hq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{117}\cdot\epsilon^{abef}\eta^{ci}\eta^{dj}\eta^{gp}\eta^{hk}\eta^{lq}+\nu_{118}\cdot\epsilon^{abef}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{119}\cdot\epsilon^{abef}\eta^{ci}\eta^{dk}\eta^{gp}\eta^{hj}\eta^{lq}+\nu_{120}\cdot\epsilon^{abep}\eta^{cf}\eta^{dg}\eta^{hi}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{121}\cdot\epsilon^{abep}\eta^{cf}\eta^{dg}\eta^{hq}\eta^{ik}\eta^{jl}+\nu_{122}\cdot\epsilon^{abep}\eta^{cf}\eta^{di}\eta^{gj}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{123}\cdot\epsilon^{abep}\eta^{cf}\eta^{di}\eta^{gk}\eta^{hl}\eta^{jq}+\nu_{124}\cdot\epsilon^{abep}\eta^{cf}\eta^{di}\eta^{gk}\eta^{hq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{125}\cdot\epsilon^{abep}\eta^{cg}\eta^{dh}\eta^{fi}\eta^{jk}\eta^{lq}+\nu_{126}\cdot\epsilon^{abep}\eta^{cg}\eta^{dh}\eta^{fq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{127}\cdot\epsilon^{abep}\eta^{cg}\eta^{di}\eta^{fj}\eta^{hk}\eta^{lq}+\nu_{128}\cdot\epsilon^{abep}\eta^{cg}\eta^{di}\eta^{fk}\eta^{hq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{129}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hp}\eta^{jk}\eta^{lq}+\nu_{130}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hj}\eta^{pk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{131}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hk}\eta^{pj}\eta^{lq}+\nu_{132}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hk}\eta^{pl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{133}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hk}\eta^{pq}\eta^{jl}+\nu_{134}\cdot\epsilon^{abei}\eta^{cf}\eta^{dg}\eta^{hq}\eta^{pk}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{135}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{136}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{137}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gk}\eta^{hq}\eta^{jl}+\nu_{138}\cdot\epsilon^{abei}\eta^{cf}\eta^{dj}\eta^{gp}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{139}\cdot\epsilon^{abei}\eta^{cf}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{140}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gp}\eta^{hj}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{141}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gp}\eta^{hl}\eta^{jq}+\nu_{142}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gj}\eta^{hq}\eta^{pl}\\
& & &\hphantom{=}
+\nu_{143}\cdot\epsilon^{abeq}\eta^{cf}\eta^{dg}\eta^{hp}\eta^{ik}\eta^{jl}+\nu_{144}\cdot\epsilon^{efgh}\eta^{ac}\eta^{bd}\eta^{pi}\eta^{jk}\eta^{lq}
\end{alignedat}
\end{align}
\begin{align}\label{LorentzArea4}
\begin{alignedat}{2}
\boldsymbol{a^{ABpCq}}&: \ \ \ & a^{abcdefghpijklq}  &=
\nu_{145}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jl}\eta^{pq}+\nu_{146}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{147}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jl}\eta^{pq}+\nu_{148}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{149}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fi}\eta^{hp}\eta^{jk}\eta^{lq}+\nu_{150}\cdot\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fp}\eta^{hq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{151}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{152}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{153}\cdot\eta^{ac}\eta^{bd}\eta^{ei}\eta^{fk}\eta^{gj}\eta^{hl}\eta^{pq}+\nu_{154}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}\eta^{ik}\eta^{jl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{155}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fh}\eta^{ik}\eta^{jp}\eta^{lq}+\nu_{156}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hk}\eta^{jl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{157}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hk}\eta^{jp}\eta^{lq}+\nu_{158}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fi}\eta^{hp}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{159}\cdot\eta^{ac}\eta^{be}\eta^{dg}\eta^{fp}\eta^{hq}\eta^{ik}\eta^{jl}+\nu_{160}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{jl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{161}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hk}\eta^{jp}\eta^{lq}+\nu_{162}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fg}\eta^{hp}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{163}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{164}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fj}\eta^{gk}\eta^{hp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{165}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fk}\eta^{gj}\eta^{hp}\eta^{lq}+\nu_{166}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fk}\eta^{gl}\eta^{hp}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{167}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fp}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{168}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fp}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{169}\cdot\eta^{ac}\eta^{be}\eta^{di}\eta^{fp}\eta^{gk}\eta^{hq}\eta^{jl}+\nu_{170}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fg}\eta^{hq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{171}\cdot\eta^{ac}\eta^{be}\eta^{dp}\eta^{fi}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{172}\cdot\eta^{ac}\eta^{bi}\eta^{dk}\eta^{eg}\eta^{fp}\eta^{hq}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{173}\cdot\eta^{ac}\eta^{bi}\eta^{dk}\eta^{ej}\eta^{fp}\eta^{gl}\eta^{hq}+\nu_{174}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{ik}\eta^{jl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{175}\cdot\eta^{ae}\eta^{bf}\eta^{cg}\eta^{dh}\eta^{ik}\eta^{jp}\eta^{lq}+\nu_{176}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{177}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hp}\eta^{lq}+\nu_{178}\cdot\eta^{ae}\eta^{bf}\eta^{ci}\eta^{dk}\eta^{gj}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{179}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh}\eta^{ik}\eta^{jl}\eta^{pq}+\nu_{180}\cdot\eta^{ae}\eta^{bg}\eta^{cf}\eta^{dh}\eta^{ik}\eta^{jp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{181}\cdot\eta^{ae}\eta^{bg}\eta^{ci}\eta^{dj}\eta^{fk}\eta^{hl}\eta^{pq}+\nu_{182}\cdot\eta^{ae}\eta^{bg}\eta^{ci}\eta^{dj}\eta^{fk}\eta^{hp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{183}\cdot\eta^{ae}\eta^{bi}\eta^{cg}\eta^{dk}\eta^{fp}\eta^{hq}\eta^{jl}+ \nu_{184}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{185}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fh}\eta^{ik}\eta^{jp}\eta^{lq}+\nu_{186}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jl}\eta^{pq}
\\
& & &\hphantom{=}
+\nu_{187}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hk}\eta^{jp}\eta^{lq}+\nu_{188}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fi}\eta^{hp}\eta^{jk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{189}\cdot\epsilon^{abcd}\eta^{eg}\eta^{fp}\eta^{hq}\eta^{ik}\eta^{jl}+\nu_{190}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{191}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fj}\eta^{gk}\eta^{hp}\eta^{lq}+\nu_{192}\cdot\epsilon^{abcd}\eta^{ei}\eta^{fk}\eta^{gj}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{193}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{ik}\eta^{jl}\eta^{pq}+\nu_{194}\cdot\epsilon^{abef}\eta^{cg}\eta^{dh}\eta^{ik}\eta^{jp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{195}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{jl}\eta^{pq}+\nu_{196}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hk}\eta^{jp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{197}\cdot\epsilon^{abef}\eta^{cg}\eta^{di}\eta^{hp}\eta^{jk}\eta^{lq}+\nu_{198}\cdot\epsilon^{abef}\eta^{cg}\eta^{dp}\eta^{hq}\eta^{ik}\eta^{jl}\\
& & &\hphantom{=}
+\nu_{199}\cdot\epsilon^{abef}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{200}\cdot\epsilon^{abef}\eta^{ci}\eta^{dj}\eta^{gk}\eta^{hp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{201}\cdot\epsilon^{abef}\eta^{ci}\eta^{dk}\eta^{gj}\eta^{hl}\eta^{pq}+\nu_{202}\cdot\epsilon^{abei}\eta^{cf}\eta^{dj}\eta^{gk}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{203}\cdot\epsilon^{abei}\eta^{cf}\eta^{dj}\eta^{gk}\eta^{hp}\eta^{lq}+\nu_{204}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gj}\eta^{hl}\eta^{pq}\\
& & &\hphantom{=}
+\nu_{205}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gj}\eta^{hp}\eta^{lq}+\nu_{206}\cdot\epsilon^{abei}\eta^{cf}\eta^{dk}\eta^{gl}\eta^{hp}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{207}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gj}\eta^{hk}\eta^{lq}+\nu_{208}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gk}\eta^{hl}\eta^{jq}\\
& & &\hphantom{=}
+\nu_{209}\cdot\epsilon^{abei}\eta^{cf}\eta^{dp}\eta^{gk}\eta^{hq}\eta^{jl}+\nu_{210}\cdot\epsilon^{abep}\eta^{cf}\eta^{di}\eta^{gj}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{211}\cdot\epsilon^{abij}\eta^{ce}\eta^{df}\eta^{gk}\eta^{hl}\eta^{pq}+\nu_{212}\cdot\epsilon^{abij}\eta^{ce}\eta^{df}\eta^{gk}\eta^{hp}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{213}\cdot\epsilon^{abij}\eta^{ce}\eta^{dk}\eta^{fp}\eta^{gl}\eta^{hq}+\nu_{214}\cdot\epsilon^{abip}\eta^{ce}\eta^{df}\eta^{gj}\eta^{hk}\eta^{lq}\\
& & &\hphantom{=}
+\nu_{215}\cdot\epsilon^{abip}\eta^{ce}\eta^{df}\eta^{gk}\eta^{hl}\eta^{jq}+\nu_{216}\cdot\epsilon^{ijkl}\eta^{ac}\eta^{bd}\eta^{eg}\eta^{fh}\eta^{pq}
\end{alignedat}
\end{align}
%comment on prod of eps = eta

Inserting these expressions in the perturbative equivariance equations (\ref{order1}), (\ref{order2}) and (\ref{order3}) ans solving these with respect to the $52$ parameters:
\begin{multline}\label{AreaParas}
\bigl\{\,\  \mu_{{3}},\mu_{{5}},\mu_{{6}},\mu_{{7}},\mu_{{11}},\mu_{{13}}
,\mu_{{14}},\mu_{{17}},\mu_{{19}},\mu_{{23}},\nu_{{7}},\nu_{{9}},\nu_{
{11}},\nu_{{20}},\nu_{{41}},\nu_{{42}},\nu_{{48}},\nu_{{51}},\nu_{{55}
},\nu_{{58}},\\\nu_{{59}},\nu_{{69}},\nu_{{77}},\nu_{{79}},\nu_{{80}},
\nu_{{81}},\nu_{{83}},\nu_{{90}},\nu_{{108}},\nu_{{109}},\nu_{{119}},
\nu_{{124}},\nu_{{127}},\nu_{{131}},\nu_{{132}},\nu_{{135}},\nu_{{137}
},\\\nu_{{139}},\nu_{{149}},\nu_{{151}},\nu_{{153}},\nu_{{155}},\nu_{{
160}},\nu_{{164}},\nu_{{165}},\nu_{{175}},\nu_{{178}},\nu_{{181}},\nu_
{{194}},\nu_{{202}},\nu_{{204}},\nu_{{206}} \,\  \bigr\}
\end{multline}
We get the following expressions for the remaining $188$ parameters in terms of these:
\begin{align}\label{AreaSol1}
    \begin{aligned}
\mu_{{1}}&=-1536\,\mu_{{5}}-3072\,\mu_{{6}}-1536\,\mu_{{7}}-
192\,\mu_{{3}}\\
\\
\mu_{{2}}&=-16\,\mu_{{5}}-32\,\mu_{{6}}-16\,\mu_{{7}},\\
\\
\mu_{{4}}&=-1/4\,\mu_{{5}}-1/3\,\mu_{{6}}+1/12\,\mu_{{7}}, \\
\\
\mu_{{8}}&=8\,
\mu_{{19}}+1/6\,\mu_{{7}}-1/6\,\mu_{{6}}+8\,\mu_{{23}}-1/24\,\mu_{{5}}
-4\,\mu_{{11}}-14/3\,\mu_{{14}}, \\
\\
\mu_{{9}}&=-24\,\mu_{{23}}+1/8\,\mu_{{5
}}+12\,\mu_{{11}}+14\,\mu_{{14}}-24\,\mu_{{19}}, \\
\\
\mu_{{10}}&=-{\frac {5
\,\mu_{{11}}}{36}}-{\frac {\mu_{{6}}}{864}}+{\frac {\mu_{{7}}}{864}}+1
/12\,\mu_{{13}}-{\frac {\mu_{{14}}}{216}}+1/36\,\mu_{{17}}+{\frac {\mu
_{{5}}}{1728}}, \\
\\
\mu_{{12}}&=1/48\,\mu_{{6}}-{\frac {\mu_{{7}}}{192}}-1/2
\,\mu_{{11}}-1/2\,\mu_{{13}}-1/2\,\mu_{{14}}, \\
\\
\mu_{{15}}&=-\mu_{{14}},\\
\\
\mu_{{16}}&=-{\frac {\mu_{{5}}}{96}}-1/6\,\mu_{{14}}-1/24\,\mu_{{6}}-1/
48\,\mu_{{7}}-1/2\,\mu_{{17}}, \\
\\
\mu_{{18}}&=-{\frac {\mu_{{6}}}{288}}-{
\frac {\mu_{{7}}}{2304}}+1/8\,\mu_{{13}}-{\frac {\mu_{{14}}}{72}}-1/12
\,\mu_{{19}}-{\frac {\mu_{{5}}}{1152}}, \\
\\
\mu_{{20}}&=-{\frac {17\,\mu_{{7
}}}{3456}}-1/12\,\mu_{{13}}-1/36\,\mu_{{17}}+{\frac {7\,\mu_{{6}}}{
1728}}-{\frac {7\,\mu_{{19}}}{12}}-{\frac {7\,\mu_{{23}}}{36}}+{\frac 
{7\,\mu_{{5}}}{6912}}+{\frac {7\,\mu_{{11}}}{72}}+{\frac {49\,\mu_{{14
}}}{432}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol2}
\begin{aligned}
\mu_{{21}}&={\frac {\mu_{{6}}}{432}}-{\frac {\mu_{{7}}}{1728}
}-1/3\,\mu_{{19}}+1/6\,\mu_{{13}}+1/18\,\mu_{{17}}-1/9\,\mu_{{23}}+{
\frac {\mu_{{5}}}{1728}}+1/18\,\mu_{{11}}+{\frac {7\,\mu_{{14}}}{108}},\\
\\
\mu_{{22}}&={\frac {\mu_{{5}}}{192}}+1/4\,\mu_{{11}}+1/3\,\mu_{{14}}+{
\frac {\mu_{{6}}}{96}}+{\frac {\mu_{{7}}}{192}}-1/4\,\mu_{{23}}-1/2\,
\mu_{{19}}+1/8\,\mu_{{17}}, \\
\\
\mu_{{24}}&=-{\frac {\mu_{{5}}}{128}}-3/4\,
\mu_{{11}}-{\frac {7\,\mu_{{14}}}{8}}+3/2\,\mu_{{19}}+1/2\,\mu_{{23}},\\
\\
\nu_{{1}}&=-64\,\nu_{{83}}-32\,\nu_{{90}}-48\,\nu_{{59}}+2\,\nu_{{9}}-
48\,\nu_{{48}}-16\,\nu_{{55}}-48\,\nu_{{58}}-1/2\,\nu_{{7}}, \\
\\
\nu_{{2}}&=
4\,\nu_{{7}}-16\,\nu_{{9}}, \\
\\
\nu_{{3}}&=3/4\,\nu_{{7}}-3\,\nu_{{9}}-32\,
\nu_{{83}}-16\,\nu_{{90}}-24\,\nu_{{59}}-24\,\nu_{{48}}-8\,\nu_{{55}}-
24\,\nu_{{58}}, \\
\\
\nu_{{4}}&=-8\,\nu_{{83}}-4\,\nu_{{90}}-6\,\nu_{{59}}+1/
4\,\nu_{{9}}-6\,\nu_{{48}}-2\,\nu_{{55}}-6\,\nu_{{58}}+{\frac {7\,\nu_
{{7}}}{16}}, \\
\\
\nu_{{5}}&={\frac {\nu_{{7}}}{96}}-8/3\,\nu_{{48}}-4/3\,\nu
_{{55}}-8/3\,\nu_{{59}}+1/8\,\nu_{{11}}-4\,\nu_{{83}}-2\,\nu_{{90}}-8/
3\,\nu_{{58}}+{\frac {5\,\nu_{{9}}}{24}}, \\
\\
\nu_{{6}}&=-3/4\,\nu_{{7}}, \\
\\
\nu_{{8}}&=16\,\nu_{{83}}+8\,\nu_{{90}}+8\,\nu_{{59}}+8\,\nu_{{48}}+8\,\nu
_{{55}}+8\,\nu_{{58}}-1/2\,\nu_{{7}}, \\
\\
\nu_{{10}}&=-2\,\nu_{{7}}+4\,\nu_{
{9}}+2\,\nu_{{11}}, \\
\\
\nu_{{12}}&=1/8\,\nu_{{7}}+2\,\nu_{{59}}+2\,\nu_{{48
}}-2\,\nu_{{55}}+2\,\nu_{{58}}, \\
\\
\nu_{{13}}&={\frac {224\,\nu_{{108}}}{61
}}+{\frac {3\,\nu_{{7}}}{32}}-{\frac {16\,\nu_{{124}}}{61}}-{\frac {48
\,\nu_{{131}}}{61}}-{\frac {48\,\nu_{{132}}}{61}}+{\frac {80\,\nu_{{
127}}}{61}}-3\,\nu_{{48}}-\nu_{{55}}-3\,\nu_{{59}}\\
 &\hphantom{=} -{\frac {436\,\nu_{{
83}}}{61}}+{\frac {70\,\nu_{{90}}}{61}}+{\frac {224\,\nu_{{109}}}{61}}
-{\frac {1264\,\nu_{{137}}}{61}}-3\,\nu_{{58}}-3/8\,\nu_{{9}}, \\
\\
\nu_{{14
}}&={\frac {152\,\nu_{{108}}}{61}}-{\frac {5\,\nu_{{7}}}{48}}+{\frac {
24\,\nu_{{124}}}{61}}-{\frac {28\,\nu_{{131}}}{183}}-{\frac {28\,\nu_{
{132}}}{183}}+{\frac {128\,\nu_{{127}}}{183}}-1/3\,\nu_{{48}}+1/3\,\nu
_{{55}}\\
 &\hphantom{=}-1/3\,\nu_{{59}}+1/8\,\nu_{{11}}-{\frac {356\,\nu_{{83}}}{183}}
+{\frac {356\,\nu_{{90}}}{183}}+{\frac {152\,\nu_{{109}}}{61}}-{\frac 
{2120\,\nu_{{137}}}{183}}-1/3\,\nu_{{58}}+1/6\,\nu_{{9}}, \\
\end{aligned}
\end{align}
\begin{align}\label{AreaSol3}
\begin{aligned}
\nu_{{15}}&=-{
\frac {48\,\nu_{{124}}}{61}}+{\frac {672\,\nu_{{109}}}{61}}-{\frac {
3792\,\nu_{{137}}}{61}}+{\frac {240\,\nu_{{127}}}{61}}+{\frac {672\,
\nu_{{108}}}{61}}-{\frac {144\,\nu_{{132}}}{61}}\\
 &\hphantom{=}-{\frac {144\,\nu_{{
131}}}{61}}-{\frac {576\,\nu_{{83}}}{61}}+{\frac {576\,\nu_{{90}}}{61}
}, \\
\\
\nu_{{16}}&=-{\frac {16\,\nu_{{124}}}{61}}+{\frac {224\,\nu_{{109}}}{
61}}-{\frac {1264\,\nu_{{137}}}{61}}+{\frac {80\,\nu_{{127}}}{61}}+{
\frac {224\,\nu_{{108}}}{61}}-{\frac {48\,\nu_{{132}}}{61}}\\
 &\hphantom{=}-{\frac {48
\,\nu_{{131}}}{61}}-{\frac {192\,\nu_{{83}}}{61}}+{\frac {192\,\nu_{{
90}}}{61}}, \\
\\
\nu_{{17}}&={\frac {8\,\nu_{{124}}}{61}}-{\frac {112\,\nu_{{
109}}}{61}}+{\frac {632\,\nu_{{137}}}{61}}-{\frac {40\,\nu_{{127}}}{61
}}-{\frac {112\,\nu_{{108}}}{61}}+{\frac {24\,\nu_{{132}}}{61}}+{
\frac {24\,\nu_{{131}}}{61}}+{\frac {96\,\nu_{{83}}}{61}}-{\frac {96\,
\nu_{{90}}}{61}}, \\
\\
\nu_{{18}}&=-{\frac {232\,\nu_{{108}}}{61}}-{\frac {88
\,\nu_{{124}}}{61}}-{\frac {20\,\nu_{{131}}}{61}}-{\frac {20\,\nu_{{
132}}}{61}}-{\frac {48\,\nu_{{127}}}{61}}+{\frac {164\,\nu_{{83}}}{61}
}-{\frac {164\,\nu_{{90}}}{61}}\\
 &\hphantom{=}-{\frac {232\,\nu_{{109}}}{61}}+{\frac 
{856\,\nu_{{137}}}{61}}, \\
\\
\nu_{{19}}&=-1/2\,\nu_{{20}}-4\,\nu_{{69}}-4/3
\,\nu_{{80}}-4\,\nu_{{83}}+4/3\,\nu_{{51}}-4\,\nu_{{90}}+5/6\,\nu_{{59
}}-{\frac {5\,\nu_{{9}}}{16}}-5/6\,\nu_{{48}}\\
 &\hphantom{=}+1/6\,\nu_{{55}}+5/6\,\nu
_{{58}}-{\frac {17\,\nu_{{7}}}{192}}, \\
\\
\nu_{{21}}&=4\,\nu_{{80}}+32\,\nu_
{{83}}-4\,\nu_{{51}}+16\,\nu_{{90}}+8\,\nu_{{59}}+5/4\,\nu_{{9}}+4\,
\nu_{{48}}+8\,\nu_{{58}}+3/8\,\nu_{{7}}, \\
\\
\nu_{{22}}&=-32\,\nu_{{83}}-16
\,\nu_{{90}}-8\,\nu_{{59}}-\nu_{{9}}-8\,\nu_{{58}}-1/2\,\nu_{{7}}, \\
\\
\nu_
{{23}}&=32\,\nu_{{83}}+16\,\nu_{{90}}+8\,\nu_{{59}}+8\,\nu_{{58}}+3/4\,
\nu_{{7}}, \\
\\
\nu_{{24}}&=4\,\nu_{{83}}+2\,\nu_{{90}}+3\,\nu_{{59}}-1/8\,
\nu_{{9}}+3\,\nu_{{48}}+\nu_{{55}}+3\,\nu_{{58}}+1/32\,\nu_{{7}},, \\
\\
\nu_{{25}}&=-4\,\nu_{{83}}+2\,\nu_{{90}}-4\,\nu_{{59}}-1/4\,\nu_{{9}}-4\,\nu
_{{58}}+1/8\,\nu_{{7}}+8\,\nu_{{69}}, \\
\\
\nu_{{26}}&=32\,\nu_{{83}}+16\,\nu_{{90}}+8\,\nu_{{59}}+2\,\nu_{{9}}+8\,\nu_{{58}}+1/4\,\nu_{{7}}, \\
\\
\nu_{{
27}}&=-16\,\nu_{{69}}-16\,\nu_{{83}}-16\,\nu_{{90}}+6\,\nu_{{59}}-\nu_{
{9}}+6\,\nu_{{58}}-{\frac {9\,\nu_{{7}}}{16}}-4\,\nu_{{80}}+4\,\nu_{{
51}}+2\,\nu_{{48}}+2\,\nu_{{55}}, \\
\\
\nu_{{28}}&=-{\frac {29\,\nu_{{108}}}{61}}-2\,\nu_{{69}}-{\frac {11\,\nu_{{7}}}{768}}-{\frac {127\,\nu_{{124}}}{366}}+{\frac {28\,\nu_{{131}}}{61}}+{\frac {51\,\nu_{{132}}}{244}}-{\frac {11\,\nu_{{127}}}{732}}+\nu_{{79}}+1/2\,\nu_{{51}}+{\frac {19\,\nu_{{48}}}{24}}\\
 &\hphantom{=}+{\frac {13\,\nu_{{55}}}{24}}+{\frac {43\,\nu_{{59}}}{24}}-1/6\,\nu_{{80}}+{\frac {917\,\nu_{{83}}}{732}}-{\frac {92\,\nu_
{{90}}}{183}}+{\frac {3\,\nu_{{109}}}{122}}+{\frac {153\,\nu_{{137}}}{
122}}-1/4\,\nu_{{20}}+{\frac {43\,\nu_{{58}}}{24}}-{\frac {7\,\nu_{{9}
}}{64}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol4}
\begin{aligned}
\nu_{{29}}&=1/2\,\nu_{{20}}+{\frac {16\,\nu_{{124}}}{61}}+{
\frac {304\,\nu_{{109}}}{183}}-{\frac {444\,\nu_{{137}}}{61}}+{\frac {
4\,\nu_{{127}}}{183}}+{\frac {304\,\nu_{{108}}}{183}}-{\frac {100\,\nu
_{{132}}}{183}}-{\frac {100\,\nu_{{131}}}{183}}\\
 &\hphantom{=}-{\frac {1132\,\nu_{{83
}}}{183}}-{\frac {332\,\nu_{{90}}}{183}}-4/3\,\nu_{{59}}-4/3\,\nu_{{58
}}-1/8\,\nu_{{7}}, \\
\\
\nu_{{30}}&={\frac {174\,\nu_{{108}}}{61}}+{\frac {5
\,\nu_{{7}}}{128}}+{\frac {127\,\nu_{{124}}}{61}}-{\frac {168\,\nu_{{
131}}}{61}}-{\frac {153\,\nu_{{132}}}{122}}+{\frac {11\,\nu_{{127}}}{
122}}-6\,\nu_{{79}}-\nu_{{51}}-9/4\,\nu_{{48}}\\
 &\hphantom{=}-7/4\,\nu_{{55}}-5/4\,
\nu_{{59}}-\nu_{{80}}+{\frac {59\,\nu_{{83}}}{122}}+{\frac {62\,\nu_{{
90}}}{61}}-{\frac {9\,\nu_{{109}}}{61}}-{\frac {459\,\nu_{{137}}}{61}}
-5/4\,\nu_{{58}}+{\frac {7\,\nu_{{9}}}{32}}, \\
\\
\nu_{{31}}&=-{\frac {48\,
\nu_{{124}}}{61}}-{\frac {304\,\nu_{{109}}}{61}}+{\frac {1332\,\nu_{{
137}}}{61}}-{\frac {4\,\nu_{{127}}}{61}}-{\frac {304\,\nu_{{108}}}{61}
}+{\frac {100\,\nu_{{132}}}{61}}+{\frac {100\,\nu_{{131}}}{61}}\\
 &\hphantom{=}+{
\frac {156\,\nu_{{83}}}{61}}-{\frac {156\,\nu_{{90}}}{61}}, \\
\\
\nu_{{32}}&=
3/16\,\nu_{{7}}-3/4\,\nu_{{9}}-8\,\nu_{{83}}-4\,\nu_{{90}}-6\,\nu_{{59
}}-6\,\nu_{{48}}-2\,\nu_{{55}}-6\,\nu_{{58}}, \\
\\
\nu_{{33}}&=-{\frac {48\,
\nu_{{124}}}{61}}-{\frac {304\,\nu_{{109}}}{61}}+{\frac {1332\,\nu_{{
137}}}{61}}-{\frac {4\,\nu_{{127}}}{61}}-{\frac {304\,\nu_{{108}}}{61}
}+{\frac {100\,\nu_{{132}}}{61}}+{\frac {100\,\nu_{{131}}}{61}}-{
\frac {88\,\nu_{{83}}}{61}}\\
 &\hphantom{=}-{\frac {278\,\nu_{{90}}}{61}}+{\frac {3\,
\nu_{{7}}}{32}}-3/8\,\nu_{{9}}-3\,\nu_{{59}}-3\,\nu_{{48}}-\nu_{{55}}-
3\,\nu_{{58}}, \\
\\
\nu_{{34}}&=-{\frac {11\,\nu_{{108}}}{183}}-2\,\nu_{{69}}
-{\frac {65\,\nu_{{7}}}{768}}-{\frac {103\,\nu_{{124}}}{366}}+{\frac {
59\,\nu_{{131}}}{183}}+{\frac {53\,\nu_{{132}}}{732}}-{\frac {7\,\nu_{
{127}}}{732}}+\nu_{{79}}+1/2\,\nu_{{51}}\\
 &\hphantom{=}+{\frac {25\,\nu_{{48}}}{24}}+
5/8\,\nu_{{55}}+{\frac {11\,\nu_{{59}}}{8}}-1/6\,\nu_{{80}}-{\frac {
947\,\nu_{{83}}}{732}}-{\frac {533\,\nu_{{90}}}{366}}+{\frac {161\,\nu
_{{109}}}{366}}-{\frac {69\,\nu_{{137}}}{122}}\\
 &\hphantom{=}+{\frac {11\,\nu_{{58}}
}{8}}-{\frac {5\,\nu_{{9}}}{64}}, \\
\\
\nu_{{35}}&=-1/2\,\nu_{{69}}+{\frac {
31\,\nu_{{7}}}{1536}}+1/6\,\nu_{{51}}+{\frac {67\,\nu_{{48}}}{48}}+1/
48\,\nu_{{55}}+{\frac {65\,\nu_{{59}}}{48}}-1/6\,\nu_{{80}}-1/4\,\nu_{
{90}}-3/16\,\nu_{{20}}\\
 &\hphantom{=}-1/2\,\nu_{{41}}+\nu_{{42}}+{\frac {65\,\nu_{{58
}}}{48}}-{\frac {21\,\nu_{{9}}}{128}}, \\
\\
\nu_{{36}}&=-{\frac {3\,\nu_{{20}
}}{64}}-{\frac {305\,\nu_{{7}}}{55296}}-{\frac {2011\,\nu_{{9}}}{13824
}}+{\frac {7\,\nu_{{11}}}{2304}}+1/4\,\nu_{{42}}+{\frac {1231\,\nu_{{
48}}}{1728}}+{\frac {31\,\nu_{{51}}}{144}}+{\frac {593\,\nu_{{55}}}{
1728}}+{\frac {841\,\nu_{{58}}}{1728}}\\
 &\hphantom{=}+{\frac {841\,\nu_{{59}}}{1728}}
-1/4\,\nu_{{69}}+{\frac {5\,\nu_{{77}}}{24}}+1/6\,\nu_{{79}}-{\frac {
23\,\nu_{{80}}}{144}}+1/32\,\nu_{{81}}-{\frac {50597\,\nu_{{83}}}{
52704}}-{\frac {61859\,\nu_{{90}}}{105408}}\\
 &\hphantom{=}-{\frac {143\,\nu_{{108}}}{
4392}}-{\frac {103\,\nu_{{109}}}{8784}}+{\frac {\nu_{{119}}}{72}}-{
\frac {53\,\nu_{{124}}}{1098}}-{\frac {1249\,\nu_{{127}}}{52704}}+{
\frac {2101\,\nu_{{131}}}{26352}}+{\frac {359\,\nu_{{132}}}{52704}}+1/
8\,\nu_{{135}}\\
 &\hphantom{=}-{\frac {295\,\nu_{{137}}}{6588}}+1/12\,\nu_{{139}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol5}
\begin{aligned}
\nu_
{{37}}&=-3/4\,\nu_{{48}}-1/2\,\nu_{{59}}+1/4\,\nu_{{83}}+1/8\,\nu_{{90}
}+3/16\,\nu_{{20}}+3/4\,\nu_{{41}}-3/2\,\nu_{{42}}-1/2\,\nu_{{58}}+{
\frac {3\,\nu_{{9}}}{32}}, \\
\\
\nu_{{38}}&=1/16\,\nu_{{7}}+\nu_{{48}}+\nu_{{
59}}+\nu_{{83}}+1/2\,\nu_{{90}}-1/4\,\nu_{{20}}-\nu_{{41}}+2\,\nu_{{42
}}+\nu_{{58}}-1/8\,\nu_{{9}}, \\
\\
\nu_{{39}}&={\frac {5\,\nu_{{20}}}{32}}-{
\frac {193\,\nu_{{7}}}{1536}}+{\frac {293\,\nu_{{9}}}{384}}+{\frac {7
\,\nu_{{11}}}{64}}-1/4\,\nu_{{41}}-\nu_{{42}}-{\frac {43\,\nu_{{48}}}{
48}}-{\frac {7\,\nu_{{51}}}{12}}-{\frac {7\,\nu_{{55}}}{16}}-3/16\,\nu
_{{58}}\\
 &\hphantom{=}-3/16\,\nu_{{59}}+\nu_{{69}}-1/2\,\nu_{{77}}+{\frac {7\,\nu_{{
80}}}{12}}-1/8\,\nu_{{81}}+{\frac {6679\,\nu_{{83}}}{1464}}+{\frac {
8419\,\nu_{{90}}}{2928}}+{\frac {53\,\nu_{{108}}}{122}}+{\frac {45\,
\nu_{{109}}}{244}}\\
 &\hphantom{=}-1/2\,\nu_{{119}}+{\frac {79\,\nu_{{124}}}{122}}+{
\frac {323\,\nu_{{127}}}{1464}}-{\frac {347\,\nu_{{131}}}{732}}-{
\frac {877\,\nu_{{132}}}{1464}}-1/2\,\nu_{{135}}-{\frac {65\,\nu_{{137
}}}{366}}-\nu_{{139}}, \\
\\
\nu_{{40}}&=1/32\,\nu_{{7}}+5/4\,\nu_{{48}}+\nu_{
{59}}+1/4\,\nu_{{83}}+1/8\,\nu_{{90}}-1/8\,\nu_{{20}}-1/4\,\nu_{{41}}+
1/2\,\nu_{{42}}+\nu_{{58}}-{\frac {3\,\nu_{{9}}}{32}}, \\
\\
\nu_{{43}}&=-{
\frac {53\,\nu_{{108}}}{244}}+{\frac {227\,\nu_{{7}}}{3072}}-{\frac {
79\,\nu_{{124}}}{244}}+1/4\,\nu_{{135}}+{\frac {347\,\nu_{{131}}}{1464
}}+{\frac {877\,\nu_{{132}}}{2928}}-{\frac {323\,\nu_{{127}}}{2928}}-{
\frac {19\,\nu_{{48}}}{96}}-{\frac {5\,\nu_{{55}}}{96}}\\
 &\hphantom{=}-{\frac {43\,
\nu_{{59}}}{96}}+1/16\,\nu_{{81}}-{\frac {7\,\nu_{{11}}}{128}}-{\frac 
{2653\,\nu_{{83}}}{2928}}-{\frac {2929\,\nu_{{90}}}{5856}}-{\frac {45
\,\nu_{{109}}}{488}}+{\frac {65\,\nu_{{137}}}{732}}+1/4\,\nu_{{41}}+1/
4\,\nu_{{42}}\\
 &\hphantom{=}-{\frac {43\,\nu_{{58}}}{96}}-{\frac {167\,\nu_{{9}}}{768
}}+1/4\,\nu_{{119}}+1/2\,\nu_{{139}},\\
\\
\nu_{{44}}&=1/2\,\nu_{{80}}+4\,\nu
_{{83}}-1/2\,\nu_{{51}}+2\,\nu_{{90}}-1/2\,\nu_{{59}}+{\frac {9\,\nu_{
{9}}}{32}}-1/2\,\nu_{{48}}-1/2\,\nu_{{58}}+{\frac {3\,\nu_{{7}}}{64}},\\
\\
\nu_{{45}}&=-{\frac {4\,\nu_{{108}}}{183}}+{\frac {299\,\nu_{{7}}}{2304
}}-{\frac {25\,\nu_{{124}}}{732}}-1/2\,\nu_{{135}}-{\frac {265\,\nu_{{
131}}}{1098}}+{\frac {142\,\nu_{{132}}}{549}}+{\frac {35\,\nu_{{127}}
}{1098}}-\nu_{{79}}-5/6\,\nu_{{51}}\\
 &\hphantom{=}-{\frac {169\,\nu_{{48}}}{72}}-{
\frac {107\,\nu_{{55}}}{72}}-{\frac {85\,\nu_{{59}}}{72}}+1/2\,\nu_{{
80}}-{\frac {7\,\nu_{{11}}}{96}}-5/6\,\nu_{{77}}+{\frac {5101\,\nu_{{
83}}}{1098}}+{\frac {2585\,\nu_{{90}}}{1098}}-{\frac {4\,\nu_{{109}}}{
183}}\\
 &\hphantom{=}+{\frac {785\,\nu_{{137}}}{2196}}-{\frac {85\,\nu_{{58}}}{72}}+{
\frac {175\,\nu_{{9}}}{576}}+1/6\,\nu_{{119}}, \\
\\
\nu_{{46}}&=-4\,\nu_{{83}
}-2\,\nu_{{90}}-\nu_{{59}}-\nu_{{48}}-\nu_{{58}}-1/8\,\nu_{{7}},\\
\\
\nu_{{
47}}&=-2\,\nu_{{83}}-\nu_{{90}}-1/8\,\nu_{{9}}+1/2\,\nu_{{48}}, \\
\\
\nu_{{49
}}&=\nu_{{77}}-\nu_{{80}}-9\,\nu_{{83}}+\nu_{{51}}-9/2\,\nu_{{90}}-1/2
\,\nu_{{59}}-5/8\,\nu_{{9}}+\nu_{{48}}+\nu_{{55}}-1/2\,\nu_{{58}}-1/16
\,\nu_{{7}},  \\
\\
\nu_{{50}}&=4\,\nu_{{83}}+2\,\nu_{{90}}+1/4\,\nu_{{9}}-\nu_
{{48}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol6}
\begin{aligned}
\nu_{{52}}&=-\nu_{{79}}+\nu_{{83}}+1/2\,\nu_{{90}}-1/2\,\nu_{{59
}}+1/8\,\nu_{{9}}-\nu_{{48}}-1/2\,\nu_{{55}}-1/2\,\nu_{{58}}, \\
\\
\nu_{{53}
}&=-1/8\,\nu_{{11}}-1/2\,\nu_{{59}}-1/2\,\nu_{{9}}-1/2\,\nu_{{48}}+1/2
\,\nu_{{55}}-1/2\,\nu_{{58}}+{\frac {7\,\nu_{{7}}}{32}}, \\
\\
\nu_{{54}}&=-1/
16\,\nu_{{11}}-4\,\nu_{{83}}-2\,\nu_{{90}}-3/4\,\nu_{{59}}-3/8\,\nu_{{
9}}+3/4\,\nu_{{48}}-1/4\,\nu_{{55}}-3/4\,\nu_{{58}}+{\frac {3\,\nu_{{7
}}}{64}}, \\
\\
\nu_{{56}}&=2\,\nu_{{83}}-\nu_{{51}}+\nu_{{90}}-1/2\,\nu_{{55}
}+{\frac {7\,\nu_{{7}}}{64}}-1/2\,\nu_{{59}}-1/2\,\nu_{{48}}-1/2\,\nu_
{{58}}, \\
\\
\nu_{{57}}&=2\,\nu_{{51}}+2\,\nu_{{59}}+\nu_{{48}}+\nu_{{55}}+
\nu_{{58}}-1/16\,\nu_{{7}}, \\
\\
\nu_{{60}}&=-\nu_{{77}}+\nu_{{80}}+8\,\nu_{{
83}}-\nu_{{51}}+4\,\nu_{{90}}-1/2\,\nu_{{59}}+5/8\,\nu_{{9}}-2\,\nu_{{
48}}-\nu_{{55}}-1/2\,\nu_{{58}}+1/16\,\nu_{{7}}, \\
\\
\nu_{{61}}&=-\nu_{{80}}
-4\,\nu_{{83}}-2\,\nu_{{90}}-1/2\,\nu_{{59}}-{\frac {5\,\nu_{{9}}}{16}
}+1/2\,\nu_{{48}}+1/2\,\nu_{{55}}-1/2\,\nu_{{58}}-1/32\,\nu_{{7}}, \\
\\
\nu_
{{62}}&=\nu_{{59}}+1/4\,\nu_{{9}}+\nu_{{48}}-\nu_{{55}}+\nu_{{58}}-1/16
\,\nu_{{7}}, \\
\\
\nu_{{63}}&=4\,\nu_{{83}}+2\,\nu_{{90}}+5/4\,\nu_{{59}}+3/8
\,\nu_{{9}}-1/4\,\nu_{{48}}-1/4\,\nu_{{55}}+5/4\,\nu_{{58}}-{\frac {
\nu_{{7}}}{64}}, \\
\\
\nu_{{64}}&=2\,\nu_{{83}}+\nu_{{90}}+1/2\,\nu_{{59}}-1/
8\,\nu_{{9}}+1/2\,\nu_{{48}}+1/2\,\nu_{{55}}+1/2\,\nu_{{58}}+{\frac {3
\,\nu_{{7}}}{32}}, \\
\\
\nu_{{65}}&=-1/2\,\nu_{{59}}-1/2\,\nu_{{48}}+1/2\,\nu
_{{55}}-1/2\,\nu_{{58}}-1/32\,\nu_{{7}}, \\
\\
\nu_{{66}}&=\nu_{{79}}-1/2\,\nu
_{{83}}-1/4\,\nu_{{90}}-5/8\,\nu_{{59}}-{\frac {\nu_{{9}}}{64}}-5/8\,
\nu_{{48}}+1/8\,\nu_{{55}}-5/8\,\nu_{{58}}+{\frac {\nu_{{7}}}{256}},\\
\\
\nu_{{67}}&=\nu_{{80}}+2\,\nu_{{83}}+\nu_{{90}}+2\,\nu_{{59}}+1/8\,\nu_
{{9}}+2\,\nu_{{48}}+2\,\nu_{{58}}, \\
\\
\nu_{{68}}&={\frac {11\,\nu_{{7}}}{
1536}}-{\frac {5\,\nu_{{48}}}{24}}-1/24\,\nu_{{55}}-{\frac {5\,\nu_{{
59}}}{24}}-{\frac {\nu_{{11}}}{128}}-1/4\,\nu_{{83}}-1/8\,\nu_{{90}}-{
\frac {5\,\nu_{{58}}}{24}}-{\frac {5\,\nu_{{9}}}{384}}, \\
\\
\nu_{{70}}&=1/3
\,\nu_{{135}}-{\frac {113\,\nu_{{124}}}{366}}-{\frac {65\,\nu_{{109}}
}{366}}+{\frac {169\,\nu_{{137}}}{244}}-{\frac {15\,\nu_{{127}}}{122}}
-{\frac {21\,\nu_{{108}}}{61}}+{\frac {9\,\nu_{{132}}}{122}}+{\frac {
44\,\nu_{{131}}}{183}}\\
 &\hphantom{=}+1/2\,\nu_{{79}}-{\frac {7\,\nu_{{77}}}{36}}-{
\frac {\nu_{{11}}}{96}}+1/18\,\nu_{{80}}+{\frac {259\,\nu_{{83}}}{4392
}}+1/9\,\nu_{{51}}-{\frac {3629\,\nu_{{90}}}{8784}}+{\frac {73\,\nu_{{
59}}}{288}}-{\frac {91\,\nu_{{9}}}{2304}}\\
 &\hphantom{=}+{\frac {97\,\nu_{{48}}}{288}
}+{\frac {59\,\nu_{{55}}}{288}}+{\frac {73\,\nu_{{58}}}{288}}+{\frac {
11\,\nu_{{7}}}{9216}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol7}
\begin{aligned}
\nu_{{71}}&=\nu_{{83}}+\nu_{{51}}+1/2\,\nu_{{90}}
+\nu_{{55}}-1/32\,\nu_{{7}}, \\
\\
\nu_{{72}}&=-\nu_{{79}}+2\,\nu_{{83}}+\nu_{
{90}}+1/8\,\nu_{{59}}+{\frac {9\,\nu_{{9}}}{64}}-3/8\,\nu_{{48}}-1/8\,
\nu_{{55}}+1/8\,\nu_{{58}}-{\frac {\nu_{{7}}}{256}},\\
\\
\nu_{{73}}&=2\,\nu_
{{83}}-\nu_{{51}}+\nu_{{90}}-\nu_{{55}}+1/8\,\nu_{{7}}, \\
\\
\nu_{{74}}&=2\,
\nu_{{51}}+\nu_{{59}}+2\,\nu_{{55}}-1/8\,\nu_{{7}}, \\
\\
\nu_{{75}}&=\nu_{{58
}}, \\
\\
\nu_{{76}}&=\nu_{{83}}+1/2\,\nu_{{90}}+\nu_{{59}}, \\
\\
\nu_{{78}}&=-\nu_{{
80}}-5\,\nu_{{83}}-5/2\,\nu_{{90}}-\nu_{{59}}-3/8\,\nu_{{9}}-\nu_{{58}
}, \\
\\
\nu_{{82}}&=1/2\,\nu_{{81}}+5/2\,\nu_{{83}}+2\,\nu_{{90}}+\nu_{{59}}+
1/4\,\nu_{{9}}+\nu_{{58}}, \\
\\
\nu_{{84}}&=1/2\,\nu_{{81}}-5/2\,\nu_{{83}}-1
/4\,\nu_{{90}}-1/2\,\nu_{{59}}-1/8\,\nu_{{9}}-1/2\,\nu_{{58}}, \\
\\
\nu_{{85
}}&={\frac {227\,\nu_{{108}}}{366}}+{\frac {179\,\nu_{{7}}}{4608}}+{
\frac {229\,\nu_{{124}}}{732}}-{\frac {13\,\nu_{{131}}}{2196}}+{\frac 
{523\,\nu_{{132}}}{4392}}+{\frac {511\,\nu_{{127}}}{4392}}-1/2\,\nu_{{
79}}-1/4\,\nu_{{51}}\\
 &\hphantom{=}-{\frac {103\,\nu_{{48}}}{144}}-{\frac {65\,\nu_{{
55}}}{144}}-{\frac {55\,\nu_{{59}}}{144}}+1/12\,\nu_{{80}}-1/48\,\nu_{
{11}}-1/3\,\nu_{{77}}+{\frac {3251\,\nu_{{83}}}{4392}}+{\frac {1943\,
\nu_{{90}}}{2196}}\\
 &\hphantom{=}+{\frac {271\,\nu_{{109}}}{732}}-{\frac {2479\,\nu_{
{137}}}{1098}}-{\frac {55\,\nu_{{58}}}{144}}+{\frac {73\,\nu_{{9}}}{
1152}}+1/3\,\nu_{{119}}+\nu_{{139}}, \\
\\
\nu_{{86}}&=-\nu_{{81}}+4\,\nu_{{83
}}+1/2\,\nu_{{90}}+\nu_{{59}}+1/2\,\nu_{{9}}+\nu_{{58}}-1/8\,\nu_{{7}},\\
\\
\nu_{{87}}&=-2\,\nu_{{69}}-{\frac {5\,\nu_{{7}}}{128}}+1/2\,\nu_{{51}}
+1/4\,\nu_{{48}}+1/4\,\nu_{{55}}+1/4\,\nu_{{59}}-1/2\,\nu_{{80}}-4\,
\nu_{{83}}-2\,\nu_{{90}}\\
 &\hphantom{=}+1/4\,\nu_{{58}}-{\frac {5\,\nu_{{9}}}{16}},\\
\\
\nu_{{88}}&=-1/6\,\nu_{{119}}-1/6\,\nu_{{135}}+{\frac {159\,\nu_{{124}}
}{244}}+{\frac {23\,\nu_{{109}}}{61}}-{\frac {3827\,\nu_{{137}}}{2196}
}+{\frac {235\,\nu_{{127}}}{1098}}+{\frac {130\,\nu_{{108}}}{183}}-{
\frac {223\,\nu_{{132}}}{549}}\\
 &\hphantom{=}-{\frac {263\,\nu_{{131}}}{1098}}+2/9\,
\nu_{{77}}+1/16\,\nu_{{11}}-1/9\,\nu_{{80}}-{\frac {925\,\nu_{{83}}}{
732}}+1/9\,\nu_{{51}}+{\frac {325\,\nu_{{90}}}{1464}}+{\frac {13\,\nu_
{{59}}}{144}}+{\frac {41\,\nu_{{9}}}{1152}}\\
 &\hphantom{=}+{\frac {49\,\nu_{{48}}}{
144}}+{\frac {59\,\nu_{{55}}}{144}}+{\frac {13\,\nu_{{58}}}{144}}-{
\frac {37\,\nu_{{7}}}{512}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol8}
\begin{aligned}
\nu_{{89}}&=-8\,\nu_{{83}}-\nu_{{90}}-2\,
\nu_{{59}}-1/2\,\nu_{{9}}-2\,\nu_{{58}}, \\
\\
\nu_{{91}}&=-1/2\,\nu_{{90}}+1/
8\,\nu_{{9}}-1/16\,\nu_{{7}}, \\
\\
\nu_{{92}}&=-{\frac {227\,\nu_{{108}}}{183
}}-{\frac {107\,\nu_{{7}}}{2304}}-{\frac {229\,\nu_{{124}}}{366}}+{
\frac {13\,\nu_{{131}}}{1098}}-{\frac {523\,\nu_{{132}}}{2196}}-{
\frac {511\,\nu_{{127}}}{2196}}+\nu_{{79}}+{\frac {31\,\nu_{{48}}}{72}
}\\
 &\hphantom{=}+{\frac {29\,\nu_{{55}}}{72}}+{\frac {37\,\nu_{{59}}}{72}}+1/3\,\nu_{
{80}}+1/24\,\nu_{{11}}-1/3\,\nu_{{77}}+{\frac {5533\,\nu_{{83}}}{2196}
}+{\frac {253\,\nu_{{90}}}{1098}}-{\frac {271\,\nu_{{109}}}{366}}\\
 &\hphantom{=}+{
\frac {2479\,\nu_{{137}}}{549}}+{\frac {37\,\nu_{{58}}}{72}}+{\frac {
107\,\nu_{{9}}}{576}}-2/3\,\nu_{{119}}-2\,\nu_{{139}}, \\
\\
\nu_{{93}}&=-{
\frac {583\,\nu_{{108}}}{1464}}-1/4\,\nu_{{69}}+{\frac {85\,\nu_{{7}}
}{6144}}-{\frac {91\,\nu_{{124}}}{2928}}+{\frac {199\,\nu_{{131}}}{
1464}}+{\frac {613\,\nu_{{132}}}{5856}}-{\frac {205\,\nu_{{127}}}{1952
}}+1/8\,\nu_{{79}}\\
 &\hphantom{=}+1/16\,\nu_{{51}}+{\frac {67\,\nu_{{48}}}{192}}+{
\frac {13\,\nu_{{55}}}{192}}+{\frac {25\,\nu_{{59}}}{64}}-1/48\,\nu_{{
80}}+{\frac {3001\,\nu_{{83}}}{5856}}-{\frac {8\,\nu_{{90}}}{183}}-{
\frac {983\,\nu_{{109}}}{2928}}\\
 &\hphantom{=}+{\frac {979\,\nu_{{137}}}{976}}-{
\frac {3\,\nu_{{20}}}{32}}-1/4\,\nu_{{41}}+1/2\,\nu_{{42}}+{\frac {25
\,\nu_{{58}}}{64}}-{\frac {23\,\nu_{{9}}}{512}}, \\
\\
\nu_{{94}}&=-{\frac {3
\,\nu_{{20}}}{128}}+{\frac {1301\,\nu_{{7}}}{110592}}-{\frac {1001\,
\nu_{{9}}}{27648}}-{\frac {31\,\nu_{{11}}}{4608}}+1/8\,\nu_{{42}}+{
\frac {125\,\nu_{{48}}}{3456}}+{\frac {5\,\nu_{{51}}}{288}}+{\frac {67
\,\nu_{{55}}}{3456}}+{\frac {191\,\nu_{{58}}}{3456}}\\
 &\hphantom{=}+{\frac {191\,\nu_
{{59}}}{3456}}-1/8\,\nu_{{69}}-1/48\,\nu_{{77}}+1/24\,\nu_{{79}}-{
\frac {\nu_{{80}}}{288}}+{\frac {\nu_{{81}}}{192}}+{\frac {7289\,\nu_{
{83}}}{105408}}-{\frac {16225\,\nu_{{90}}}{210816}}\\
 &\hphantom{=}-{\frac {1045\,\nu_
{{108}}}{8784}}-{\frac {1541\,\nu_{{109}}}{17568}}+{\frac {5\,\nu_{{
119}}}{144}}-{\frac {409\,\nu_{{124}}}{8784}}-{\frac {3215\,\nu_{{127}
}}{105408}}+{\frac {3191\,\nu_{{131}}}{52704}}+{\frac {5833\,\nu_{{132
}}}{105408}}\\
 &\hphantom{=}+1/48\,\nu_{{135}}+{\frac {823\,\nu_{{137}}}{3294}}+1/24\,
\nu_{{139}}, \\
\\
\nu_{{95}}&={\frac {35\,\nu_{{108}}}{61}}-{\frac {3\,\nu_{{
7}}}{128}}-{\frac {5\,\nu_{{124}}}{122}}-{\frac {15\,\nu_{{131}}}{122}
}-{\frac {15\,\nu_{{132}}}{122}}+{\frac {25\,\nu_{{127}}}{122}}-3/8\,
\nu_{{48}}-1/4\,\nu_{{59}}-{\frac {301\,\nu_{{83}}}{488}}\\
 &\hphantom{=}+{\frac {53\,
\nu_{{90}}}{976}}+{\frac {35\,\nu_{{109}}}{61}}-{\frac {106\,\nu_{{137
}}}{61}}+{\frac {3\,\nu_{{20}}}{32}}+3/8\,\nu_{{41}}-3/4\,\nu_{{42}}-1
/4\,\nu_{{58}}+{\frac {3\,\nu_{{9}}}{64}},  \\
\\
\nu_{{96}}&=-{\frac {88\,\nu_
{{108}}}{183}}+1/32\,\nu_{{7}}+{\frac {5\,\nu_{{124}}}{61}}+{\frac {29
\,\nu_{{131}}}{366}}+{\frac {29\,\nu_{{132}}}{366}}+{\frac {5\,\nu_{{
127}}}{732}}+1/2\,\nu_{{48}}+1/3\,\nu_{{59}}+{\frac {119\,\nu_{{83}}}{
183}}\\
 &\hphantom{=}+{\frac {73\,\nu_{{90}}}{732}}-{\frac {88\,\nu_{{109}}}{183}}+{
\frac {119\,\nu_{{137}}}{122}}-1/8\,\nu_{{20}}-1/2\,\nu_{{41}}+\nu_{{
42}}+1/3\,\nu_{{58}}-1/16\,\nu_{{9}}, \\
\\
\nu_{{97}}&={\frac {5\,\nu_{{20}}
}{64}}-{\frac {839\,\nu_{{7}}}{9216}}+{\frac {587\,\nu_{{9}}}{2304}}+{
\frac {25\,\nu_{{11}}}{384}}-1/8\,\nu_{{41}}-1/2\,\nu_{{42}}+{\frac {
43\,\nu_{{48}}}{288}}-1/24\,\nu_{{51}}+{\frac {29\,\nu_{{55}}}{288}}\\
 &\hphantom{=}+{
\frac {37\,\nu_{{58}}}{288}}+{\frac {37\,\nu_{{59}}}{288}}+1/2\,\nu_{{
69}}+1/12\,\nu_{{77}}+1/24\,\nu_{{80}}-1/16\,\nu_{{81}}+{\frac {3409\,
\nu_{{83}}}{8784}}+{\frac {10201\,\nu_{{90}}}{17568}}\\
 &\hphantom{=}+{\frac {105\,\nu
_{{108}}}{244}}+{\frac {149\,\nu_{{109}}}{488}}-{\frac {5\,\nu_{{119}}
}{12}}+{\frac {21\,\nu_{{124}}}{61}}+{\frac {1289\,\nu_{{127}}}{8784}}
-{\frac {1625\,\nu_{{131}}}{4392}}-{\frac {3799\,\nu_{{132}}}{8784}}-1
/4\,\nu_{{135}}\\
 &\hphantom{=}-{\frac {577\,\nu_{{137}}}{1098}}-1/2\,\nu_{{139}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol9}
\begin{aligned}
\nu_
{{98}}&=-{\frac {59\,\nu_{{108}}}{183}}+{\frac {\nu_{{7}}}{64}}-{\frac 
{3\,\nu_{{124}}}{122}}+{\frac {17\,\nu_{{131}}}{183}}+{\frac {17\,\nu_
{{132}}}{183}}-{\frac {8\,\nu_{{127}}}{183}}+1/8\,\nu_{{48}}+1/6\,\nu_
{{59}}+{\frac {283\,\nu_{{83}}}{488}}\\
 &\hphantom{=}+{\frac {105\,\nu_{{90}}}{976}}-{
\frac {59\,\nu_{{109}}}{183}}+{\frac {129\,\nu_{{137}}}{122}}-1/16\,
\nu_{{20}}-1/8\,\nu_{{41}}+1/4\,\nu_{{42}}+1/6\,\nu_{{58}}-{\frac {\nu
_{{9}}}{64}}, \\
\\
\nu_{{99}}&={\frac {74\,\nu_{{108}}}{183}}+{\frac {23\,\nu
_{{7}}}{768}}+{\frac {11\,\nu_{{124}}}{122}}+1/3\,\nu_{{135}}+{\frac {
19\,\nu_{{131}}}{183}}-{\frac {23\,\nu_{{132}}}{366}}+{\frac {79\,\nu_
{{127}}}{366}}-3/8\,\nu_{{48}}+{\frac {5\,\nu_{{55}}}{24}}\\
 &\hphantom{=}-{\frac {13
\,\nu_{{59}}}{24}}+1/6\,\nu_{{81}}-{\frac {56\,\nu_{{83}}}{61}}-{
\frac {5\,\nu_{{90}}}{61}}+{\frac {13\,\nu_{{109}}}{183}}-{\frac {443
\,\nu_{{137}}}{366}}+1/2\,\nu_{{41}}-{\frac {13\,\nu_{{58}}}{24}}-{
\frac {9\,\nu_{{9}}}{64}}\\
 &\hphantom{=}+1/3\,\nu_{{119}}, \\
\\
\nu_{{100}}&={\frac {10\,\nu
_{{108}}}{183}}+{\frac {23\,\nu_{{7}}}{1536}}+{\frac {31\,\nu_{{124}}
}{244}}+1/6\,\nu_{{135}}+{\frac {35\,\nu_{{131}}}{732}}-{\frac {13\,
\nu_{{132}}}{366}}+{\frac {107\,\nu_{{127}}}{1464}}+1/16\,\nu_{{48}}+{
\frac {5\,\nu_{{55}}}{48}}\\
 &\hphantom{=}-{\frac {13\,\nu_{{59}}}{48}}+1/12\,\nu_{{81
}}-{\frac {148\,\nu_{{83}}}{183}}-{\frac {463\,\nu_{{90}}}{1464}}-{
\frac {41\,\nu_{{109}}}{366}}-{\frac {139\,\nu_{{137}}}{366}}+1/2\,\nu
_{{42}}-{\frac {13\,\nu_{{58}}}{48}}-{\frac {13\,\nu_{{9}}}{128}}\\
 &\hphantom{=}+1/6
\,\nu_{{119}}, \\
\\
\nu_{{101}}&={\frac {209\,\nu_{{7}}}{4608}}-{\frac {61\,
\nu_{{9}}}{576}}-{\frac {25\,\nu_{{11}}}{768}}+1/8\,\nu_{{41}}+1/8\,
\nu_{{42}}-{\frac {17\,\nu_{{48}}}{72}}-1/24\,\nu_{{51}}-{\frac {17\,
\nu_{{55}}}{144}}-{\frac {89\,\nu_{{58}}}{288}}-{\frac {89\,\nu_{{59}}
}{288}}\\
 &\hphantom{=}-1/24\,\nu_{{77}}-1/8\,\nu_{{79}}+1/32\,\nu_{{81}}-{\frac {385
\,\nu_{{83}}}{1098}}-{\frac {6895\,\nu_{{90}}}{35136}}-{\frac {53\,\nu
_{{108}}}{732}}-{\frac {53\,\nu_{{109}}}{732}}+{\frac {5\,\nu_{{119}}
}{24}}\\
 &\hphantom{=}-{\frac {377\,\nu_{{124}}}{2928}}-{\frac {445\,\nu_{{127}}}{8784
}}+{\frac {469\,\nu_{{131}}}{4392}}+{\frac {1487\,\nu_{{132}}}{8784}}+
1/8\,\nu_{{135}}-{\frac {179\,\nu_{{137}}}{2196}}+1/4\,\nu_{{139}}, \\
\\
\nu
_{{102}}&={\frac {155\,\nu_{{124}}}{488}}+{\frac {331\,\nu_{{109}}}{488
}}-{\frac {809\,\nu_{{137}}}{488}}+{\frac {219\,\nu_{{127}}}{976}}+{
\frac {257\,\nu_{{108}}}{244}}-{\frac {473\,\nu_{{132}}}{976}}-{\frac 
{41\,\nu_{{131}}}{61}}-3/4\,\nu_{{79}}\\
 &\hphantom{=}-1/8\,\nu_{{80}}-{\frac {\nu_{{
83}}}{976}}-1/8\,\nu_{{51}}+{\frac {23\,\nu_{{90}}}{122}}-{\frac {5\,
\nu_{{59}}}{32}}+{\frac {7\,\nu_{{9}}}{256}}-{\frac {9\,\nu_{{48}}}{32
}}-{\frac {7\,\nu_{{55}}}{32}}-{\frac {5\,\nu_{{58}}}{32}}+{\frac {5\,
\nu_{{7}}}{1024}}, \\
\\
\nu_{{103}}&={\frac {89\,\nu_{{108}}}{183}}-{\frac {
67\,\nu_{{7}}}{9216}}+{\frac {53\,\nu_{{124}}}{732}}-{\frac {140\,\nu_
{{131}}}{549}}-{\frac {1691\,\nu_{{132}}}{8784}}+{\frac {907\,\nu_{{
127}}}{8784}}-1/4\,\nu_{{79}}+{\frac {11\,\nu_{{48}}}{288}}+{\frac {
\nu_{{55}}}{288}}\\
 &\hphantom{=}+{\frac {5\,\nu_{{59}}}{288}}-1/12\,\nu_{{80}}+1/16\,
\nu_{{81}}+{\frac {\nu_{{11}}}{192}}+1/12\,\nu_{{77}}-{\frac {3409\,
\nu_{{83}}}{8784}}+{\frac {779\,\nu_{{90}}}{17568}}+{\frac {529\,\nu_{
{109}}}{1464}}\\
 &\hphantom{=}-{\frac {4463\,\nu_{{137}}}{4392}}+{\frac {5\,\nu_{{58}}
}{288}}-{\frac {5\,\nu_{{9}}}{2304}}-1/12\,\nu_{{119}}, \\
\\
\nu_{{104}}&={
\frac {175\,\nu_{{124}}}{244}}-{\frac {437\,\nu_{{109}}}{244}}+{\frac 
{649\,\nu_{{137}}}{244}}-{\frac {225\,\nu_{{127}}}{488}}-{\frac {127\,
\nu_{{108}}}{122}}+{\frac {135\,\nu_{{132}}}{488}}-{\frac {6\,\nu_{{
131}}}{61}}-3/2\,\nu_{{79}}\\
 &\hphantom{=}-1/4\,\nu_{{80}}+{\frac {235\,\nu_{{83}}}{
488}}-1/4\,\nu_{{51}}-{\frac {13\,\nu_{{90}}}{122}}-{\frac {5\,\nu_{{
59}}}{16}}+{\frac {7\,\nu_{{9}}}{128}}-{\frac {9\,\nu_{{48}}}{16}}-{
\frac {7\,\nu_{{55}}}{16}}-{\frac {5\,\nu_{{58}}}{16}}+{\frac {5\,\nu_
{{7}}}{512}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol10}
\begin{aligned}
\nu_{{105}}&={\frac {227\,\nu_{{124}}}{1464}}-{\frac {1165
\,\nu_{{109}}}{1464}}+{\frac {3049\,\nu_{{137}}}{1464}}-{\frac {167\,
\nu_{{127}}}{976}}-{\frac {491\,\nu_{{108}}}{732}}+{\frac {149\,\nu_{{
132}}}{976}}+{\frac {11\,\nu_{{131}}}{122}}-1/4\,\nu_{{79}}\\
 &\hphantom{=}-1/24\,\nu_
{{80}}+{\frac {751\,\nu_{{83}}}{2928}}-1/12\,\nu_{{51}}-{\frac {71\,
\nu_{{90}}}{366}}-{\frac {3\,\nu_{{59}}}{32}}+{\frac {7\,\nu_{{9}}}{
768}}-{\frac {3\,\nu_{{48}}}{32}}-{\frac {11\,\nu_{{55}}}{96}}-1/32\,
\nu_{{58}}\\
 &\hphantom{=}+{\frac {13\,\nu_{{7}}}{3072}},\\
\\
\nu_{{106}}&=-{\frac {75\,\nu_
{{124}}}{244}}+{\frac {13\,\nu_{{109}}}{244}}+{\frac {297\,\nu_{{137}}
}{244}}-{\frac {43\,\nu_{{127}}}{488}}-{\frac {85\,\nu_{{108}}}{122}}-
{\frac {23\,\nu_{{132}}}{488}}+{\frac {20\,\nu_{{131}}}{61}}+3/2\,\nu_
{{79}}+1/4\,\nu_{{80}}\\
 &\hphantom{=}-{\frac {31\,\nu_{{83}}}{488}}+1/4\,\nu_{{51}}-{
\frac {19\,\nu_{{90}}}{61}}+{\frac {5\,\nu_{{59}}}{16}}-{\frac {7\,\nu
_{{9}}}{128}}+{\frac {9\,\nu_{{48}}}{16}}+{\frac {7\,\nu_{{55}}}{16}}+
{\frac {5\,\nu_{{58}}}{16}}-{\frac {5\,\nu_{{7}}}{512}}, \\
\\
\nu_{{107}}&={
\frac {67\,\nu_{{124}}}{122}}-{\frac {145\,\nu_{{109}}}{122}}+{\frac {
176\,\nu_{{137}}}{61}}-{\frac {121\,\nu_{{127}}}{244}}-{\frac {42\,\nu
_{{108}}}{61}}+{\frac {97\,\nu_{{132}}}{244}}-{\frac {43\,\nu_{{131}}
}{122}}-3\,\nu_{{79}}-1/2\,\nu_{{80}}\\
 &\hphantom{=}+{\frac {205\,\nu_{{83}}}{244}}-1
/2\,\nu_{{51}}-{\frac {11\,\nu_{{90}}}{122}}-5/8\,\nu_{{59}}+{\frac {7
\,\nu_{{9}}}{64}}-{\frac {9\,\nu_{{48}}}{8}}-{\frac {7\,\nu_{{55}}}{8}
}-5/8\,\nu_{{58}}+{\frac {5\,\nu_{{7}}}{256}}, \\
\\
\nu_{{110}}&=1/2\,\nu_{{
119}}+{\frac {107\,\nu_{{124}}}{244}}-{\frac {39\,\nu_{{109}}}{61}}+{
\frac {593\,\nu_{{137}}}{732}}-{\frac {20\,\nu_{{127}}}{183}}-{\frac {
39\,\nu_{{108}}}{61}}+{\frac {73\,\nu_{{132}}}{183}}+{\frac {73\,\nu_{
{131}}}{183}}\\
 &\hphantom{=}-1/32\,\nu_{{11}}-{\frac {209\,\nu_{{83}}}{366}}-{\frac {
131\,\nu_{{90}}}{732}}-{\frac {7\,\nu_{{59}}}{24}}-{\frac {17\,\nu_{{9
}}}{192}}-{\frac {7\,\nu_{{48}}}{24}}-{\frac {5\,\nu_{{55}}}{24}}-{
\frac {7\,\nu_{{58}}}{24}}+{\frac {29\,\nu_{{7}}}{768}}, \\
\\
\nu_{{111}}&={
\frac {21\,\nu_{{124}}}{61}}+{\frac {72\,\nu_{{109}}}{61}}-{\frac {232
\,\nu_{{137}}}{61}}+{\frac {17\,\nu_{{127}}}{61}}+{\frac {72\,\nu_{{
108}}}{61}}-{\frac {57\,\nu_{{132}}}{122}}-{\frac {57\,\nu_{{131}}}{
122}}-{\frac {45\,\nu_{{83}}}{122}}+{\frac {45\,\nu_{{90}}}{122}}, \\
\\
\nu_
{{112}}&={\frac {75\,\nu_{{127}}}{976}}-{\frac {53\,\nu_{{124}}}{1464}}
+{\frac {193\,\nu_{{109}}}{1464}}-{\frac {283\,\nu_{{137}}}{1464}}+{
\frac {5\,\nu_{{108}}}{732}}-{\frac {45\,\nu_{{132}}}{976}}+{\frac {
\nu_{{131}}}{61}}+1/4\,\nu_{{79}}+1/24\,\nu_{{80}}\\
 &\hphantom{=}-{\frac {235\,\nu_{{
83}}}{2928}}+1/12\,\nu_{{51}}+{\frac {13\,\nu_{{90}}}{732}}+{\frac {3
\,\nu_{{59}}}{32}}-{\frac {7\,\nu_{{9}}}{768}}+{\frac {3\,\nu_{{48}}}{
32}}+{\frac {11\,\nu_{{55}}}{96}}+1/32\,\nu_{{58}}-{\frac {13\,\nu_{{7
}}}{3072}}, \\
\\
\nu_{{113}}&=-{\frac {17\,\nu_{{124}}}{61}}-{\frac {6\,\nu_{
{109}}}{61}}+{\frac {60\,\nu_{{137}}}{61}}-{\frac {13\,\nu_{{127}}}{
122}}-{\frac {6\,\nu_{{108}}}{61}}+{\frac {10\,\nu_{{132}}}{61}}+{
\frac {10\,\nu_{{131}}}{61}}+{\frac {19\,\nu_{{83}}}{122}}-{\frac {19
\,\nu_{{90}}}{122}},\\
\\
\nu_{{114}}&=-\nu_{{119}}+{\frac {6\,\nu_{{124}}}{
61}}+{\frac {15\,\nu_{{109}}}{122}}+{\frac {47\,\nu_{{137}}}{61}}-{
\frac {59\,\nu_{{127}}}{244}}-{\frac {23\,\nu_{{108}}}{61}}-{\frac {
111\,\nu_{{132}}}{244}}-{\frac {43\,\nu_{{131}}}{61}}+{\frac {105\,\nu
_{{83}}}{244}}\\
 &\hphantom{=}+{\frac {39\,\nu_{{90}}}{122}}+3/8\,\nu_{{59}}+{\frac {3
\,\nu_{{9}}}{64}}+3/8\,\nu_{{48}}+1/8\,\nu_{{55}}+3/8\,\nu_{{58}}-{
\frac {3\,\nu_{{7}}}{256}}, \\
\\
\nu_{{115}}&=-1/2\,\nu_{{119}}-{\frac {59\,
\nu_{{124}}}{244}}+{\frac {33\,\nu_{{109}}}{244}}+{\frac {341\,\nu_{{
137}}}{244}}+{\frac {41\,\nu_{{127}}}{488}}-{\frac {7\,\nu_{{108}}}{61
}}-{\frac {49\,\nu_{{132}}}{488}}-{\frac {55\,\nu_{{131}}}{244}}\\
 &\hphantom{=}+{
\frac {231\,\nu_{{83}}}{488}}-{\frac {6\,\nu_{{90}}}{61}}+3/16\,\nu_{{
59}}+{\frac {3\,\nu_{{9}}}{128}}+3/16\,\nu_{{48}}+1/16\,\nu_{{55}}+3/
16\,\nu_{{58}}-{\frac {3\,\nu_{{7}}}{512}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol11}
\begin{aligned}
\nu_{{116}}&={\frac {43\,\nu
_{{124}}}{122}}-{\frac {57\,\nu_{{109}}}{61}}+{\frac {347\,\nu_{{137}}
}{122}}-{\frac {16\,\nu_{{127}}}{61}}-{\frac {57\,\nu_{{108}}}{61}}+{
\frac {7\,\nu_{{132}}}{122}}+{\frac {7\,\nu_{{131}}}{122}}+{\frac {14
\,\nu_{{83}}}{61}}-{\frac {14\,\nu_{{90}}}{61}}, \\
\\
\nu_{{117}}&=-1/2\,\nu_
{{119}}-{\frac {13\,\nu_{{124}}}{244}}+{\frac {91\,\nu_{{109}}}{122}}-
{\frac {783\,\nu_{{137}}}{244}}+{\frac {65\,\nu_{{127}}}{244}}+{\frac 
{91\,\nu_{{108}}}{122}}-{\frac {39\,\nu_{{132}}}{244}}-{\frac {39\,\nu
_{{131}}}{244}}\\
 &\hphantom{=}-{\frac {95\,\nu_{{83}}}{244}}+{\frac {95\,\nu_{{90}}}{
244}}, \\
\\
\nu_{{118}}&=-1/2\,\nu_{{139}}-1/2\,\nu_{{119}}-{\frac {119\,\nu_
{{124}}}{488}}+{\frac {19\,\nu_{{109}}}{488}}+{\frac {437\,\nu_{{137}}
}{366}}-{\frac {151\,\nu_{{127}}}{2928}}-{\frac {41\,\nu_{{108}}}{122}
}-{\frac {373\,\nu_{{132}}}{2928}}\\
 &\hphantom{=}+{\frac {11\,\nu_{{131}}}{183}}+3/4
\,\nu_{{79}}+1/32\,\nu_{{11}}+1/8\,\nu_{{80}}+{\frac {1375\,\nu_{{83}}
}{2928}}+1/8\,\nu_{{51}}+{\frac {17\,\nu_{{90}}}{183}}+{\frac {43\,\nu
_{{59}}}{96}}+{\frac {47\,\nu_{{9}}}{768}}\\
 &\hphantom{=}+{\frac {55\,\nu_{{48}}}{96}
}+{\frac {41\,\nu_{{55}}}{96}}+{\frac {43\,\nu_{{58}}}{96}}-{\frac {
131\,\nu_{{7}}}{3072}}, \\
\\
\nu_{{120}}&=-{\frac {101\,\nu_{{124}}}{122}}+{
\frac {11\,\nu_{{109}}}{122}}+{\frac {195\,\nu_{{137}}}{122}}-{\frac {
149\,\nu_{{127}}}{244}}-{\frac {25\,\nu_{{108}}}{61}}-{\frac {301\,\nu
_{{132}}}{244}}+{\frac {\nu_{{131}}}{61}}+3\,\nu_{{79}}+1/2\,\nu_{{80}
}\\
 &\hphantom{=}-{\frac {45\,\nu_{{83}}}{244}}+1/2\,\nu_{{51}}-{\frac {69\,\nu_{{90}}
}{122}}+5/8\,\nu_{{59}}-{\frac {7\,\nu_{{9}}}{64}}+{\frac {9\,\nu_{{48
}}}{8}}+{\frac {7\,\nu_{{55}}}{8}}+5/8\,\nu_{{58}}-{\frac {5\,\nu_{{7}
}}{256}}, \\
\\
\nu_{{121}}&=-{\frac {257\,\nu_{{124}}}{732}}+{\frac {487\,\nu
_{{109}}}{732}}-{\frac {749\,\nu_{{137}}}{244}}-{\frac {53\,\nu_{{127}
}}{1464}}+{\frac {335\,\nu_{{108}}}{366}}-{\frac {749\,\nu_{{132}}}{
1464}}-{\frac {25\,\nu_{{131}}}{183}}+1/2\,\nu_{{79}}\\
 &\hphantom{=}+1/32\,\nu_{{11}}
+1/12\,\nu_{{80}}+{\frac {115\,\nu_{{83}}}{1464}}+1/6\,\nu_{{51}}+{
\frac {100\,\nu_{{90}}}{183}}+{\frac {23\,\nu_{{59}}}{48}}+{\frac {9\,
\nu_{{9}}}{128}}+{\frac {23\,\nu_{{48}}}{48}}+{\frac {7\,\nu_{{55}}}{
16}}\\
 &\hphantom{=}+{\frac {17\,\nu_{{58}}}{48}}-{\frac {71\,\nu_{{7}}}{1536}}, \\
\\
\nu_{{
122}}&=-\nu_{{135}}-{\frac {11\,\nu_{{124}}}{61}}-{\frac {29\,\nu_{{109
}}}{61}}+{\frac {229\,\nu_{{137}}}{61}}-{\frac {73\,\nu_{{127}}}{122}}
-{\frac {90\,\nu_{{108}}}{61}}+{\frac {117\,\nu_{{132}}}{122}}+{\frac 
{28\,\nu_{{131}}}{61}}-{\frac {81\,\nu_{{83}}}{122}}\\
 &\hphantom{=}-{\frac {51\,\nu_{
{90}}}{61}}-3/4\,\nu_{{59}}-{\frac {3\,\nu_{{9}}}{32}}-3/4\,\nu_{{48}}
-1/4\,\nu_{{55}}-3/4\,\nu_{{58}}+{\frac {3\,\nu_{{7}}}{128}}, \\
\\
\nu_{{123
}}&=-1/2\,\nu_{{135}}-{\frac {25\,\nu_{{124}}}{61}}-{\frac {93\,\nu_{{
109}}}{122}}+{\frac {282\,\nu_{{137}}}{61}}-{\frac {49\,\nu_{{127}}}{
244}}-{\frac {77\,\nu_{{108}}}{61}}+{\frac {127\,\nu_{{132}}}{244}}+{
\frac {33\,\nu_{{131}}}{122}}\\
 &\hphantom{=}-{\frac {41\,\nu_{{83}}}{244}}-{\frac {71
\,\nu_{{90}}}{122}}-3/8\,\nu_{{59}}-{\frac {3\,\nu_{{9}}}{64}}-3/8\,
\nu_{{48}}-1/8\,\nu_{{55}}-3/8\,\nu_{{58}}+{\frac {3\,\nu_{{7}}}{256}},\\
\\
\nu_{{125}}&=-{\frac {13\,\nu_{{124}}}{122}}-{\frac {123\,\nu_{{109}}
}{122}}+{\frac {188\,\nu_{{137}}}{61}}-{\frac {53\,\nu_{{127}}}{244}}-
{\frac {31\,\nu_{{108}}}{61}}-{\frac {17\,\nu_{{132}}}{244}}+{\frac {
11\,\nu_{{131}}}{61}}+{\frac {115\,\nu_{{83}}}{244}}-{\frac {115\,\nu_
{{90}}}{244}}, \\
\\
\nu_{{126}}&={\frac {13\,\nu_{{124}}}{244}}+{\frac {\nu_{
{109}}}{244}}-{\frac {457\,\nu_{{137}}}{732}}-{\frac {85\,\nu_{{127}}
}{1464}}+{\frac {31\,\nu_{{108}}}{122}}-{\frac {193\,\nu_{{132}}}{1464
}}-{\frac {5\,\nu_{{131}}}{732}}+{\frac {\nu_{{11}}}{64}}+{\frac {265
\,\nu_{{83}}}{1464}}+{\frac {71\,\nu_{{90}}}{366}}\\
 &\hphantom{=}+{\frac {7\,\nu_{{59
}}}{48}}+{\frac {17\,\nu_{{9}}}{384}}+{\frac {7\,\nu_{{48}}}{48}}+{
\frac {5\,\nu_{{55}}}{48}}+{\frac {7\,\nu_{{58}}}{48}}-{\frac {29\,\nu
_{{7}}}{1536}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol12}
\begin{aligned}
\nu_{{128}}&=-{\frac {38\,\nu_{{124}}}{61}}+{\frac {44\,
\nu_{{109}}}{61}}-{\frac {196\,\nu_{{137}}}{61}}+{\frac {7\,\nu_{{127}
}}{61}}+{\frac {44\,\nu_{{108}}}{61}}+{\frac {8\,\nu_{{132}}}{61}}+{
\frac {8\,\nu_{{131}}}{61}}-{\frac {29\,\nu_{{83}}}{61}}+{\frac {29\,
\nu_{{90}}}{61}}, \\
\\
\nu_{{129}}&={\frac {71\,\nu_{{124}}}{122}}+{\frac {43
\,\nu_{{109}}}{122}}-{\frac {613\,\nu_{{137}}}{122}}+{\frac {327\,\nu_
{{127}}}{244}}+{\frac {113\,\nu_{{108}}}{61}}-{\frac {\nu_{{132}}}{244
}}-{\frac {46\,\nu_{{131}}}{61}}-3\,\nu_{{79}}-1/2\,\nu_{{80}}\\
 &\hphantom{=}-{\frac 
{65\,\nu_{{83}}}{244}}-1/2\,\nu_{{51}}+{\frac {62\,\nu_{{90}}}{61}}-5/
8\,\nu_{{59}}+{\frac {7\,\nu_{{9}}}{64}}-{\frac {9\,\nu_{{48}}}{8}}-{
\frac {7\,\nu_{{55}}}{8}}-5/8\,\nu_{{58}}+{\frac {5\,\nu_{{7}}}{256}},\\
\\
\nu_{{130}}&=-{\frac {97\,\nu_{{124}}}{61}}+{\frac {77\,\nu_{{109}}}{61
}}+{\frac {84\,\nu_{{137}}}{61}}-{\frac {67\,\nu_{{127}}}{122}}-{
\frac {106\,\nu_{{108}}}{61}}-{\frac {33\,\nu_{{132}}}{122}}+{\frac {
14\,\nu_{{131}}}{61}}+6\,\nu_{{79}}+\nu_{{80}}\\
 &\hphantom{=}-{\frac {71\,\nu_{{83}}
}{122}}+\nu_{{51}}-{\frac {56\,\nu_{{90}}}{61}}+5/4\,\nu_{{59}}-{
\frac {7\,\nu_{{9}}}{32}}+9/4\,\nu_{{48}}+7/4\,\nu_{{55}}+5/4\,\nu_{{
58}}-{\frac {5\,\nu_{{7}}}{128}}, \\
\\
\nu_{{133}}&=-\nu_{{119}}+{\frac {93\,
\nu_{{124}}}{122}}+{\frac {20\,\nu_{{109}}}{61}}-{\frac {41\,\nu_{{137
}}}{366}}+{\frac {4\,\nu_{{127}}}{183}}+{\frac {20\,\nu_{{108}}}{61}}-
{\frac {161\,\nu_{{132}}}{183}}-{\frac {161\,\nu_{{131}}}{183}}\\
 &\hphantom{=}+1/16\,
\nu_{{11}}+{\frac {149\,\nu_{{83}}}{183}}+{\frac {251\,\nu_{{90}}}{366
}}+{\frac {7\,\nu_{{59}}}{12}}+{\frac {17\,\nu_{{9}}}{96}}+{\frac {7\,
\nu_{{48}}}{12}}+{\frac {5\,\nu_{{55}}}{12}}+{\frac {7\,\nu_{{58}}}{12
}}-{\frac {29\,\nu_{{7}}}{384}}, \\
\\
\nu_{{134}}&=-{\frac {26\,\nu_{{124}}}{
61}}-{\frac {2\,\nu_{{109}}}{61}}+{\frac {20\,\nu_{{137}}}{61}}+{
\frac {8\,\nu_{{127}}}{61}}-{\frac {2\,\nu_{{108}}}{61}}+{\frac {44\,
\nu_{{132}}}{61}}+{\frac {44\,\nu_{{131}}}{61}}-{\frac {7\,\nu_{{83}}
}{61}}+{\frac {7\,\nu_{{90}}}{61}}, \\
\\
\nu_{{136}}&=1/2\,\nu_{{135}}+{
\frac {7\,\nu_{{124}}}{61}}+{\frac {24\,\nu_{{109}}}{61}}-{\frac {179
\,\nu_{{137}}}{61}}-{\frac {9\,\nu_{{127}}}{122}}+{\frac {24\,\nu_{{
108}}}{61}}-{\frac {19\,\nu_{{132}}}{122}}-{\frac {19\,\nu_{{131}}}{
122}}\\
 &\hphantom{=}-{\frac {15\,\nu_{{83}}}{122}}+{\frac {15\,\nu_{{90}}}{122}}, \\
\\
\nu_
{{138}}&=-\nu_{{135}}-{\frac {47\,\nu_{{124}}}{61}}-{\frac {13\,\nu_{{
109}}}{61}}+{\frac {252\,\nu_{{137}}}{61}}-{\frac {79\,\nu_{{127}}}{
122}}-{\frac {74\,\nu_{{108}}}{61}}+{\frac {23\,\nu_{{132}}}{122}}-{
\frac {19\,\nu_{{131}}}{61}}\\
 &\hphantom{=}+{\frac {153\,\nu_{{83}}}{122}}+{\frac {15
\,\nu_{{90}}}{61}}+3/4\,\nu_{{59}}+{\frac {3\,\nu_{{9}}}{32}}+3/4\,\nu
_{{48}}+1/4\,\nu_{{55}}+3/4\,\nu_{{58}}-{\frac {3\,\nu_{{7}}}{128}},\\
\\
\nu_{{140}}&=\nu_{{135}}+{\frac {44\,\nu_{{124}}}{61}}+{\frac {55\,\nu_
{{109}}}{61}}-{\frac {428\,\nu_{{137}}}{61}}+{\frac {109\,\nu_{{127}}
}{122}}+{\frac {116\,\nu_{{108}}}{61}}-{\frac {41\,\nu_{{132}}}{122}}+
{\frac {10\,\nu_{{131}}}{61}}\\
 &\hphantom{=}-{\frac {225\,\nu_{{83}}}{122}}+{\frac {
21\,\nu_{{90}}}{61}}-3/4\,\nu_{{59}}-{\frac {3\,\nu_{{9}}}{32}}-3/4\,
\nu_{{48}}-1/4\,\nu_{{55}}-3/4\,\nu_{{58}}+{\frac {3\,\nu_{{7}}}{128}},\\
\\
\nu_{{141}}&=-\nu_{{135}}-{\frac {45\,\nu_{{124}}}{61}}-{\frac {41\,
\nu_{{109}}}{61}}+{\frac {349\,\nu_{{137}}}{61}}-{\frac {99\,\nu_{{127
}}}{122}}-{\frac {102\,\nu_{{108}}}{61}}+{\frac {35\,\nu_{{132}}}{122}
}-{\frac {13\,\nu_{{131}}}{61}}\\
 &\hphantom{=}+{\frac {201\,\nu_{{83}}}{122}}-{\frac 
{9\,\nu_{{90}}}{61}}+3/4\,\nu_{{59}}+{\frac {3\,\nu_{{9}}}{32}}+3/4\,
\nu_{{48}}+1/4\,\nu_{{55}}+3/4\,\nu_{{58}}-{\frac {3\,\nu_{{7}}}{128}},\\
\\
\nu_{{142}}&={\frac {22\,\nu_{{124}}}{61}}+{\frac {58\,\nu_{{109}}}{61
}}-{\frac {214\,\nu_{{137}}}{61}}+{\frac {12\,\nu_{{127}}}{61}}+{
\frac {58\,\nu_{{108}}}{61}}+{\frac {5\,\nu_{{132}}}{61}}+{\frac {5\,
\nu_{{131}}}{61}}-{\frac {41\,\nu_{{83}}}{61}}+{\frac {41\,\nu_{{90}}
}{61}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol13}
\begin{aligned}
\\
\nu_{{143}}&={\frac {53\,\nu_{{124}}}{732}}-{\frac {193\,\nu_{{
109}}}{732}}+{\frac {283\,\nu_{{137}}}{732}}+{\frac {169\,\nu_{{127}}
}{488}}-{\frac {5\,\nu_{{108}}}{366}}+{\frac {45\,\nu_{{132}}}{488}}-{
\frac {2\,\nu_{{131}}}{61}}-1/2\,\nu_{{79}}-1/12\,\nu_{{80}}\\
 &\hphantom{=}+{\frac {
235\,\nu_{{83}}}{1464}}-1/6\,\nu_{{51}}-{\frac {13\,\nu_{{90}}}{366}}-
3/16\,\nu_{{59}}+{\frac {7\,\nu_{{9}}}{384}}-3/16\,\nu_{{48}}-{\frac {
11\,\nu_{{55}}}{48}}-1/16\,\nu_{{58}}+{\frac {13\,\nu_{{7}}}{1536}},\\
\\
\nu_{{144}}&=-{\frac {233\,\nu_{{108}}}{1464}}-1/4\,\nu_{{69}}-{\frac {
83\,\nu_{{7}}}{6144}}-{\frac {385\,\nu_{{124}}}{2928}}+{\frac {185\,
\nu_{{131}}}{1464}}+{\frac {923\,\nu_{{132}}}{5856}}-{\frac {481\,\nu_
{{127}}}{5856}}+1/8\,\nu_{{79}}\\
 &\hphantom{=}+1/16\,\nu_{{51}}+{\frac {43\,\nu_{{48}
}}{192}}+{\frac {7\,\nu_{{55}}}{64}}+{\frac {17\,\nu_{{59}}}{64}}-1/48
\,\nu_{{80}}-{\frac {29\,\nu_{{83}}}{5856}}-{\frac {443\,\nu_{{90}}}{
2928}}+{\frac {83\,\nu_{{109}}}{2928}}+{\frac {435\,\nu_{{137}}}{976}}
\\
 &\hphantom{=}+{\frac {17\,\nu_{{58}}}{64}}+{\frac {\nu_{{9}}}{512}}, \\
\\
\nu_{{145}}&=-1/
48\,\nu_{{149}}+1/4\,\nu_{{151}}+1/4\,\nu_{{153}}+1/6\,\nu_{{155}}-{
\frac {13\,\nu_{{160}}}{48}}+2/9\,\nu_{{164}}+{\frac {35\,\nu_{{165}}
}{144}}+1/4\,\nu_{{175}}\\
 &\hphantom{=}+1/24\,\nu_{{178}}+1/16\,\nu_{{181}}+1/12\,\nu
_{{202}}-1/24\,\nu_{{204}}+1/24\,\nu_{{206}}-{\frac {\nu_{{20}}}{96}}+
{\frac {419\,\nu_{{7}}}{18432}}+{\frac {575\,\nu_{{9}}}{4608}}\\
 &\hphantom{=}-{\frac 
{49\,\nu_{{48}}}{1152}}-{\frac {29\,\nu_{{51}}}{192}}-{\frac {23\,\nu_
{{55}}}{384}}+{\frac {161\,\nu_{{58}}}{288}}+{\frac {161\,\nu_{{59}}}{
288}}-{\frac {13\,\nu_{{69}}}{48}}+1/24\,\nu_{{79}}+{\frac {95\,\nu_{{
80}}}{576}}+{\frac {1727\,\nu_{{83}}}{732}}\\
 &\hphantom{=}+{\frac {12323\,\nu_{{90}}
}{11712}}-{\frac {11\,\nu_{{108}}}{4392}}+{\frac {161\,\nu_{{109}}}{
8784}}-{\frac {103\,\nu_{{124}}}{8784}}-{\frac {7\,\nu_{{127}}}{17568}
}+{\frac {59\,\nu_{{131}}}{4392}}+{\frac {53\,\nu_{{132}}}{17568}}-{
\frac {23\,\nu_{{137}}}{976}}, \\
\\
\nu_{{146}}&=-1/2\,\nu_{{175}}-{\frac {11
\,\nu_{{164}}}{72}}-1/3\,\nu_{{155}}-1/6\,\nu_{{69}}-{\frac {31\,\nu_{
{7}}}{1152}}+{\frac {7\,\nu_{{51}}}{72}}+{\frac {7\,\nu_{{48}}}{144}}+
{\frac {7\,\nu_{{55}}}{144}}-{\frac {5\,\nu_{{59}}}{12}}-{\frac {7\,
\nu_{{80}}}{72}}\\
 &\hphantom{=}+1/3\,\nu_{{149}}-{\frac {83\,\nu_{{83}}}{36}}-{\frac 
{89\,\nu_{{90}}}{72}}-{\frac {\nu_{{20}}}{64}}-{\frac {5\,\nu_{{58}}}{
12}}-{\frac {7\,\nu_{{165}}}{36}}-{\frac {77\,\nu_{{9}}}{576}}, \\
\\
\nu_{{
147}}&=1/2\,\nu_{{69}}-{\frac {67\,\nu_{{7}}}{1536}}-2\,\nu_{{151}}-\nu
_{{153}}+{\frac {5\,\nu_{{51}}}{24}}+{\frac {5\,\nu_{{48}}}{48}}+{
\frac {5\,\nu_{{55}}}{48}}-{\frac {47\,\nu_{{59}}}{48}}-{\frac {5\,\nu
_{{80}}}{24}}+\nu_{{160}}-4\,\nu_{{83}}\\
 &\hphantom{=}-7/4\,\nu_{{90}}+1/16\,\nu_{{20
}}-{\frac {47\,\nu_{{58}}}{48}}-3/16\,\nu_{{9}}, \\
\\
\nu_{{148}}&=-{\frac {
\nu_{{7}}}{256}}+1/4\,\nu_{{51}}+1/8\,\nu_{{48}}+1/8\,\nu_{{55}}+5/8\,
\nu_{{59}}-1/4\,\nu_{{80}}-\nu_{{149}}+3/2\,\nu_{{83}}\\
 &\hphantom{=}+3/4\,\nu_{{90}}
+1/16\,\nu_{{20}}+5/8\,\nu_{{58}}+1/8\,\nu_{{9}}, \\
\\
\nu_{{150}}&=1/4\,\nu_
{{69}}-{\frac {5\,\nu_{{7}}}{1536}}+1/24\,\nu_{{51}}+{\frac {5\,\nu_{{
48}}}{24}}+1/12\,\nu_{{55}}+{\frac {11\,\nu_{{59}}}{48}}-1/24\,\nu_{{
80}}+5/8\,\nu_{{83}}\\
 &\hphantom{=}+{\frac {7\,\nu_{{90}}}{16}}+1/32\,\nu_{{20}}+{
\frac {11\,\nu_{{58}}}{48}}+1/16\,\nu_{{9}}, \\
\\
\nu_{{152}}&=\nu_{{149}}-
\nu_{{165}}-\nu_{{164}}-1/8\,\nu_{{20}}-5\,\nu_{{83}}-5/2\,\nu_{{90}}-
5/4\,\nu_{{59}}-3/8\,\nu_{{9}}-5/4\,\nu_{{58}}-{\frac {3\,\nu_{{7}}}{
128}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol14}
\begin{aligned}
\nu_{{154}}&=-1/2\,\nu_{{155}}+5/6\,\nu_{{160}}-{\frac {13\,\nu_{
{164}}}{16}}-3/4\,\nu_{{165}}-1/4\,\nu_{{181}}-1/2\,\nu_{{202}}+1/4\,
\nu_{{204}}-1/4\,\nu_{{206}}\\
 &\hphantom{=}-{\frac {221\,\nu_{{7}}}{6144}}-{\frac {
545\,\nu_{{9}}}{1536}}-{\frac {19\,\nu_{{48}}}{192}}+{\frac {11\,\nu_{
{51}}}{48}}+{\frac {\nu_{{55}}}{64}}-{\frac {343\,\nu_{{58}}}{192}}-{
\frac {343\,\nu_{{59}}}{192}}+\nu_{{69}}-1/4\,\nu_{{79}}\\
 &\hphantom{=}-{\frac {5\,
\nu_{{80}}}{16}}-{\frac {4445\,\nu_{{83}}}{732}}-{\frac {15131\,\nu_{{
90}}}{5856}}+{\frac {11\,\nu_{{108}}}{732}}-{\frac {161\,\nu_{{109}}}{
1464}}+{\frac {103\,\nu_{{124}}}{1464}}+{\frac {7\,\nu_{{127}}}{2928}}
-{\frac {59\,\nu_{{131}}}{732}}\\
 &\hphantom{=}-{\frac {53\,\nu_{{132}}}{2928}}+{
\frac {69\,\nu_{{137}}}{488}}, \\
\\
\nu_{{156}}&={\frac {\nu_{{7}}}{256}}+3/8
\,\nu_{{59}}-\nu_{{160}}+3/2\,\nu_{{83}}+3/4\,\nu_{{90}}+3/8\,\nu_{{58
}}+1/8\,\nu_{{9}}, \\
\\
\nu_{{157}}&=\nu_{{164}}+{\frac {11\,\nu_{{7}}}{128}}
-1/2\,\nu_{{51}}-1/4\,\nu_{{48}}-1/4\,\nu_{{55}}+5/4\,\nu_{{59}}+1/2\,
\nu_{{80}}+7\,\nu_{{83}}+7/2\,\nu_{{90}}\\
 &\hphantom{=}+5/4\,\nu_{{58}}+\nu_{{165}}+3
/8\,\nu_{{9}}, \\
\\
\nu_{{158}}&=-\nu_{{165}}-\nu_{{164}}-\nu_{{80}}-12\,\nu_
{{83}}+\nu_{{51}}-6\,\nu_{{90}}-2\,\nu_{{59}}-5/8\,\nu_{{9}}+1/2\,\nu_
{{48}}+1/2\,\nu_{{55}}-2\,\nu_{{58}}\\
 &\hphantom{=}-{\frac {5\,\nu_{{7}}}{32}}, \\
\\
\nu_{{
159}}&=-{\frac {11\,\nu_{{7}}}{512}}+1/4\,\nu_{{51}}-1/16\,\nu_{{48}}+1
/16\,\nu_{{55}}-{\frac {5\,\nu_{{59}}}{16}}-1/4\,\nu_{{80}}-7/4\,\nu_{
{83}}-{\frac {7\,\nu_{{90}}}{8}}-{\frac {5\,\nu_{{58}}}{16}}\\
 &\hphantom{=}-{\frac {
11\,\nu_{{9}}}{128}}, \\
\\
\nu_{{161}}&=-\nu_{{164}}-{\frac {11\,\nu_{{7}}}{
128}}+1/2\,\nu_{{51}}+1/4\,\nu_{{48}}+1/4\,\nu_{{55}}-3/2\,\nu_{{59}}-
1/2\,\nu_{{80}}-8\,\nu_{{83}}-4\,\nu_{{90}}-3/2\,\nu_{{58}}\\
 &\hphantom{=}-\nu_{{165}
}-{\frac {15\,\nu_{{9}}}{32}}, \\
\\
\nu_{{162}}&=\nu_{{165}}+\nu_{{164}}+1/2
\,\nu_{{80}}+12\,\nu_{{83}}-1/2\,\nu_{{51}}+6\,\nu_{{90}}+5/2\,\nu_{{
59}}+{\frac {11\,\nu_{{9}}}{16}}-1/4\,\nu_{{48}}-1/4\,\nu_{{55}}\\
 &\hphantom{=}+5/2\,
\nu_{{58}}+1/8\,\nu_{{7}}, \\
\\
\nu_{{163}}&=13/2\,\nu_{{83}}+{\frac {13\,\nu
_{{90}}}{4}}+{\frac {13\,\nu_{{59}}}{8}}+{\frac {13\,\nu_{{58}}}{8}}+{
\frac {15\,\nu_{{7}}}{256}}-\nu_{{160}}+3/8\,\nu_{{9}}, \\
\\
\nu_{{166}}&=-
\nu_{{165}}-\nu_{{164}}+4\,\nu_{{69}}+2\,\nu_{{83}}+3\,\nu_{{90}}-\nu_
{{59}}+1/8\,\nu_{{9}}-\nu_{{58}}+{\frac {3\,\nu_{{7}}}{32}}, \\
\\
\nu_{{167}
}&=-\nu_{{165}}, \\
\\
\nu_{{168}}&=2\,\nu_{{69}}+\nu_{{83}}+3/2\,\nu_{{90}}-1/
2\,\nu_{{59}}+1/16\,\nu_{{9}}-1/2\,\nu_{{58}}+{\frac {3\,\nu_{{7}}}{64
}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol15}
\begin{aligned}
\nu_{{169}}&=\nu_{{80}}+2\,\nu_{{83}}-\nu_{{51}}+\nu_{{90}}-1/2\,\nu
_{{59}}+1/8\,\nu_{{9}}-1/2\,\nu_{{48}}-1/2\,\nu_{{55}}-1/2\,\nu_{{58}}
+{\frac {3\,\nu_{{7}}}{64}}, \\
\\
\nu_{{170}}&={\frac {7\,\nu_{{7}}}{1024}}+{
\frac {9\,\nu_{{48}}}{32}}+{\frac {3\,\nu_{{55}}}{32}}+{\frac {21\,\nu
_{{59}}}{32}}+{\frac {15\,\nu_{{83}}}{8}}+{\frac {15\,\nu_{{90}}}{16}}
+{\frac {21\,\nu_{{58}}}{32}}+{\frac {29\,\nu_{{9}}}{256}}, \\
\\
\nu_{{171}}
&=\nu_{{165}}+\nu_{{164}}+\nu_{{80}}+2\,\nu_{{83}}-\nu_{{51}}+\nu_{{90}
}-1/2\,\nu_{{59}}+1/8\,\nu_{{9}}-1/2\,\nu_{{48}}-1/2\,\nu_{{55}}-1/2\,
\nu_{{58}}+{\frac {3\,\nu_{{7}}}{64}}, \\
\\
\nu_{{172}}&={\frac {\nu_{{7}}}{
64}}+1/4\,\nu_{{51}}-1/4\,\nu_{{48}}+1/4\,\nu_{{59}}-1/4\,\nu_{{80}}+
\nu_{{83}}+1/2\,\nu_{{90}}+1/4\,\nu_{{58}}+{\frac {3\,\nu_{{9}}}{64}},\\
\\
\nu_{{173}}&=1/2\,\nu_{{80}}-\nu_{{83}}-1/2\,\nu_{{51}}-1/2\,\nu_{{90}}
-3/4\,\nu_{{59}}-1/16\,\nu_{{9}}-1/4\,\nu_{{48}}-1/4\,\nu_{{55}}-3/4\,
\nu_{{58}}+{\frac {\nu_{{7}}}{128}}, \\
\\
\nu_{{174}}&=1/24\,\nu_{{149}}+1/6
\,\nu_{{151}}-1/6\,\nu_{{153}}-{\frac {11\,\nu_{{160}}}{72}}+{\frac {7
\,\nu_{{164}}}{72}}+{\frac {\nu_{{165}}}{72}}-1/2\,\nu_{{175}}-1/12\,
\nu_{{178}}+1/8\,\nu_{{181}}\\
 &\hphantom{=}+1/6\,\nu_{{202}}-1/12\,\nu_{{204}}+1/12\,
\nu_{{206}}-{\frac {\nu_{{20}}}{192}}-{\frac {3\,\nu_{{7}}}{2048}}+{
\frac {95\,\nu_{{9}}}{2304}}+{\frac {13\,\nu_{{48}}}{144}}+{\frac {\nu
_{{51}}}{144}}+1/24\,\nu_{{55}}+{\frac {215\,\nu_{{58}}}{576}}\\
 &\hphantom{=}+{\frac 
{215\,\nu_{{59}}}{576}}-1/3\,\nu_{{69}}+1/12\,\nu_{{79}}+1/48\,\nu_{{
80}}+{\frac {371\,\nu_{{83}}}{488}}+{\frac {1343\,\nu_{{90}}}{5856}}-{
\frac {11\,\nu_{{108}}}{2196}}+{\frac {161\,\nu_{{109}}}{4392}}-{
\frac {103\,\nu_{{124}}}{4392}}\\
 &\hphantom{=}-{\frac {7\,\nu_{{127}}}{8784}}+{\frac 
{59\,\nu_{{131}}}{2196}}+{\frac {53\,\nu_{{132}}}{8784}}-{\frac {23\,
\nu_{{137}}}{488}}, \\
\\
\nu_{{176}}&=-{\frac {\nu_{{7}}}{64}}-1/16\,\nu_{{51
}}-1/8\,\nu_{{48}}-1/16\,\nu_{{55}}-{\frac {11\,\nu_{{59}}}{16}}+1/16
\,\nu_{{80}}+1/2\,\nu_{{160}}-9/4\,\nu_{{83}}-{\frac {9\,\nu_{{90}}}{8
}}-1/2\,\nu_{{178}}\\
 &\hphantom{=}-1/2\,\nu_{{181}}-{\frac {11\,\nu_{{58}}}{16}}-{
\frac {33\,\nu_{{9}}}{256}}, \\
\\
\nu_{{177}}&=-\nu_{{165}}-3/2\,\nu_{{164}}+
2\,\nu_{{69}}-1/2\,\nu_{{80}}-3\,\nu_{{83}}+1/2\,\nu_{{51}}-1/2\,\nu_{
{90}}-\nu_{{59}}-3/16\,\nu_{{9}}+1/4\,\nu_{{48}}\\
 &\hphantom{=}+1/4\,\nu_{{55}}-\nu_{
{58}}, \\
\\
\nu_{{179}}&=-1/12\,\nu_{{149}}-1/3\,\nu_{{151}}+1/3\,\nu_{{153}}
+1/2\,\nu_{{155}}-1/36\,\nu_{{160}}+{\frac {89\,\nu_{{164}}}{144}}+{
\frac {13\,\nu_{{165}}}{18}}+\nu_{{175}}+1/6\,\nu_{{178}}\\
 &\hphantom{=}+1/6\,\nu_{{
202}}-1/12\,\nu_{{204}}+1/12\,\nu_{{206}}+1/24\,\nu_{{20}}+{\frac {5\,
\nu_{{7}}}{128}}+{\frac {623\,\nu_{{9}}}{2304}}-{\frac {13\,\nu_{{48}}
}{288}}-{\frac {19\,\nu_{{51}}}{72}}-{\frac {3\,\nu_{{55}}}{32}}\\
 &\hphantom{=}+{
\frac {131\,\nu_{{58}}}{144}}+{\frac {131\,\nu_{{59}}}{144}}-1/12\,\nu
_{{69}}+1/12\,\nu_{{79}}+{\frac {7\,\nu_{{80}}}{24}}+{\frac {12779\,
\nu_{{83}}}{2928}}+{\frac {3157\,\nu_{{90}}}{1464}}-{\frac {11\,\nu_{{
108}}}{2196}}+{\frac {161\,\nu_{{109}}}{4392}}\\
 &\hphantom{=}-{\frac {103\,\nu_{{124}
}}{4392}}-{\frac {7\,\nu_{{127}}}{8784}}+{\frac {59\,\nu_{{131}}}{2196
}}+{\frac {53\,\nu_{{132}}}{8784}}-{\frac {23\,\nu_{{137}}}{488}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol16}
\begin{aligned}
\nu_{{180}}&=-2\,\nu_{{175}}-\nu_{{164}}-\nu_{{155}}-{\frac {5\,\nu_{{7}}}{
64}}+1/2\,\nu_{{51}}+1/4\,\nu_{{48}}+1/4\,\nu_{{55}}-{\frac {11\,\nu_{
{59}}}{8}}-1/2\,\nu_{{80}}-15/2\,\nu_{{83}}\\
 &\hphantom{=}-{\frac {15\,\nu_{{90}}}{4}
}-1/16\,\nu_{{20}}-{\frac {11\,\nu_{{58}}}{8}}-\nu_{{165}}-{\frac {29
\,\nu_{{9}}}{64}},\\
\\
\nu_{{182}}&=\nu_{{164}}-4\,\nu_{{69}}-2\,\nu_{{83}}-
3\,\nu_{{90}}+\nu_{{59}}-1/8\,\nu_{{9}}+\nu_{{58}}-{\frac {3\,\nu_{{7}
}}{32}}, \\
\\
\nu_{{183}}&=-2\,\nu_{{83}}-\nu_{{90}}-1/2\,\nu_{{59}}-1/8\,\nu
_{{9}}-1/2\,\nu_{{58}}-{\frac {\nu_{{7}}}{64}}, \\
\\
\nu_{{184}}&=-1/48\,\nu_
{{149}}+1/12\,\nu_{{151}}+1/6\,\nu_{{153}}+1/12\,\nu_{{155}}+{\frac {5
\,\nu_{{160}}}{144}}+{\frac {13\,\nu_{{164}}}{192}}+{\frac {5\,\nu_{{
165}}}{48}}+1/4\,\nu_{{175}}\\
 &\hphantom{=}-1/48\,\nu_{{181}}+1/6\,\nu_{{194}}-1/24\,
\nu_{{202}}-1/48\,\nu_{{204}}+1/48\,\nu_{{206}}+{\frac {119\,\nu_{{7}}
}{18432}}+{\frac {283\,\nu_{{9}}}{9216}}-{\frac {\nu_{{48}}}{1152}}\\
 &\hphantom{=}-{
\frac {7\,\nu_{{51}}}{576}}-{\frac {\nu_{{55}}}{1152}}+{\frac {71\,\nu
_{{58}}}{576}}+{\frac {71\,\nu_{{59}}}{576}}+1/48\,\nu_{{69}}+1/32\,
\nu_{{79}}+{\frac {13\,\nu_{{80}}}{576}}+{\frac {13379\,\nu_{{83}}}{
23424}}+{\frac {6995\,\nu_{{90}}}{23424}}\\
 &\hphantom{=}-{\frac {109\,\nu_{{108}}}{
17568}}+{\frac {331\,\nu_{{109}}}{35136}}-{\frac {37\,\nu_{{124}}}{
3904}}-{\frac {25\,\nu_{{127}}}{70272}}+{\frac {101\,\nu_{{131}}}{8784
}}+{\frac {259\,\nu_{{132}}}{70272}}+{\frac {5\,\nu_{{137}}}{3904}},\\
\\
\nu_{{185}}&=1/6\,\nu_{{149}}-1/6\,\nu_{{155}}-{\frac {89\,\nu_{{164}}
}{288}}-{\frac {23\,\nu_{{165}}}{72}}-1/2\,\nu_{{175}}-1/3\,\nu_{{194}
}+1/24\,\nu_{{206}}-{\frac {\nu_{{20}}}{64}}-{\frac {77\,\nu_{{7}}}{
4096}}\\
 &\hphantom{=}-{\frac {1271\,\nu_{{9}}}{9216}}-{\frac {23\,\nu_{{48}}}{1152}}+
{\frac {\nu_{{51}}}{72}}-{\frac {13\,\nu_{{55}}}{1152}}-{\frac {217\,
\nu_{{58}}}{384}}-{\frac {217\,\nu_{{59}}}{384}}-1/12\,\nu_{{79}}-1/24
\,\nu_{{80}}-{\frac {13375\,\nu_{{83}}}{5856}}\\
 &\hphantom{=}-{\frac {13327\,\nu_{{90
}}}{11712}}+{\frac {17\,\nu_{{108}}}{549}}-{\frac {47\,\nu_{{109}}}{
4392}}+{\frac {121\,\nu_{{124}}}{4392}}+{\frac {5\,\nu_{{127}}}{4392}}
-{\frac {311\,\nu_{{131}}}{8784}}-{\frac {8\,\nu_{{132}}}{549}}-{
\frac {65\,\nu_{{137}}}{976}}, \\
\\
\nu_{{186}}&=-\nu_{{151}}-1/2\,\nu_{{153}
}-1/12\,\nu_{{160}}+1/4\,\nu_{{181}}+1/2\,\nu_{{202}}+1/4\,\nu_{{204}}
+1/32\,\nu_{{20}}+{\frac {25\,\nu_{{7}}}{6144}}+{\frac {41\,\nu_{{9}}
}{1536}}\\
 &\hphantom{=}-{\frac {5\,\nu_{{48}}}{64}}-{\frac {7\,\nu_{{51}}}{48}}-{
\frac {17\,\nu_{{55}}}{192}}-{\frac {41\,\nu_{{58}}}{192}}-{\frac {41
\,\nu_{{59}}}{192}}+1/4\,\nu_{{69}}-1/8\,\nu_{{79}}+{\frac {5\,\nu_{{
80}}}{48}}+{\frac {101\,\nu_{{83}}}{1952}}+{\frac {163\,\nu_{{90}}}{
976}}\\
 &\hphantom{=}+{\frac {29\,\nu_{{108}}}{488}}-{\frac {3\,\nu_{{109}}}{976}}+{
\frac {127\,\nu_{{124}}}{2928}}+{\frac {11\,\nu_{{127}}}{5856}}-{
\frac {7\,\nu_{{131}}}{122}}-{\frac {51\,\nu_{{132}}}{1952}}-{\frac {
153\,\nu_{{137}}}{976}}, \\
\\
\nu_{{187}}&={\frac {17\,\nu_{{164}}}{24}}-{
\frac {29\,\nu_{{108}}}{244}}-\nu_{{69}}-{\frac {\nu_{{7}}}{128}}-{
\frac {127\,\nu_{{124}}}{1464}}+{\frac {7\,\nu_{{131}}}{61}}+{\frac {
51\,\nu_{{132}}}{976}}-{\frac {11\,\nu_{{127}}}{2928}}+1/4\,\nu_{{79}}
+{\frac {5\,\nu_{{51}}}{24}}\\
 &\hphantom{=}+{\frac {5\,\nu_{{48}}}{24}}+1/6\,\nu_{{55
}}+4/3\,\nu_{{59}}-1/8\,\nu_{{80}}-1/2\,\nu_{{206}}-1/2\,\nu_{{149}}+{
\frac {8359\,\nu_{{83}}}{2928}}+{\frac {2621\,\nu_{{90}}}{2928}}+{
\frac {3\,\nu_{{109}}}{488}}\\
 &\hphantom{=}+{\frac {153\,\nu_{{137}}}{488}}+1/32\,\nu
_{{20}}+4/3\,\nu_{{58}}+{\frac {7\,\nu_{{165}}}{12}}+{\frac {35\,\nu_{
{9}}}{192}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol17}
\begin{aligned}
\nu_{{188}}&=-{\frac {17\,\nu_{{164}}}{24}}-{\frac {65\,\nu
_{{108}}}{732}}+\nu_{{69}}+{\frac {79\,\nu_{{124}}}{1464}}-{\frac {17
\,\nu_{{131}}}{366}}+{\frac {47\,\nu_{{132}}}{2928}}+{\frac {\nu_{{127
}}}{976}}-1/4\,\nu_{{79}}-{\frac {5\,\nu_{{51}}}{24}}-{\frac {5\,\nu_{
{48}}}{24}}\\
 &\hphantom{=}-1/6\,\nu_{{55}}-{\frac {17\,\nu_{{59}}}{12}}+1/8\,\nu_{{80
}}+1/2\,\nu_{{206}}+1/2\,\nu_{{149}}-{\frac {9023\,\nu_{{83}}}{2928}}-
{\frac {3421\,\nu_{{90}}}{2928}}-{\frac {313\,\nu_{{109}}}{1464}}\\
 &\hphantom{=}+{
\frac {291\,\nu_{{137}}}{488}}-{\frac {17\,\nu_{{58}}}{12}}-{\frac {7
\,\nu_{{165}}}{12}}-{\frac {35\,\nu_{{9}}}{192}}, \\
\\
\nu_{{189}}&=-{\frac {
123\,\nu_{{108}}}{976}}+1/8\,\nu_{{69}}+{\frac {11\,\nu_{{7}}}{12288}}
-{\frac {17\,\nu_{{124}}}{5856}}+{\frac {11\,\nu_{{131}}}{488}}+{
\frac {149\,\nu_{{132}}}{3904}}-{\frac {13\,\nu_{{127}}}{11712}}-1/16
\,\nu_{{79}}-1/32\,\nu_{{51}}\\
 &\hphantom{=}-{\frac {19\,\nu_{{48}}}{384}}-{\frac {13
\,\nu_{{55}}}{384}}-{\frac {43\,\nu_{{59}}}{384}}+{\frac {\nu_{{80}}}{
96}}+{\frac {19\,\nu_{{83}}}{11712}}-{\frac {71\,\nu_{{90}}}{1464}}-{
\frac {307\,\nu_{{109}}}{1952}}+{\frac {1179\,\nu_{{137}}}{1952}}+{
\frac {\nu_{{20}}}{64}}-{\frac {43\,\nu_{{58}}}{384}}\\
 &\hphantom{=}+{\frac {7\,\nu_{
{9}}}{1024}}, \\
\\
\nu_{{190}}&={\frac {5\,\nu_{{164}}}{576}}+1/16\,\nu_{{69}
}+{\frac {25\,\nu_{{7}}}{18432}}+{\frac {5\,\nu_{{151}}}{12}}+1/12\,
\nu_{{153}}+{\frac {11\,\nu_{{51}}}{288}}-{\frac {17\,\nu_{{48}}}{1152
}}+{\frac {\nu_{{55}}}{128}}+{\frac {5\,\nu_{{59}}}{576}}-{\frac {11\,
\nu_{{80}}}{288}}\\
 &\hphantom{=}-1/16\,\nu_{{206}}+{\frac {5\,\nu_{{160}}}{144}}-1/48
\,\nu_{{149}}+{\frac {19\,\nu_{{83}}}{288}}+{\frac {37\,\nu_{{90}}}{
576}}-1/24\,\nu_{{178}}-1/8\,\nu_{{181}}-{\frac {5\,\nu_{{202}}}{24}}-
1/16\,\nu_{{204}}\\
 &\hphantom{=}+{\frac {\nu_{{20}}}{384}}+{\frac {5\,\nu_{{58}}}{576
}}-{\frac {\nu_{{165}}}{144}}+{\frac {47\,\nu_{{9}}}{9216}}, \\
\\
\nu_{{191}
}&=-{\frac {17\,\nu_{{164}}}{24}}+{\frac {11\,\nu_{{108}}}{732}}+\nu_{{
69}}+{\frac {\nu_{{7}}}{64}}+{\frac {103\,\nu_{{124}}}{1464}}-{\frac {
59\,\nu_{{131}}}{732}}-{\frac {53\,\nu_{{132}}}{2928}}+{\frac {7\,\nu_
{{127}}}{2928}}-1/4\,\nu_{{79}}-{\frac {5\,\nu_{{51}}}{24}}\\
 &\hphantom{=}-{\frac {5
\,\nu_{{48}}}{24}}-1/6\,\nu_{{55}}-5/4\,\nu_{{59}}+1/8\,\nu_{{80}}+1/2
\,\nu_{{206}}+1/2\,\nu_{{149}}-{\frac {2409\,\nu_{{83}}}{976}}-{\frac 
{763\,\nu_{{90}}}{976}}-{\frac {161\,\nu_{{109}}}{1464}}\\
 &\hphantom{=}+{\frac {69\,
\nu_{{137}}}{488}}-1/16\,\nu_{{20}}-5/4\,\nu_{{58}}-{\frac {7\,\nu_{{
165}}}{12}}-{\frac {35\,\nu_{{9}}}{192}}, \\
\\
\nu_{{192}}&=-{\frac {5\,\nu_{
{164}}}{288}}-1/8\,\nu_{{69}}-{\frac {23\,\nu_{{7}}}{4608}}+1/6\,\nu_{
{151}}+1/3\,\nu_{{153}}+{\frac {\nu_{{51}}}{144}}+{\frac {5\,\nu_{{48}
}}{576}}+{\frac {\nu_{{55}}}{192}}-1/36\,\nu_{{59}}-{\frac {\nu_{{80}}
}{144}}\\
 &\hphantom{=}+1/8\,\nu_{{206}}+{\frac {\nu_{{160}}}{72}}+1/24\,\nu_{{149}}-{
\frac {49\,\nu_{{83}}}{144}}-{\frac {67\,\nu_{{90}}}{288}}+1/12\,\nu_{
{178}}-1/12\,\nu_{{202}}-1/8\,\nu_{{204}}-{\frac {\nu_{{20}}}{192}}\\
 &\hphantom{=}-1/
36\,\nu_{{58}}+{\frac {\nu_{{165}}}{72}}-{\frac {107\,\nu_{{9}}}{4608}
}, \\
\\
\nu_{{193}}&=-1/16\,\nu_{{160}}+{\frac {11\,\nu_{{164}}}{192}}+1/24\,
\nu_{{165}}+1/8\,\nu_{{178}}+1/32\,\nu_{{181}}-1/2\,\nu_{{194}}+1/16\,
\nu_{{202}}+1/32\,\nu_{{204}}\\
 &\hphantom{=}-1/16\,\nu_{{206}}+{\frac {11\,\nu_{{7}}
}{16384}}+{\frac {289\,\nu_{{9}}}{12288}}-{\frac {5\,\nu_{{48}}}{1536}
}-{\frac {\nu_{{51}}}{96}}-{\frac {5\,\nu_{{55}}}{512}}+{\frac {211\,
\nu_{{58}}}{1536}}+{\frac {211\,\nu_{{59}}}{1536}}-1/8\,\nu_{{69}}-{
\frac {3\,\nu_{{79}}}{64}}\\
 &\hphantom{=}-{\frac {\nu_{{80}}}{192}}+{\frac {19061\,
\nu_{{83}}}{46848}}+{\frac {2273\,\nu_{{90}}}{23424}}-{\frac {499\,\nu
_{{108}}}{11712}}-{\frac {1547\,\nu_{{109}}}{23424}}+{\frac {47\,\nu_{
{124}}}{7808}}-{\frac {7\,\nu_{{127}}}{46848}}-{\frac {\nu_{{131}}}{
5856}}\\
 &\hphantom{=}+{\frac {541\,\nu_{{132}}}{46848}}+{\frac {1761\,\nu_{{137}}}{
7808}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol18}
\begin{aligned}
\nu_{{195}}&={\frac {65\,\nu_{{108}}}{976}}-{\frac {37\,\nu_{{7}
}}{4096}}-{\frac {79\,\nu_{{124}}}{1952}}+{\frac {17\,\nu_{{131}}}{488
}}-{\frac {47\,\nu_{{132}}}{3904}}-{\frac {3\,\nu_{{127}}}{3904}}+3/16
\,\nu_{{79}}+{\frac {\nu_{{48}}}{128}}+{\frac {3\,\nu_{{55}}}{128}}-{
\frac {39\,\nu_{{59}}}{128}}\\
 &\hphantom{=}+1/16\,\nu_{{80}}+1/4\,\nu_{{160}}-{\frac 
{4763\,\nu_{{83}}}{3904}}-{\frac {251\,\nu_{{90}}}{488}}+{\frac {313\,
\nu_{{109}}}{1952}}-{\frac {873\,\nu_{{137}}}{1952}}-3/8\,\nu_{{181}}-
3/4\,\nu_{{202}}\\
 &\hphantom{=}-3/8\,\nu_{{204}}-{\frac {39\,\nu_{{58}}}{128}}-{
\frac {73\,\nu_{{9}}}{1024}},\\
\\
\nu_{{196}}&=-{\frac {7\,\nu_{{164}}}{16}}
+{\frac {19\,\nu_{{108}}}{122}}+3/2\,\nu_{{69}}+{\frac {63\,\nu_{{7}}
}{2048}}+{\frac {3\,\nu_{{124}}}{122}}-{\frac {25\,\nu_{{131}}}{488}}-
{\frac {25\,\nu_{{132}}}{488}}+{\frac {\nu_{{127}}}{488}}-{\frac {3\,
\nu_{{48}}}{64}}-{\frac {\nu_{{55}}}{64}}\\
 &\hphantom{=}-{\frac {39\,\nu_{{59}}}{64}}
+3/4\,\nu_{{206}}-{\frac {139\,\nu_{{83}}}{976}}+{\frac {1559\,\nu_{{
90}}}{1952}}+{\frac {19\,\nu_{{109}}}{122}}-{\frac {333\,\nu_{{137}}}{
488}}-{\frac {39\,\nu_{{58}}}{64}}-1/4\,\nu_{{165}}-{\frac {3\,\nu_{{9
}}}{512}}, \\
\\
\nu_{{197}}&={\frac {7\,\nu_{{164}}}{16}}+{\frac {57\,\nu_{{
108}}}{122}}-3/2\,\nu_{{69}}-{\frac {63\,\nu_{{7}}}{2048}}+{\frac {9\,
\nu_{{124}}}{122}}-{\frac {75\,\nu_{{131}}}{488}}-{\frac {75\,\nu_{{
132}}}{488}}+{\frac {3\,\nu_{{127}}}{488}}+{\frac {3\,\nu_{{48}}}{64}}
+{\frac {\nu_{{55}}}{64}}\\
 &\hphantom{=}+{\frac {39\,\nu_{{59}}}{64}}-3/4\,\nu_{{206}
}-{\frac {173\,\nu_{{83}}}{976}}-{\frac {935\,\nu_{{90}}}{1952}}+{
\frac {57\,\nu_{{109}}}{122}}-{\frac {999\,\nu_{{137}}}{488}}+{\frac {
39\,\nu_{{58}}}{64}}+1/4\,\nu_{{165}}+{\frac {3\,\nu_{{9}}}{512}}, \\
\\
\nu_
{{198}}&={\frac {3\,\nu_{{124}}}{61}}+{\frac {19\,\nu_{{109}}}{61}}-{
\frac {333\,\nu_{{137}}}{244}}+{\frac {\nu_{{127}}}{244}}+{\frac {19\,
\nu_{{108}}}{61}}-{\frac {25\,\nu_{{132}}}{244}}-{\frac {25\,\nu_{{131
}}}{244}}-{\frac {39\,\nu_{{83}}}{244}}+{\frac {39\,\nu_{{90}}}{244}},\\
\\
\nu_{{199}}&=-{\frac {11\,\nu_{{108}}}{1952}}-3/16\,\nu_{{69}}-{\frac {
33\,\nu_{{7}}}{8192}}-{\frac {103\,\nu_{{124}}}{3904}}+{\frac {59\,\nu
_{{131}}}{1952}}+{\frac {53\,\nu_{{132}}}{7808}}-{\frac {7\,\nu_{{127}
}}{7808}}+{\frac {3\,\nu_{{79}}}{32}}+1/16\,\nu_{{51}}\\
 &\hphantom{=}+{\frac {33\,\nu
_{{48}}}{256}}+{\frac {19\,\nu_{{55}}}{256}}+{\frac {77\,\nu_{{59}}}{
256}}-1/32\,\nu_{{80}}-1/8\,\nu_{{160}}+{\frac {3445\,\nu_{{83}}}{7808
}}+{\frac {565\,\nu_{{90}}}{3904}}+{\frac {161\,\nu_{{109}}}{3904}}\\
 &\hphantom{=}-{
\frac {207\,\nu_{{137}}}{3904}}+3/16\,\nu_{{181}}+1/8\,\nu_{{202}}+1/
16\,\nu_{{204}}+{\frac {77\,\nu_{{58}}}{256}}+{\frac {51\,\nu_{{9}}}{
2048}}, \\
\\
\nu_{{200}}&={\frac {7\,\nu_{{164}}}{16}}-{\frac {11\,\nu_{{108}
}}{1464}}-\nu_{{69}}-{\frac {23\,\nu_{{7}}}{1536}}-{\frac {103\,\nu_{{
124}}}{2928}}+{\frac {59\,\nu_{{131}}}{1464}}+{\frac {53\,\nu_{{132}}
}{5856}}-{\frac {7\,\nu_{{127}}}{5856}}+1/8\,\nu_{{79}}\\
 &\hphantom{=}-1/16\,\nu_{{51
}}+1/48\,\nu_{{48}}+{\frac {7\,\nu_{{59}}}{16}}+{\frac {5\,\nu_{{80}}
}{48}}-1/4\,\nu_{{206}}+{\frac {2347\,\nu_{{83}}}{5856}}-{\frac {1615
\,\nu_{{90}}}{5856}}+{\frac {161\,\nu_{{109}}}{2928}}\\
 &\hphantom{=}-{\frac {69\,\nu_
{{137}}}{976}}+{\frac {7\,\nu_{{58}}}{16}}+1/4\,\nu_{{165}}+{\frac {3
\,\nu_{{9}}}{128}}, \\
\\
\nu_{{201}}&={\frac {11\,\nu_{{108}}}{976}}+3/8\,\nu
_{{69}}+{\frac {65\,\nu_{{7}}}{4096}}+{\frac {103\,\nu_{{124}}}{1952}}
-{\frac {59\,\nu_{{131}}}{976}}-{\frac {53\,\nu_{{132}}}{3904}}+{
\frac {7\,\nu_{{127}}}{3904}}-3/16\,\nu_{{79}}\\
 &\hphantom{=}-{\frac {3\,\nu_{{51}}}{
32}}-{\frac {25\,\nu_{{48}}}{128}}-{\frac {15\,\nu_{{55}}}{128}}-{
\frac {33\,\nu_{{59}}}{128}}+1/32\,\nu_{{80}}+{\frac {947\,\nu_{{83}}
}{3904}}+{\frac {533\,\nu_{{90}}}{1952}}-{\frac {161\,\nu_{{109}}}{
1952}}\\
 &\hphantom{=}+{\frac {207\,\nu_{{137}}}{1952}}-{\frac {33\,\nu_{{58}}}{128}}+
{\frac {15\,\nu_{{9}}}{1024}},
\end{aligned}
\end{align}
\begin{align}\label{AreaSol19}
\begin{aligned}
\nu_{{203}}&={\frac {11\,\nu_{{108}}}{732
}}-\nu_{{69}}-{\frac {97\,\nu_{{7}}}{3072}}+{\frac {103\,\nu_{{124}}}{
1464}}-{\frac {59\,\nu_{{131}}}{732}}-{\frac {53\,\nu_{{132}}}{2928}}+
{\frac {7\,\nu_{{127}}}{2928}}-1/4\,\nu_{{79}}+1/8\,\nu_{{51}}+{\frac 
{5\,\nu_{{48}}}{96}}\\
 &\hphantom{=}+1/32\,\nu_{{55}}+{\frac {11\,\nu_{{59}}}{32}}-{
\frac {5\,\nu_{{80}}}{24}}-{\frac {1981\,\nu_{{83}}}{2928}}-{\frac {
1297\,\nu_{{90}}}{1464}}-{\frac {161\,\nu_{{109}}}{1464}}+{\frac {69\,
\nu_{{137}}}{488}}+{\frac {11\,\nu_{{58}}}{32}}-{\frac {9\,\nu_{{9}}}{
256}}, \\
\\
\nu_{{205}}&=-\nu_{{206}}-{\frac {103\,\nu_{{124}}}{488}}+{\frac 
{161\,\nu_{{109}}}{488}}-{\frac {207\,\nu_{{137}}}{488}}-{\frac {7\,
\nu_{{127}}}{976}}-{\frac {11\,\nu_{{108}}}{244}}+{\frac {53\,\nu_{{
132}}}{976}}+{\frac {59\,\nu_{{131}}}{244}}+3/4\,\nu_{{79}}\\
 &\hphantom{=}+1/8\,\nu_{
{80}}-{\frac {215\,\nu_{{83}}}{976}}+1/8\,\nu_{{51}}+{\frac {2\,\nu_{{
90}}}{61}}+{\frac {5\,\nu_{{59}}}{32}}-{\frac {7\,\nu_{{9}}}{256}}+{
\frac {9\,\nu_{{48}}}{32}}+{\frac {7\,\nu_{{55}}}{32}}+{\frac {5\,\nu_
{{58}}}{32}}-{\frac {5\,\nu_{{7}}}{1024}}, \\
\\
\nu_{{207}}&=\nu_{{206}}-{
\frac {103\,\nu_{{124}}}{1464}}+{\frac {161\,\nu_{{109}}}{1464}}+\nu_{
{69}}-{\frac {69\,\nu_{{137}}}{488}}-{\frac {7\,\nu_{{127}}}{2928}}-{
\frac {11\,\nu_{{108}}}{732}}+{\frac {53\,\nu_{{132}}}{2928}}+{\frac {
59\,\nu_{{131}}}{732}}+1/4\,\nu_{{79}}\\
 &\hphantom{=}+{\frac {5\,\nu_{{80}}}{24}}+{
\frac {1249\,\nu_{{83}}}{2928}}-1/8\,\nu_{{51}}+{\frac {557\,\nu_{{90}
}}{732}}-{\frac {17\,\nu_{{59}}}{32}}+{\frac {3\,\nu_{{9}}}{256}}-{
\frac {23\,\nu_{{48}}}{96}}-{\frac {3\,\nu_{{55}}}{32}}-{\frac {17\,
\nu_{{58}}}{32}}+{\frac {115\,\nu_{{7}}}{3072}}, \\
\\
\nu_{{208}}&={\frac {3
\,\nu_{{124}}}{61}}+{\frac {19\,\nu_{{109}}}{61}}-{\frac {333\,\nu_{{
137}}}{244}}+{\frac {\nu_{{127}}}{244}}+{\frac {19\,\nu_{{108}}}{61}}-
{\frac {25\,\nu_{{132}}}{244}}-{\frac {25\,\nu_{{131}}}{244}}-{\frac {
139\,\nu_{{83}}}{488}}+{\frac {95\,\nu_{{90}}}{976}}-{\frac {3\,\nu_{{
59}}}{32}}\\
 &\hphantom{=}-{\frac {3\,\nu_{{9}}}{256}}-{\frac {3\,\nu_{{48}}}{32}}-1/
32\,\nu_{{55}}-{\frac {3\,\nu_{{58}}}{32}}+{\frac {3\,\nu_{{7}}}{1024}
}, \\
\\
\nu_{{209}}&=-{\frac {12\,\nu_{{124}}}{61}}-{\frac {76\,\nu_{{109}}}{
61}}+{\frac {333\,\nu_{{137}}}{61}}-{\frac {\nu_{{127}}}{61}}-{\frac {
76\,\nu_{{108}}}{61}}+{\frac {25\,\nu_{{132}}}{61}}+{\frac {25\,\nu_{{
131}}}{61}}+{\frac {39\,\nu_{{83}}}{61}}-{\frac {39\,\nu_{{90}}}{61}},\\
\\
\nu_{{210}}&=-\nu_{{206}}-{\frac {103\,\nu_{{124}}}{732}}+{\frac {161\,
\nu_{{109}}}{732}}-\nu_{{69}}-{\frac {69\,\nu_{{137}}}{244}}-{\frac {7
\,\nu_{{127}}}{1464}}-{\frac {11\,\nu_{{108}}}{366}}+{\frac {53\,\nu_{
{132}}}{1464}}+{\frac {59\,\nu_{{131}}}{366}}+1/2\,\nu_{{79}}\\
 &\hphantom{=}-1/12\,
\nu_{{80}}-{\frac {1313\,\nu_{{83}}}{1464}}+1/4\,\nu_{{51}}-{\frac {
1249\,\nu_{{90}}}{1464}}+1/2\,\nu_{{59}}-1/16\,\nu_{{9}}+1/3\,\nu_{{48
}}+1/4\,\nu_{{55}}+1/2\,\nu_{{58}}\\
 &\hphantom{=}-{\frac {7\,\nu_{{7}}}{192}}, \\
\\
\nu_{{
211}}&=-{\frac {239\,\nu_{{108}}}{1952}}+{\frac {27\,\nu_{{7}}}{8192}}-
{\frac {175\,\nu_{{124}}}{3904}}+{\frac {67\,\nu_{{131}}}{976}}+{
\frac {353\,\nu_{{132}}}{7808}}-{\frac {19\,\nu_{{127}}}{7808}}+{
\frac {3\,\nu_{{79}}}{32}}+1/32\,\nu_{{51}}+{\frac {17\,\nu_{{48}}}{
256}}\\
 &\hphantom{=}+{\frac {11\,\nu_{{55}}}{256}}+{\frac {49\,\nu_{{59}}}{256}}-1/8
\,\nu_{{160}}+{\frac {4645\,\nu_{{83}}}{7808}}+{\frac {55\,\nu_{{90}}
}{244}}-{\frac {295\,\nu_{{109}}}{3904}}+{\frac {1791\,\nu_{{137}}}{
3904}}+3/16\,\nu_{{181}}\\
 &\hphantom{=}+1/8\,\nu_{{202}}+1/16\,\nu_{{204}}+{\frac {49
\,\nu_{{58}}}{256}}+{\frac {59\,\nu_{{9}}}{2048}}, \\
\\
\nu_{{212}}&={\frac {
7\,\nu_{{164}}}{32}}+{\frac {217\,\nu_{{108}}}{2928}}-1/2\,\nu_{{69}}-
{\frac {23\,\nu_{{7}}}{3072}}-{\frac {31\,\nu_{{124}}}{5856}}-{\frac {
\nu_{{131}}}{183}}-{\frac {247\,\nu_{{132}}}{11712}}+{\frac {5\,\nu_{{
127}}}{11712}}+1/16\,\nu_{{79}}-1/32\,\nu_{{51}}\\
 &\hphantom{=}+{\frac {\nu_{{48}}}{
96}}+{\frac {7\,\nu_{{59}}}{32}}+{\frac {5\,\nu_{{80}}}{96}}-1/8\,\nu_
{{206}}+{\frac {1879\,\nu_{{83}}}{11712}}-{\frac {1147\,\nu_{{90}}}{
11712}}+{\frac {617\,\nu_{{109}}}{5856}}-{\frac {735\,\nu_{{137}}}{
1952}}+{\frac {7\,\nu_{{58}}}{32}}\\
 &\hphantom{=}+1/8\,\nu_{{165}}+{\frac {3\,\nu_{{9
}}}{256}}, 
\end{aligned}
\end{align}
\begin{align}\label{AreaSol20}
\begin{aligned}
\nu_{{213}}&={\frac {79\,\nu_{{124}}}{488}}-{\frac {313\,\nu_
{{109}}}{488}}+{\frac {873\,\nu_{{137}}}{488}}+{\frac {3\,\nu_{{127}}
}{976}}-{\frac {65\,\nu_{{108}}}{244}}+{\frac {47\,\nu_{{132}}}{976}}-
{\frac {17\,\nu_{{131}}}{122}}-3/4\,\nu_{{79}}\\
 &\hphantom{=}-1/8\,\nu_{{80}}+{\frac 
{371\,\nu_{{83}}}{976}}-1/8\,\nu_{{51}}-{\frac {47\,\nu_{{90}}}{244}}-
{\frac {5\,\nu_{{59}}}{32}}+{\frac {7\,\nu_{{9}}}{256}}-{\frac {9\,\nu
_{{48}}}{32}}-{\frac {7\,\nu_{{55}}}{32}}-{\frac {5\,\nu_{{58}}}{32}}+
{\frac {5\,\nu_{{7}}}{1024}}, \\
\\
\nu_{{214}}&=-{\frac {11\,\nu_{{108}}}{732
}}-1/2\,\nu_{{69}}-{\frac {37\,\nu_{{7}}}{1536}}-{\frac {103\,\nu_{{
124}}}{1464}}+{\frac {59\,\nu_{{131}}}{732}}+{\frac {53\,\nu_{{132}}}{
2928}}-{\frac {7\,\nu_{{127}}}{2928}}+1/4\,\nu_{{79}}\\
 &\hphantom{=}+1/8\,\nu_{{51}}+
{\frac {17\,\nu_{{48}}}{48}}+3/16\,\nu_{{55}}+{\frac {7\,\nu_{{59}}}{
16}}-1/24\,\nu_{{80}}-{\frac {581\,\nu_{{83}}}{2928}}-{\frac {883\,\nu
_{{90}}}{2928}}+{\frac {161\,\nu_{{109}}}{1464}}\\
 &\hphantom{=}-{\frac {69\,\nu_{{137
}}}{488}}+{\frac {7\,\nu_{{58}}}{16}}-{\frac {\nu_{{9}}}{128}}, \\
\\
\nu_{{
215}}&={\frac {7\,\nu_{{164}}}{32}}+{\frac {217\,\nu_{{108}}}{976}}-3/4
\,\nu_{{69}}-{\frac {5\,\nu_{{7}}}{256}}-{\frac {31\,\nu_{{124}}}{1952
}}-{\frac {\nu_{{131}}}{61}}-{\frac {247\,\nu_{{132}}}{3904}}+{\frac {
5\,\nu_{{127}}}{3904}}+3/16\,\nu_{{79}}\\
 &\hphantom{=}+1/32\,\nu_{{51}}+3/16\,\nu_{{
48}}+{\frac {3\,\nu_{{55}}}{32}}+{\frac {7\,\nu_{{59}}}{16}}+1/32\,\nu
_{{80}}-1/8\,\nu_{{206}}-{\frac {73\,\nu_{{83}}}{3904}}-{\frac {659\,
\nu_{{90}}}{3904}}+{\frac {617\,\nu_{{109}}}{1952}}\\
 &\hphantom{=}-{\frac {2205\,\nu_
{{137}}}{1952}}+{\frac {7\,\nu_{{58}}}{16}}+1/8\,\nu_{{165}}+{\frac {
\nu_{{9}}}{128}}, \\
\\
\nu_{{216}}&=-{\frac {\nu_{{149}}}{96}}-1/24\,\nu_{{
151}}+1/24\,\nu_{{153}}+{\frac {\nu_{{160}}}{96}}+{\frac {5\,\nu_{{164
}}}{384}}+1/48\,\nu_{{165}}+1/48\,\nu_{{178}}+1/48\,\nu_{{202}}\\
 &\hphantom{=}-1/32\,
\nu_{{204}}+1/32\,\nu_{{206}}+{\frac {\nu_{{20}}}{768}}+{\frac {67\,
\nu_{{7}}}{49152}}+{\frac {9\,\nu_{{9}}}{4096}}-{\frac {41\,\nu_{{48}}
}{1536}}-{\frac {\nu_{{51}}}{64}}-{\frac {9\,\nu_{{55}}}{512}}-{\frac 
{17\,\nu_{{58}}}{512}}-{\frac {17\,\nu_{{59}}}{512}}\\
 &\hphantom{=}+1/32\,\nu_{{69}}-
1/32\,\nu_{{79}}+{\frac {\nu_{{80}}}{192}}+{\frac {199\,\nu_{{83}}}{
11712}}+{\frac {851\,\nu_{{90}}}{46848}}+{\frac {11\,\nu_{{108}}}{5856
}}-{\frac {161\,\nu_{{109}}}{11712}}+{\frac {103\,\nu_{{124}}}{11712}}
+{\frac {7\,\nu_{{127}}}{23424}}\\
 &\hphantom{=}-{\frac {59\,\nu_{{131}}}{5856}}-{
\frac {53\,\nu_{{132}}}{23424}}+{\frac {69\,\nu_{{137}}}{3904}}
\end{aligned}
\end{align}
\printbibliography

\end{document}
